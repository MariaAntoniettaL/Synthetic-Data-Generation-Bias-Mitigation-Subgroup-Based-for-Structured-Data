{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ANALISI CONDOTTA CON LA FEATURE error rate (PASSATA A BOOLEAN OUTCOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_for_adult import preprocessing_funct_not_enc, encoding_funct, K_subgroups_dataset_and_or, metrics_to_compare,encoding_funct_SMOTE\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "\n",
    "      \n",
    "from divexplorer import DivergenceExplorer\n",
    "from divexplorer import DivergencePatternProcessor       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "df = pd.read_csv(\"adult.data\", header = None, names = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "pd.options.display.float_format = '{:.3f}'.format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup = 0.1\n",
    "percentage = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET ROWS:  13014\n",
      "VALIDATION SET ROWS:  6507\n",
      "HOLDOUT SET ROWS:  6508\n",
      "TEST SET ROWS:  6508\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val, df_test, df_holdout = preprocessing_funct_not_enc(df)\n",
    "#controllo divisione dataset\n",
    "print(f\"TRAIN SET ROWS: \", df_train.shape[0]) #su 32536, il 40%  dovrebbe essere circa 13014\n",
    "print(f\"VALIDATION SET ROWS: \", df_val.shape[0]) #su 32536, il 20%  dovrebbe essere circa 6500\n",
    "print(f\"HOLDOUT SET ROWS: \", df_holdout.shape[0])\n",
    "print(f\"TEST SET ROWS: \", df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET ROWS:  13014\n",
      "VALIDATION SET ROWS:  6507\n",
      "HOLDOUT SET ROWS:  6508\n",
      "TEST SET ROWS:  6508\n"
     ]
    }
   ],
   "source": [
    "df_train_enc, df_test_enc, df_holdout_enc, df_val_enc = encoding_funct(df_train=df_train, df_test=df_test, df_holdout=df_holdout, df_val=df_val)\n",
    "#controllo coerenza con numerosità precedente\n",
    "print(f\"TRAIN SET ROWS: \", df_train_enc.shape[0]) #su 32536, il 40%  dovrebbe essere circa 13014\n",
    "print(f\"VALIDATION SET ROWS: \", df_val_enc.shape[0]) #su 32536, il 20%  dovrebbe essere circa 6500\n",
    "print(f\"HOLDOUT SET ROWS: \", df_holdout_enc.shape[0])\n",
    "print(f\"TEST SET ROWS: \", df_test_enc.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#GDecisioN tree predictions\n",
    "X_train = df_train_enc.drop(columns = 'income', axis = 1)\n",
    "y_train = df_train_enc['income']\n",
    "\n",
    "X_test = df_test_enc.drop(columns = 'income', axis = 1)\n",
    "y_test = df_test_enc['income']\n",
    "\n",
    "X_val = df_val_enc.drop(columns = 'income', axis = 1)\n",
    "y_val = df_val_enc['income']\n",
    "\n",
    "X_holdout = df_holdout_enc.drop(columns = 'income', axis = 1)\n",
    "y_holdout = df_holdout_enc['income']\n",
    "\n",
    "classifier_train = LogisticRegression(random_state=seed)\n",
    "classifier_train.fit(X_train, y_train)\n",
    "y_pred = classifier_train.predict(X_test)\n",
    "cm_classifier = confusion_matrix(y_test, y_pred)\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=cm_classifier, display_labels=[False, True])\n",
    "#disp.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics            Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation     0.809     0.474                0.047   \n",
       "\n",
       "Metrics            False Negative Rate  False Positives  False Negatives  \\\n",
       "Before Mitigation                0.643              234             1008   \n",
       "\n",
       "Metrics            Test Size  \n",
       "Before Mitigation       6508  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before = metrics_to_compare(y_true = y_test, y_pred = y_pred )\n",
    "metrics_before_df = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Test Size'],\n",
    "    'Before Mitigation': [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_test)],\n",
    "})\n",
    "metrics_before_df = metrics_before_df.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_before_df[metric] = metrics_before_df[metric].astype(int)\n",
    "\n",
    "metrics_before_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBGROUPS SEARCH\n",
    "\n",
    "-Identifico i gruppi applicando DivExplorer sul validation not encoded (a cui ho aggiunto la feature sui falsi positivi da passare a boolean outcomes e la feature accuracy che vale 1 se la predizione è giusta e 0 se sbagliata )\n",
    "\n",
    "-Integro nel training set dati che matchano sottogruppi problematici prendendoli dall'holdout, (primi K = 5, tutte le righe holdout che matchano)\n",
    "\n",
    "-Ripeto training e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>native-country</th>\n",
       "      <th>age_group</th>\n",
       "      <th>edu_num_group</th>\n",
       "      <th>hours_per_week_group</th>\n",
       "      <th>y_val_true</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18761</th>\n",
       "      <td>2</td>\n",
       "      <td>0.077</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27582</th>\n",
       "      <td>3</td>\n",
       "      <td>0.048</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30911</th>\n",
       "      <td>2</td>\n",
       "      <td>0.174</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11128</th>\n",
       "      <td>0</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.507</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>0</td>\n",
       "      <td>0.284</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       workclass  fnlwgt  education  marital-status  occupation  relationship  \\\n",
       "18761          2   0.077          3               0           4             1   \n",
       "27582          3   0.048          3               1           4             0   \n",
       "30911          2   0.174          3               3           4             4   \n",
       "11128          0   0.012          2               1           2             0   \n",
       "683            0   0.284          3               3           2             4   \n",
       "\n",
       "       race  sex  capital-gain  capital-loss  native-country  age_group  \\\n",
       "18761     4    0         0.000         0.000               5          2   \n",
       "27582     4    1         0.000         0.000               5          5   \n",
       "30911     2    0         0.039         0.000               5          1   \n",
       "11128     2    1         0.000         0.507               5          3   \n",
       "683       2    1         0.000         0.000               5          0   \n",
       "\n",
       "       edu_num_group  hours_per_week_group  y_val_true  y_pred  \n",
       "18761              1                     1           0       0  \n",
       "27582              1                     2           1       0  \n",
       "30911              1                     1           0       0  \n",
       "11128              4                     1           1       1  \n",
       "683                1                     1           0       0  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predizioni per il validation set\n",
    "y_pred_val_dt = classifier_train.predict(X_val)\n",
    "\n",
    "df_val_class = X_val.copy()\n",
    "df_val_class['y_val_true'] = y_val\n",
    "df_val_class['y_pred'] = y_pred_val_dt\n",
    "\n",
    "df_val_class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trues = df_val_class[\"y_val_true\"]\n",
    "y_preds = df_val_class[\"y_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "      <th>age_group</th>\n",
       "      <th>edu_num_group</th>\n",
       "      <th>hours_per_week_group</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18761</th>\n",
       "      <td>Private</td>\n",
       "      <td>0.077</td>\n",
       "      <td>Non Graduated</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Self-emp-occ</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>35-44</td>\n",
       "      <td>10 College</td>\n",
       "      <td>Overtime</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27582</th>\n",
       "      <td>Self-emp</td>\n",
       "      <td>0.048</td>\n",
       "      <td>Non Graduated</td>\n",
       "      <td>Married</td>\n",
       "      <td>Self-emp-occ</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>65-100</td>\n",
       "      <td>10 College</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30911</th>\n",
       "      <td>Private</td>\n",
       "      <td>0.174</td>\n",
       "      <td>Non Graduated</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Self-emp-occ</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>25-34</td>\n",
       "      <td>10 College</td>\n",
       "      <td>Overtime</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11128</th>\n",
       "      <td>Government</td>\n",
       "      <td>0.012</td>\n",
       "      <td>Master's Degree</td>\n",
       "      <td>Married</td>\n",
       "      <td>Private-occ</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.507</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>45-54</td>\n",
       "      <td>14 Master's Degree</td>\n",
       "      <td>Overtime</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>Government</td>\n",
       "      <td>0.284</td>\n",
       "      <td>Non Graduated</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Private-occ</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>17-24</td>\n",
       "      <td>10 College</td>\n",
       "      <td>Overtime</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        workclass  fnlwgt        education marital-status    occupation  \\\n",
       "18761     Private   0.077    Non Graduated       Divorced  Self-emp-occ   \n",
       "27582    Self-emp   0.048    Non Graduated        Married  Self-emp-occ   \n",
       "30911     Private   0.174    Non Graduated      Separated  Self-emp-occ   \n",
       "11128  Government   0.012  Master's Degree        Married   Private-occ   \n",
       "683    Government   0.284    Non Graduated      Separated   Private-occ   \n",
       "\n",
       "         relationship    race      sex  capital-gain  capital-loss  \\\n",
       "18761   Not-in-family   White   Female         0.000         0.000   \n",
       "27582         Husband   White     Male         0.000         0.000   \n",
       "30911       Unmarried   Black   Female         0.039         0.000   \n",
       "11128         Husband   Black     Male         0.000         0.507   \n",
       "683         Unmarried   Black     Male         0.000         0.000   \n",
       "\n",
       "      native-country  income age_group       edu_num_group  \\\n",
       "18761  United-States       0     35-44          10 College   \n",
       "27582  United-States       1    65-100          10 College   \n",
       "30911  United-States       0     25-34          10 College   \n",
       "11128  United-States       1     45-54  14 Master's Degree   \n",
       "683    United-States       0     17-24          10 College   \n",
       "\n",
       "      hours_per_week_group  y_pred  error  \n",
       "18761             Overtime       0      0  \n",
       "27582            Part-time       0      1  \n",
       "30911             Overtime       0      0  \n",
       "11128             Overtime       1      0  \n",
       "683               Overtime       0      0  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aggiungo la feature accuracy a df_val non encoded che assume valore 1 se la predizione è giusta 0 se la predizione è sbagliata\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_val['y_pred'] = df_val_class['y_pred'] \n",
    "df_val['error'] = (df_val_class['y_val_true'] != df_val_class['y_pred']).astype(int)\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemset</th>\n",
       "      <th>error</th>\n",
       "      <th>error_div</th>\n",
       "      <th>error_t</th>\n",
       "      <th>length</th>\n",
       "      <th>support_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.101</td>\n",
       "      <td>(capital-gain=0.0, marital-status=Married, education=Bachelor's Degree)</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.265</td>\n",
       "      <td>13.239</td>\n",
       "      <td>3</td>\n",
       "      <td>659.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.102</td>\n",
       "      <td>(capital-loss=0.0, capital-gain=0.0, hours_per_week_group=Overtime, occupation=Private-occ, marital-status=Married)</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.250</td>\n",
       "      <td>12.564</td>\n",
       "      <td>5</td>\n",
       "      <td>663.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.102</td>\n",
       "      <td>(native-country=United-States, capital-gain=0.0, occupation=Private-occ, hours_per_week_group=Overtime, marital-status=Married)</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.250</td>\n",
       "      <td>12.564</td>\n",
       "      <td>5</td>\n",
       "      <td>663.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.102</td>\n",
       "      <td>(capital-gain=0.0, hours_per_week_group=Overtime, occupation=Private-occ, marital-status=Married, race= White)</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.247</td>\n",
       "      <td>12.420</td>\n",
       "      <td>5</td>\n",
       "      <td>663.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.104</td>\n",
       "      <td>(native-country=United-States, capital-loss=0.0, hours_per_week_group=Overtime, occupation=Private-occ, marital-status=Married, race= White)</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.244</td>\n",
       "      <td>12.405</td>\n",
       "      <td>6</td>\n",
       "      <td>676.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   support  \\\n",
       "0    0.101   \n",
       "1    0.102   \n",
       "2    0.102   \n",
       "3    0.102   \n",
       "4    0.104   \n",
       "\n",
       "                                                                                                                                        itemset  \\\n",
       "0                                                                       (capital-gain=0.0, marital-status=Married, education=Bachelor's Degree)   \n",
       "1                           (capital-loss=0.0, capital-gain=0.0, hours_per_week_group=Overtime, occupation=Private-occ, marital-status=Married)   \n",
       "2               (native-country=United-States, capital-gain=0.0, occupation=Private-occ, hours_per_week_group=Overtime, marital-status=Married)   \n",
       "3                                (capital-gain=0.0, hours_per_week_group=Overtime, occupation=Private-occ, marital-status=Married, race= White)   \n",
       "4  (native-country=United-States, capital-loss=0.0, hours_per_week_group=Overtime, occupation=Private-occ, marital-status=Married, race= White)   \n",
       "\n",
       "   error  error_div  error_t  length  support_count  \n",
       "0  0.464      0.265   13.239       3        659.000  \n",
       "1  0.449      0.250   12.564       5        663.000  \n",
       "2  0.449      0.250   12.564       5        663.000  \n",
       "3  0.446      0.247   12.420       5        663.000  \n",
       "4  0.444      0.244   12.405       6        676.000  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_diver = DivergenceExplorer(df_val)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = error_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by=[\"error_div\", \"error_t\"], ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "FP_fm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemset</th>\n",
       "      <th>length</th>\n",
       "      <th>support_count</th>\n",
       "      <th>error</th>\n",
       "      <th>error_div</th>\n",
       "      <th>error_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.101</td>\n",
       "      <td>(capital-gain=0.0, marital-status=Married, education=Bachelor's Degree)</td>\n",
       "      <td>3</td>\n",
       "      <td>659.000</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.265</td>\n",
       "      <td>13.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.118</td>\n",
       "      <td>(marital-status=Married, education=Bachelor's Degree)</td>\n",
       "      <td>2</td>\n",
       "      <td>768.000</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.229</td>\n",
       "      <td>12.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.120</td>\n",
       "      <td>(marital-status=Married, age_group=45-54)</td>\n",
       "      <td>2</td>\n",
       "      <td>781.000</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.205</td>\n",
       "      <td>11.269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.107</td>\n",
       "      <td>(relationship= Husband, age_group=45-54)</td>\n",
       "      <td>2</td>\n",
       "      <td>695.000</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.202</td>\n",
       "      <td>10.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.143</td>\n",
       "      <td>(marital-status=Married, age_group=35-44)</td>\n",
       "      <td>2</td>\n",
       "      <td>932.000</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.200</td>\n",
       "      <td>11.917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     support  \\\n",
       "0      0.101   \n",
       "29     0.118   \n",
       "144    0.120   \n",
       "163    0.107   \n",
       "195    0.143   \n",
       "\n",
       "                                                                     itemset  \\\n",
       "0    (capital-gain=0.0, marital-status=Married, education=Bachelor's Degree)   \n",
       "29                     (marital-status=Married, education=Bachelor's Degree)   \n",
       "144                                (marital-status=Married, age_group=45-54)   \n",
       "163                                 (relationship= Husband, age_group=45-54)   \n",
       "195                                (marital-status=Married, age_group=35-44)   \n",
       "\n",
       "     length  support_count  error  error_div  error_t  \n",
       "0         3        659.000  0.464      0.265   13.239  \n",
       "29        2        768.000  0.428      0.229   12.380  \n",
       "144       2        781.000  0.405      0.205   11.269  \n",
       "163       2        695.000  0.401      0.202   10.526  \n",
       "195       2        932.000  0.399      0.200   11.917  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pruning \n",
    "error_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = error_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values(\"error_div\", ascending=False)\n",
    "df_pruned_error.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total subgroups 67\n",
      "total problematic 38\n"
     ]
    }
   ],
   "source": [
    "# Numero totale di istanze\n",
    "total_instances = len(df_pruned_error)\n",
    "\n",
    "# Numero di istanze con fp_div > 0 e fp_t > 2\n",
    "filtered_instances = len(df_pruned_error[(df_pruned_error['error_div'] > 0) & (df_pruned_error['error_t'] > 2)])\n",
    "\n",
    "print('total subgroups', total_instances)\n",
    "print('total problematic', filtered_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim original:  (3793, 7)\n",
      "Dim pruned th_redundancy  (67, 7)\n"
     ]
    }
   ],
   "source": [
    "prun_size = df_pruned_error.shape\n",
    "original_size = FP_fm.shape\n",
    "print(\"Dim original: \", original_size)\n",
    "print(\"Dim pruned th_redundancy \", prun_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = int((percentage / 100) * filtered_instances)\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIAS MITIGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIAS MITIGATION: ADDING DATA: prendo dati dall'hold-out e li aggiungo al train set, questi dati matchano gli itemset trovati prima (i primi 5)\n",
    "\n",
    "1. prendo dati dall'holdout con la funzione K_subgroups_dataset_and_or li aggiungo train \n",
    "2. riapplico encoding tutto\n",
    "3. Decision tree nuovamente e vedo come sono cambiate le performance (ad es Accuracy, false positive rate, false negative rate) overall e per sottogruppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holdout_filtered = K_subgroups_dataset_and_or(df_pruned_error, df_holdout, K) #da aggiungere a train set e ripetere train e test\n",
    "\n",
    "df_combinated = pd.concat([df_holdout_filtered, df_train], ignore_index=True)\n",
    "df_train_mitigated= df_combinated.sample(frac=1, random_state=seed).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2341\n"
     ]
    }
   ],
   "source": [
    "print(len(df_holdout_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET ROWS:  13014\n",
      "TRAIN SET MITIGATED ROWS:  15355\n",
      "VALIDATION SET ROWS:  6508\n",
      "FILTERED DF holdout ROWS:  2341\n",
      "TEST SET FILTERED ROWS:  6507\n"
     ]
    }
   ],
   "source": [
    "#riapplico funzione di encoding, ma al posto di holdout, uso il df filtrato che devo usare per inserire dati \n",
    "df_train_enc_mit, inutile1, inutile3, inutile2 = encoding_funct(df_train=df_train_mitigated, df_test=df_test, df_holdout=df_holdout_filtered, df_val=df_val)\n",
    "#controllo divisione dataset\n",
    "df_train_enc_mit_fp = df_train_enc_mit  \n",
    "print(f\"TRAIN SET ROWS: \", df_train_enc.shape[0])\n",
    "print(f\"TRAIN SET MITIGATED ROWS: \", df_train_enc_mit.shape[0]) #su 32536, il 40%  dovrebbe essere circa 13014\n",
    "print(f\"VALIDATION SET ROWS: \", inutile1.shape[0]) #su 32536, il 20%  dovrebbe essere circa 6500\n",
    "print(f\"FILTERED DF holdout ROWS: \", inutile3.shape[0])\n",
    "print(f\"TEST SET FILTERED ROWS: \", inutile2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train_mitigated = df_train_enc_mit.drop(columns = 'income', axis = 1)\n",
    "y_train_mitigated = df_train_enc_mit['income']\n",
    "\n",
    "\n",
    "classifier_train_mitigated = LogisticRegression(random_state=seed)\n",
    "\n",
    "classifier_train_mitigated.fit(X_train_mitigated, y_train_mitigated)\n",
    "y_mitigated_pred = classifier_train_mitigated.predict(X_test)\n",
    "#cm_dt = confusion_matrix(y_test, y_mitigated_pred)\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=[False, True])\n",
    "#disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2341\n",
      "verifica : 2341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#per veriicare cosa accade se aggiungo in modo randomico lo stesso numero di righe al train, ripeto l'analisi facebdo mitigation con righe randomiche (uguali in numero)\n",
    "print(len(df_holdout_filtered))\n",
    "n = len(df_holdout_filtered)\n",
    "df_holdout_sampled = df_holdout_enc.sample(n=len(df_holdout_filtered), replace=True, random_state=seed)\n",
    "print(\"verifica :\", len(df_holdout_sampled)) #verifica\n",
    "\n",
    "\n",
    "\n",
    "df_combinated_random = pd.concat([df_holdout_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random= df_combinated_random.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "X_train_mitigated_random = df_train_mitigated_random.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random = df_train_mitigated_random['income']\n",
    "\n",
    "classifier_train_mitigated_random = LogisticRegression(random_state=seed)\n",
    "\n",
    "classifier_train_mitigated_random.fit(X_train_mitigated_random, y_train_mitigated_random)\n",
    "y_mitigated_pred_random = classifier_train_mitigated_random.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, fp)</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.611</td>\n",
       "      <td>305</td>\n",
       "      <td>958</td>\n",
       "      <td>15355</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.637</td>\n",
       "      <td>245</td>\n",
       "      <td>999</td>\n",
       "      <td>15355</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                    Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation             0.809     0.474                0.047   \n",
       "After Mitigation(K=5, fp)     0.806     0.491                0.062   \n",
       "After RANDOM mitigation       0.809     0.478                0.050   \n",
       "\n",
       "Metrics                    False Negative Rate  False Positives  \\\n",
       "Before Mitigation                        0.643              234   \n",
       "After Mitigation(K=5, fp)                0.611              305   \n",
       "After RANDOM mitigation                  0.637              245   \n",
       "\n",
       "Metrics                    False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                     1008       13014       6508  \n",
       "After Mitigation(K=5, fp)              958       15355       6508  \n",
       "After RANDOM mitigation                999       15355       6508  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_fp_after, f1_score_fp_after, fpr_fp_after, fnr_fp_after, fp_fp_after, fn_fp_after = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred )\n",
    "accuracy_fp_after_random, f1_score_fp_after_random, fpr_fp_after_random, fnr_fp_after_random, fp_fp_after_random, fn_fp_after_random= metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random)\n",
    "\n",
    "metrics_after_fp = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After Mitigation(K=5, fp)': [accuracy_fp_after, f1_score_fp_after, fpr_fp_after, fnr_fp_after, fp_fp_after, fn_fp_after, len(y_train_mitigated), len(y_test)],\n",
    "    'After RANDOM mitigation' : [accuracy_fp_after_random, f1_score_fp_after_random, fpr_fp_after_random, fnr_fp_after_random, fp_fp_after_random, fn_fp_after_random, len(y_train_mitigated_random), len(y_test)]\n",
    "})\n",
    "metrics_after_fp = metrics_after_fp.set_index('Metrics').T\n",
    "\n",
    "\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp[metric] = metrics_after_fp[metric].astype(int)\n",
    "\n",
    "metrics_after_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance su sottogruppi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation, on subgroups</th>\n",
       "      <td>0.627</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.617</td>\n",
       "      <td>147</td>\n",
       "      <td>723</td>\n",
       "      <td>13014</td>\n",
       "      <td>2330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp)</th>\n",
       "      <td>0.633</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.588</td>\n",
       "      <td>165</td>\n",
       "      <td>689</td>\n",
       "      <td>15355</td>\n",
       "      <td>2330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5, on subgroups, fp)</th>\n",
       "      <td>0.628</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.612</td>\n",
       "      <td>149</td>\n",
       "      <td>717</td>\n",
       "      <td>15355</td>\n",
       "      <td>2330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                         Accuracy  F1 Score  \\\n",
       "Before Mitigation, on subgroups                    0.627     0.507   \n",
       "After Mitigation(K=5, on subgroups, fp)            0.633     0.530   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)     0.628     0.512   \n",
       "\n",
       "Metrics                                         False Positive Rate  \\\n",
       "Before Mitigation, on subgroups                               0.127   \n",
       "After Mitigation(K=5, on subgroups, fp)                       0.142   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                0.129   \n",
       "\n",
       "Metrics                                         False Negative Rate  \\\n",
       "Before Mitigation, on subgroups                               0.617   \n",
       "After Mitigation(K=5, on subgroups, fp)                       0.588   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                0.612   \n",
       "\n",
       "Metrics                                         False Positives  \\\n",
       "Before Mitigation, on subgroups                             147   \n",
       "After Mitigation(K=5, on subgroups, fp)                     165   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)              149   \n",
       "\n",
       "Metrics                                         False Negatives  Train Size  \\\n",
       "Before Mitigation, on subgroups                             723       13014   \n",
       "After Mitigation(K=5, on subgroups, fp)                     689       15355   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)              717       15355   \n",
       "\n",
       "Metrics                                         Test Size  \n",
       "Before Mitigation, on subgroups                      2330  \n",
       "After Mitigation(K=5, on subgroups, fp)              2330  \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)       2330  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_filtered_fp = K_subgroups_dataset_and_or(df_pruned_error, df_test, K)\n",
    "inutile, df_test_filtered_enc_fp, inutile2, inutile3 = encoding_funct(df_train, df_test_filtered_fp, df_holdout, df_val)\n",
    "\n",
    "X_test_filtered_fp = df_test_filtered_enc_fp.drop(columns='income', axis = 1)\n",
    "y_true_test_filtered_fp = df_test_filtered_enc_fp['income']\n",
    "\n",
    "y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after = classifier_train_mitigated.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_random = classifier_train_mitigated_random.predict(X_test_filtered_fp)\n",
    "\n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after, f1_score_fp_sottogruppi_after, fpr_fp_sottogruppi_after, fnr_fp_sottogruppi_after, fp_fp_sottogruppi_after, fn_fp_sottogruppi_after = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after )\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_random)\n",
    "\n",
    "\n",
    "metrics_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after, f1_score_fp_sottogruppi_after, fpr_fp_sottogruppi_after, fnr_fp_sottogruppi_after, fp_fp_sottogruppi_after, fn_fp_sottogruppi_after, len(y_train_mitigated), len(y_pred_test_filtered_fp_after)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(y_train_mitigated_random), len(y_pred_test_filtered_fp_after_random)]\n",
    "})\n",
    "metrics_after_fp_sottogruppi = metrics_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi[metric] = metrics_after_fp_sottogruppi[metric].astype(int)\n",
    "\n",
    "metrics_after_fp\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp\")\n",
    "metrics_after_fp_sottogruppi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALISI DIVERGENZE\n",
    "\n",
    "\n",
    "Per vedere cosa accade ai sottogruppi: vedo cosa succede alle divergenze del test dopo la mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp)</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.106</td>\n",
       "      <td>2341.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.109</td>\n",
       "      <td>2341.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                          Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                   0.809     0.474             0.040   \n",
       "After Mitigation(K=5 fp)            0.806     0.491             0.036   \n",
       "After RANDOM Mitigation(K=5 fp)     0.809     0.478             0.036   \n",
       "\n",
       "Metrics                          max div  media div primi 10  \\\n",
       "Before Mitigation                  0.245               0.206   \n",
       "After Mitigation(K=5 fp)           0.235               0.198   \n",
       "After RANDOM Mitigation(K=5 fp)    0.241               0.204   \n",
       "\n",
       "Metrics                          media div primi 20  media div primi 40  \\\n",
       "Before Mitigation                             0.166               0.115   \n",
       "After Mitigation(K=5 fp)                      0.157               0.106   \n",
       "After RANDOM Mitigation(K=5 fp)               0.161               0.109   \n",
       "\n",
       "Metrics                          # new samples  \n",
       "Before Mitigation                        0.000  \n",
       "After Mitigation(K=5 fp)              2341.000  \n",
       "After RANDOM Mitigation(K=5 fp)       2341.000  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by=\"error_div\", ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values(\"error_div\", ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "error_div_list_no_mitigation  = df_pruned_error[\"error_div\"].tolist()\n",
    "#error_div_list_no_mitigation\n",
    "\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred \n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by=\"error_div\", ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values(\"error_div\", ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "error_div_list_baseline1  = df_pruned_error[\"error_div\"].tolist()\n",
    "#error_div_list_baseline1\n",
    "\n",
    "#PER VEDERE COSA SUCCEDE ALLE DIVERGENZE DEI SOTTOGRUPPI CON MITIGATION RADOMICA\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by=\"error_div\", ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values(\"error_div\", ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "error_div_list_random_per_confrontare_con_baseline1  = df_pruned_error[\"error_div\"].tolist()\n",
    "#error_div_list_random_per_confrontare_con_baseline1\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_error_div_list_no_mitigation = np.nanmean(error_div_list_no_mitigation)\n",
    "media_error_div_list_nomitigation_primi10 = np.nanmean(error_div_list_no_mitigation[:10])\n",
    "media_error_div_list_nomitigation_primi20 = np.nanmean(error_div_list_no_mitigation[:20])\n",
    "media_error_div_list_nomitigation_primi40 = np.nanmean(error_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_error_div_no_mitigation = max(abs(x) for x in error_div_list_no_mitigation)\n",
    "\n",
    "media_error_div_list_baseline1 = np.nanmean(error_div_list_baseline1)\n",
    "media_error_div_list_baseline1_primi10 = np.nanmean(error_div_list_baseline1[:10])\n",
    "media_error_div_list_baseline1_primi20 = np.nanmean(error_div_list_baseline1[:20])\n",
    "media_error_div_list_baseline1_primi40 = np.nanmean(error_div_list_baseline1[:40])\n",
    "error_div_massimo_valore_assoluto_error_div_baseline1 = max(abs(x) for x in error_div_list_baseline1)\n",
    "\n",
    "media_error_div_list_random_per_confrontare_con_baseline1 = np.nanmean(error_div_list_random_per_confrontare_con_baseline1)\n",
    "media_error_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(error_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_error_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(error_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_error_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(error_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_error_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in error_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_correctness_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_error_div_list_no_mitigation, massimo_valore_assoluto_error_div_no_mitigation,\n",
    "        media_error_div_list_nomitigation_primi10, media_error_div_list_nomitigation_primi20, media_error_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after, f1_score_fp_after, media_error_div_list_baseline1, error_div_massimo_valore_assoluto_error_div_baseline1,\n",
    "        media_error_div_list_baseline1_primi10, media_error_div_list_baseline1_primi20, media_error_div_list_baseline1_primi40, len(df_holdout_filtered)\n",
    "    ],\n",
    "    'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_error_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_error_div_random_per_confrontare_con_baseline1, media_error_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_error_div_list_random_per_confrontare_con_baseline1_primi20, media_error_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        len(df_holdout_filtered)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_correctness_sottogruppi = divergence_after_correctness_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_correctness_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIAS MITIGATION SIMULANDO DATI ATTRAVERSO SMOTE\n",
    "\n",
    "SEGUE CODICE USANDO SMOTE \n",
    "DIVIDO IN TRAIN, TEST E VALIDATION -- ora uso quelli gia esistenti\n",
    "DIV EXPLORER SUL VALIDATION  -- già fatto \n",
    "GENERO NUOVI DATI CON SMOTE a partire dai dati di divexplorer sul validation\n",
    "INSERISCO QUESTI NUOVI DATI NEL TRAIN SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Righe del dataset filtrato qunado K = 5 1866\n",
      "numero di dati simulati con smotenc 1880\n",
      "income\n",
      "1    940\n",
      "0    940\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "df_val_filtered = K_subgroups_dataset_and_or(df_pruned_error, df_val, K)\n",
    "print(\"Righe del dataset filtrato qunado K = 5\",len(df_val_filtered))\n",
    "#df_val_filtered.head() #var categoriche e numeriche \n",
    "#print(len(df_val_filtered)) #--2610 con = 5\n",
    "df_val_filtered, inutile12, inutile222 = encoding_funct_SMOTE(df_val_filtered, df_test, df_holdout)\n",
    "X_to_SMOTE =  df_val_filtered.drop(columns = ['error', 'y_pred', 'income'], axis = 1)\n",
    "y_to_SMOTE = df_val_filtered['income']\n",
    "X_to_SMOTE.head()\n",
    "\n",
    "categorical_features = [0, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13]\n",
    "\n",
    "smote_nc = SMOTENC( categorical_features=categorical_features, random_state=seed)\n",
    "X_resampled, y_resampled = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    "\n",
    "print(\"numero di dati simulati con smotenc\",len(y_resampled))\n",
    "\n",
    "class_counts = y_resampled.value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14894\n"
     ]
    }
   ],
   "source": [
    "X_train_mitigated_SMOTE = pd.concat([X_train, X_resampled], ignore_index=True)\n",
    "y_train_mitigated_SMOTE = pd.concat([y_train, y_resampled], ignore_index=True)\n",
    "print(len(X_train_mitigated_SMOTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "classifier_train_mitigated_SMOTE = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_SMOTE.fit(X_train_mitigated_SMOTE, y_train_mitigated_SMOTE)\n",
    "y_mitigated_SMOTE_pred = classifier_train_mitigated_SMOTE.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#vediamo che succede se prendo lo stesso numero di righe ma random da holdout\\nprint(len(X_resampled))\\nn_random_smote = len(X_resampled)\\n\\ndf_holdout_smote_sampled = df_holdout_enc.sample(n=n_random_smote, random_state=seed)\\nprint(\"verifica :\", len(df_holdout_smote_sampled)) #verifica\\n\\ndf_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\\ndf_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\\n\\nX_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\\ny_train_mitigated_random_smote = df_train_mitigated_random_smote[\\'income\\']\\n\\nclassifier_train_mitigated_random_smote = LogisticRegression(random_state=seed)\\n\\nclassifier_train_mitigated_random_smote.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\\ny_mitigated_pred_random_smote = classifier_train_mitigated_random_smote.predict(X_test)\\n\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#vediamo che succede se prendo lo stesso numero di righe ma random da holdout\n",
    "print(len(X_resampled))\n",
    "n_random_smote = len(X_resampled)\n",
    "\n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=n_random_smote, random_state=seed)\n",
    "print(\"verifica :\", len(df_holdout_smote_sampled)) #verifica\n",
    "\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "\n",
    "classifier_train_mitigated_random_smote = LogisticRegression(random_state=seed)\n",
    "\n",
    "classifier_train_mitigated_random_smote.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote = classifier_train_mitigated_random_smote.predict(X_test)\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"accuracy_fp_after_SMOTE, f1_score_fp_after_SMOTE, fpr_fp_after_SMOTE, fnr_fp_after_SMOTE, fp_fp_after_SMOTE, fn_fp_after_SMOTE = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_SMOTE_pred )\\naccuracy_fp_after_SMOTE_random, f1_score_fp_after_SMOTE_random, fpr_fp_after_SMOTE_random, fnr_fp_after_SMOTE_random, fp_fp_after_SMOTE_random, fn_fp_after_SMOTE_random = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote )\\n\\n\\nmetrics_after_fp_SMOTE = pd.DataFrame({\\n    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\\n    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\\n    'After SMOTE fp mitigation' : [accuracy_fp_after_SMOTE, f1_score_fp_after_SMOTE, fpr_fp_after_SMOTE, fnr_fp_after_SMOTE, fp_fp_after_SMOTE, fn_fp_after_SMOTE, len(y_train_mitigated_SMOTE), len(y_mitigated_SMOTE_pred)],\\n    'After RANDOM mitigation' : [accuracy_fp_after_SMOTE_random, f1_score_fp_after_SMOTE_random, fpr_fp_after_SMOTE_random, fnr_fp_after_SMOTE_random, fp_fp_after_SMOTE_random, fn_fp_after_SMOTE_random, len(y_train_mitigated_random_smote), len(y_mitigated_pred_random_smote)]\\n    \\n})\\nmetrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\\nmetrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\\nfor metric in metrics_to_cast:\\n    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\\n    \\nmetrics_after_fp_SMOTE\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''accuracy_fp_after_SMOTE, f1_score_fp_after_SMOTE, fpr_fp_after_SMOTE, fnr_fp_after_SMOTE, fp_fp_after_SMOTE, fn_fp_after_SMOTE = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_SMOTE_pred )\n",
    "accuracy_fp_after_SMOTE_random, f1_score_fp_after_SMOTE_random, fpr_fp_after_SMOTE_random, fnr_fp_after_SMOTE_random, fp_fp_after_SMOTE_random, fn_fp_after_SMOTE_random = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote )\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After SMOTE fp mitigation' : [accuracy_fp_after_SMOTE, f1_score_fp_after_SMOTE, fpr_fp_after_SMOTE, fnr_fp_after_SMOTE, fp_fp_after_SMOTE, fn_fp_after_SMOTE, len(y_train_mitigated_SMOTE), len(y_mitigated_SMOTE_pred)],\n",
    "    'After RANDOM mitigation' : [accuracy_fp_after_SMOTE_random, f1_score_fp_after_SMOTE_random, fpr_fp_after_SMOTE_random, fnr_fp_after_SMOTE_random, fp_fp_after_SMOTE_random, fn_fp_after_SMOTE_random, len(y_train_mitigated_random_smote), len(y_mitigated_pred_random_smote)]\n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "    \n",
    "metrics_after_fp_SMOTE'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A QUESTO PUNTO POSSIAMO VEDERE LE PERFORMANCE SUI SOTTOGRUPPI PRIMA E DOPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \\ny_pred_test_filtered_fp_after_SMOTE = classifier_train_mitigated_SMOTE.predict(X_test_filtered_fp)\\ny_pred_RANDOM_subgroups = classifier_train_mitigated_random_smote.predict(X_test_filtered_fp)\\n\\n\\n#accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\\naccuracy_fp_sottogruppi_after_SMOTE, f1_score_fp_sottogruppi_after_SMOTE, fpr_fp_sottogruppi_after_SMOTE, fnr_fp_sottogruppi_after_SMOTE, fp_fp_sottogruppi_after_SMOTE, fn_fp_sottogruppi_after_SMOTE = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE )\\naccuracy_fp_sottogruppi_random_SMOTE, f1_score_fp_sottogruppi_random_SMOTE, fpr_fp_sottogruppi_random_SMOTE, fnr_fp_sottogruppi_random_SMOTE, fp_fp_sottogruppi_random_SMOTE, fn_fp_sottogruppi_random_SMOTE = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_RANDOM_subgroups )\\n\\nmetrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\\n    \\'Metrics\\' : [\\'Accuracy\\', \\'F1 Score\\', \\'False Positive Rate\\', \\'False Negative Rate\\', \\'False Positives\\', \\'False Negatives\\', \\'Train Size\\', \\'Test Size\\'],\\n    \\'Before Mitigation, on subgroups\\' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\\n    \\'After RANDOM mitigation, on subgroups\\' : [accuracy_fp_sottogruppi_random_SMOTE, f1_score_fp_sottogruppi_random_SMOTE, fpr_fp_sottogruppi_random_SMOTE, fnr_fp_sottogruppi_random_SMOTE, fp_fp_sottogruppi_random_SMOTE, fn_fp_sottogruppi_random_SMOTE, len(y_train), len(y_pred_RANDOM_subgroups)],\\n    \\'After Mitigation(K=5, on subgroups, fp and SMOTE)\\': [accuracy_fp_sottogruppi_after_SMOTE, f1_score_fp_sottogruppi_after_SMOTE, fpr_fp_sottogruppi_after_SMOTE, fnr_fp_sottogruppi_after_SMOTE, fp_fp_sottogruppi_after_SMOTE, fn_fp_sottogruppi_after_SMOTE, len(y_train_mitigated_SMOTE), len(y_pred_test_filtered_fp_after_SMOTE)],\\n})\\nmetrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index(\\'Metrics\\').T\\n\\nmetrics_to_cast = [\\'False Positives\\', \\'False Negatives\\', \\'Train Size\\', \\'Test Size\\']\\n\\nfor metric in metrics_to_cast:\\n    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\\n\\nmetrics_after_fp_SMOTE\\n\\n\\nprint(\"Subgroups Decision Tree performance when boolean outcomes = correctness e SMOTE \")\\nmetrics_after_fp_sottogruppi_SMOTE'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "y_pred_test_filtered_fp_after_SMOTE = classifier_train_mitigated_SMOTE.predict(X_test_filtered_fp)\n",
    "y_pred_RANDOM_subgroups = classifier_train_mitigated_random_smote.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "#accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE, f1_score_fp_sottogruppi_after_SMOTE, fpr_fp_sottogruppi_after_SMOTE, fnr_fp_sottogruppi_after_SMOTE, fp_fp_sottogruppi_after_SMOTE, fn_fp_sottogruppi_after_SMOTE = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE )\n",
    "accuracy_fp_sottogruppi_random_SMOTE, f1_score_fp_sottogruppi_random_SMOTE, fpr_fp_sottogruppi_random_SMOTE, fnr_fp_sottogruppi_random_SMOTE, fp_fp_sottogruppi_random_SMOTE, fn_fp_sottogruppi_random_SMOTE = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_RANDOM_subgroups )\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM mitigation, on subgroups' : [accuracy_fp_sottogruppi_random_SMOTE, f1_score_fp_sottogruppi_random_SMOTE, fpr_fp_sottogruppi_random_SMOTE, fnr_fp_sottogruppi_random_SMOTE, fp_fp_sottogruppi_random_SMOTE, fn_fp_sottogruppi_random_SMOTE, len(y_train), len(y_pred_RANDOM_subgroups)],\n",
    "    'After Mitigation(K=5, on subgroups, fp and SMOTE)': [accuracy_fp_sottogruppi_after_SMOTE, f1_score_fp_sottogruppi_after_SMOTE, fpr_fp_sottogruppi_after_SMOTE, fnr_fp_sottogruppi_after_SMOTE, fp_fp_sottogruppi_after_SMOTE, fn_fp_sottogruppi_after_SMOTE, len(y_train_mitigated_SMOTE), len(y_pred_test_filtered_fp_after_SMOTE)],\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = correctness e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIAS MITIGATION SMOTENC\n",
    "\n",
    "- FISSO N VARIA p, p è la probabilità che il campione simulato sia di classe 0 qui (perchè voglio diminuire il numero di falsi positivi)\n",
    "- FISSO p VARIA N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Righe del dataset filtrato qunado K = 5 1866\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "df_val_filtered = K_subgroups_dataset_and_or(df_pruned_error, df_val, K)\n",
    "print(\"Righe del dataset filtrato qunado K = 5\",len(df_val_filtered))\n",
    "#df_val_filtered.head() #var categoriche e numeriche \n",
    "#print(len(df_val_filtered)) #--2610 con = 5\n",
    "df_val_filtered, inutile12, inutile222 = encoding_funct_SMOTE(df_val_filtered, df_test, df_holdout)\n",
    "X_to_SMOTE =  df_val_filtered.drop(columns = [ 'y_pred', 'error', 'income'], axis = 1)\n",
    "y_to_SMOTE = df_val_filtered['income']\n",
    "X_to_SMOTE.head()\n",
    "\n",
    "categorical_features = [0, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(940, 926)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_1 = df_val_filtered['income'].sum()\n",
    "count_0 = len(df_val_filtered) - count_1\n",
    "count_1, count_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "per confronto con targeted\n",
    "N come len_df_holdout_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0=0\n",
    "p1 = 0.1\n",
    "p2 = 0.2\n",
    "p3 = 0.3 \n",
    "p4 = 0.4\n",
    "p5 = 0.5\n",
    "p6 = 0.6\n",
    "p7 = 0.7\n",
    "p8 = 0.8\n",
    "p9 = 0.9\n",
    "p10 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df_holdout_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.109</td>\n",
       "      <td>2341.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.799</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.066</td>\n",
       "      <td>2341.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.095</td>\n",
       "      <td>2341.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.131</td>\n",
       "      <td>2341.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.801</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.137</td>\n",
       "      <td>2341.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.040   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.478             0.036   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.799     0.549             0.031   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.806     0.498             0.031   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.806     0.430             0.051   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.801     0.368             0.050   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.245               0.206   \n",
       "After RANDOM Mitigation(K=5 fp)            0.241               0.204   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.224               0.161   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.236               0.183   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.262               0.226   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.286               0.240   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.166   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.161   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.128   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.145   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.189   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.195   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.115          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.109       2341.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.066       2341.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.095       2341.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.131       2341.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.137       2341.000  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = len(df_holdout_filtered)\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_2K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.105</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.803</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.074</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.095</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.130</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.801</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.134</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.040   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.478             0.034   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.803     0.547             0.030   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.806     0.495             0.030   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.806     0.435             0.049   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.801     0.378             0.049   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.245               0.206   \n",
       "After RANDOM Mitigation(K=5 fp)            0.242               0.202   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.216               0.152   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.240               0.185   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.261               0.225   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.280               0.236   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.166   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.158   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.123   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.146   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.187   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.191   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.115          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.105       2000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.074       2000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.095       2000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.130       2000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.134       2000.000  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 2000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_2K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation N = 5000</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.641</td>\n",
       "      <td>246</td>\n",
       "      <td>1005</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0</th>\n",
       "      <td>0.799</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.452</td>\n",
       "      <td>601</td>\n",
       "      <td>709</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.1</th>\n",
       "      <td>0.801</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.473</td>\n",
       "      <td>556</td>\n",
       "      <td>742</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.2</th>\n",
       "      <td>0.803</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.506</td>\n",
       "      <td>491</td>\n",
       "      <td>793</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.3</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.523</td>\n",
       "      <td>431</td>\n",
       "      <td>820</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.4</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.552</td>\n",
       "      <td>378</td>\n",
       "      <td>866</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.5</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.605</td>\n",
       "      <td>316</td>\n",
       "      <td>948</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.6</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.638</td>\n",
       "      <td>263</td>\n",
       "      <td>1000</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.7</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.659</td>\n",
       "      <td>217</td>\n",
       "      <td>1034</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.8</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.690</td>\n",
       "      <td>178</td>\n",
       "      <td>1082</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.9</th>\n",
       "      <td>0.803</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.728</td>\n",
       "      <td>143</td>\n",
       "      <td>1142</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 4000 p_class 0 = 1</th>\n",
       "      <td>0.801</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.749</td>\n",
       "      <td>119</td>\n",
       "      <td>1175</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                               Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation                        0.809     0.474                0.047   \n",
       "After RANDOM mitigation N = 5000         0.808     0.474                0.050   \n",
       "After SMOTE N = 5000 p_class 0 = 0       0.799     0.567                0.122   \n",
       "After SMOTE N = 5000 p_class 0 = 0.1     0.801     0.560                0.113   \n",
       "After SMOTE N = 5000 p_class 0 = 0.2     0.803     0.547                0.099   \n",
       "After SMOTE N = 5000 p_class 0 = 0.3     0.808     0.545                0.087   \n",
       "After SMOTE N = 5000 p_class 0 = 0.4     0.809     0.530                0.077   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5     0.806     0.495                0.064   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6     0.806     0.474                0.053   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7     0.808     0.461                0.044   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8     0.806     0.435                0.036   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9     0.803     0.399                0.029   \n",
       "After SMOTE N = 4000 p_class 0 = 1       0.801     0.378                0.024   \n",
       "\n",
       "Metrics                               False Negative Rate  False Positives  \\\n",
       "Before Mitigation                                   0.643              234   \n",
       "After RANDOM mitigation N = 5000                    0.641              246   \n",
       "After SMOTE N = 5000 p_class 0 = 0                  0.452              601   \n",
       "After SMOTE N = 5000 p_class 0 = 0.1                0.473              556   \n",
       "After SMOTE N = 5000 p_class 0 = 0.2                0.506              491   \n",
       "After SMOTE N = 5000 p_class 0 = 0.3                0.523              431   \n",
       "After SMOTE N = 5000 p_class 0 = 0.4                0.552              378   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5                0.605              316   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6                0.638              263   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7                0.659              217   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8                0.690              178   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9                0.728              143   \n",
       "After SMOTE N = 4000 p_class 0 = 1                  0.749              119   \n",
       "\n",
       "Metrics                               False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                                1008       13014       6508  \n",
       "After RANDOM mitigation N = 5000                 1005       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0                709       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.1              742       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.2              793       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.3              820       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.4              866       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.5              948       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.6             1000       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.7             1034       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.8             1082       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.9             1142       15014       6508  \n",
       "After SMOTE N = 4000 p_class 0 = 1               1175       15014       6508  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_after_fp_SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.113</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.798</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.060</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.807</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.102</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.804</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.138</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.798</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.140</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.040   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.478             0.043   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.798     0.563             0.031   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.807     0.511             0.031   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.804     0.415             0.055   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.798     0.347             0.051   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.245               0.206   \n",
       "After RANDOM Mitigation(K=5 fp)            0.244               0.204   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.211               0.140   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.227               0.185   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.275               0.237   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.296               0.242   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.166   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.164   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.113   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.148   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.198   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.199   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.115          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.113       3000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.060       3000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.102       3000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.138       3000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.140       3000.000  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 3000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_3K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.105</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.794</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.044</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.807</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.103</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.801</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.146</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.795</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.148</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.040   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.478             0.035   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.794     0.571             0.033   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.807     0.518             0.033   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.801     0.401             0.056   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.795     0.318             0.053   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.245               0.206   \n",
       "After RANDOM Mitigation(K=5 fp)            0.237               0.200   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.216               0.130   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.218               0.179   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.283               0.244   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.320               0.254   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.166   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.158   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.103   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.147   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.208   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.209   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.115          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.105       4000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.044       4000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.103       4000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.146       4000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.148       4000.000  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 4000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_4K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 5000 p changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.113</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.789</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.047</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.802</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.063</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.801</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.141</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.793</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.147</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.040   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.478             0.040   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.789     0.578             0.011   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.802     0.526             0.011   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.801     0.394             0.054   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.793     0.301             0.053   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.245               0.206   \n",
       "After RANDOM Mitigation(K=5 fp)            0.248               0.204   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.214               0.129   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.205               0.145   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.286               0.241   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.291               0.248   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.166   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.165   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.103   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.117   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.203   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.207   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.115          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.113       5000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.047       5000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.063       5000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.141       5000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.147       5000.000  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 5000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_5K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation N = 5000</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.633</td>\n",
       "      <td>258</td>\n",
       "      <td>992</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0</th>\n",
       "      <td>0.762</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.311</td>\n",
       "      <td>1062</td>\n",
       "      <td>488</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.1</th>\n",
       "      <td>0.780</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.374</td>\n",
       "      <td>844</td>\n",
       "      <td>586</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.2</th>\n",
       "      <td>0.789</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.401</td>\n",
       "      <td>742</td>\n",
       "      <td>629</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.3</th>\n",
       "      <td>0.798</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.447</td>\n",
       "      <td>612</td>\n",
       "      <td>701</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.4</th>\n",
       "      <td>0.802</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.502</td>\n",
       "      <td>503</td>\n",
       "      <td>787</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.5</th>\n",
       "      <td>0.802</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.543</td>\n",
       "      <td>438</td>\n",
       "      <td>852</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.6</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.627</td>\n",
       "      <td>280</td>\n",
       "      <td>983</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.7</th>\n",
       "      <td>0.805</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.677</td>\n",
       "      <td>210</td>\n",
       "      <td>1061</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.8</th>\n",
       "      <td>0.801</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.731</td>\n",
       "      <td>151</td>\n",
       "      <td>1146</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.9</th>\n",
       "      <td>0.798</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.775</td>\n",
       "      <td>97</td>\n",
       "      <td>1215</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 4000 p_class 0 = 1</th>\n",
       "      <td>0.793</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.815</td>\n",
       "      <td>68</td>\n",
       "      <td>1278</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                               Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation                        0.809     0.474                0.047   \n",
       "After RANDOM mitigation N = 5000         0.808     0.480                0.052   \n",
       "After SMOTE N = 5000 p_class 0 = 0       0.762     0.582                0.215   \n",
       "After SMOTE N = 5000 p_class 0 = 0.1     0.780     0.579                0.171   \n",
       "After SMOTE N = 5000 p_class 0 = 0.2     0.789     0.578                0.150   \n",
       "After SMOTE N = 5000 p_class 0 = 0.3     0.798     0.569                0.124   \n",
       "After SMOTE N = 5000 p_class 0 = 0.4     0.802     0.548                0.102   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5     0.802     0.526                0.089   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6     0.806     0.481                0.057   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7     0.805     0.444                0.043   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8     0.801     0.394                0.031   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9     0.798     0.350                0.020   \n",
       "After SMOTE N = 4000 p_class 0 = 1       0.793     0.301                0.014   \n",
       "\n",
       "Metrics                               False Negative Rate  False Positives  \\\n",
       "Before Mitigation                                   0.643              234   \n",
       "After RANDOM mitigation N = 5000                    0.633              258   \n",
       "After SMOTE N = 5000 p_class 0 = 0                  0.311             1062   \n",
       "After SMOTE N = 5000 p_class 0 = 0.1                0.374              844   \n",
       "After SMOTE N = 5000 p_class 0 = 0.2                0.401              742   \n",
       "After SMOTE N = 5000 p_class 0 = 0.3                0.447              612   \n",
       "After SMOTE N = 5000 p_class 0 = 0.4                0.502              503   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5                0.543              438   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6                0.627              280   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7                0.677              210   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8                0.731              151   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9                0.775               97   \n",
       "After SMOTE N = 4000 p_class 0 = 1                  0.815               68   \n",
       "\n",
       "Metrics                               False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                                1008       13014       6508  \n",
       "After RANDOM mitigation N = 5000                  992       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0                488       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.1              586       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.2              629       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.3              701       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.4              787       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.5              852       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.6              983       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.7             1061       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.8             1146       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.9             1215       18014       6508  \n",
       "After SMOTE N = 4000 p_class 0 = 1               1278       18014       6508  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_after_fp_SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 6000 p changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.107</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.783</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.039</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.801</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.077</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.802</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.138</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.792</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.157</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.040   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.478             0.036   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.783     0.577             0.019   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.801     0.520             0.019   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.802     0.406             0.053   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.792     0.280             0.056   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.245               0.206   \n",
       "After RANDOM Mitigation(K=5 fp)            0.241               0.200   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.211               0.122   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.201               0.158   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.283               0.239   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.299               0.264   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.166   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.158   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.099   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.126   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.196   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.221   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.115          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.107       6000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.039       6000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.077       6000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.138       6000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.157       6000.000  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 6000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_6K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.114</td>\n",
       "      <td>7000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.777</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.032</td>\n",
       "      <td>7000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.802</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.071</td>\n",
       "      <td>7000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.144</td>\n",
       "      <td>7000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.790</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.166</td>\n",
       "      <td>7000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.040   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.478             0.039   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.777     0.575             0.017   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.802     0.531             0.017   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.800     0.386             0.054   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.790     0.267             0.060   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.245               0.206   \n",
       "After RANDOM Mitigation(K=5 fp)            0.243               0.208   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.213               0.123   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.198               0.154   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.288               0.244   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.308               0.271   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.166   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.166   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.098   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.123   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.205   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.229   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.115          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.114       7000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.032       7000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.071       7000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.144       7000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.166       7000.000  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 7000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_6K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.109</td>\n",
       "      <td>8000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.771</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.028</td>\n",
       "      <td>8000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.803</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.059</td>\n",
       "      <td>8000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.799</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.139</td>\n",
       "      <td>8000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.790</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.167</td>\n",
       "      <td>8000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.040   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.478             0.037   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.771     0.573             0.012   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.803     0.541             0.012   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.799     0.384             0.053   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.790     0.263             0.060   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.245               0.206   \n",
       "After RANDOM Mitigation(K=5 fp)            0.241               0.206   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.219               0.118   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.199               0.145   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.289               0.240   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.311               0.273   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.166   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.162   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.095   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.116   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.199   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.231   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.115          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.109       8000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.028       8000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.059       8000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.139       8000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.167       8000.000  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 8000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_6K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.111</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.768</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.033</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.055</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.798</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.147</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.790</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.165</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.040   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.478             0.037   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.768     0.577             0.009   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.800     0.532             0.009   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.798     0.367             0.055   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.790     0.260             0.060   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.245               0.206   \n",
       "After RANDOM Mitigation(K=5 fp)            0.243               0.201   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.222               0.119   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.197               0.138   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.298               0.245   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.312               0.275   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.166   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.161   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.095   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.110   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.206   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.231   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.115          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.111       9000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.033       9000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.055       9000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.147       9000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.165       9000.000  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 9000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_6K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creazione del grafico\n",
    "plt.figure(figsize=(10, 6))\n",
    "p =[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# Linea per i falsi positivi\n",
    "\n",
    "plt.plot(p, errors_after_2K, marker='o', label='Error 2K ', color='black')\n",
    "plt.plot(p, errors_after_3K, marker='o', label='Error 3K ', color='red')\n",
    "plt.plot(p, errors_after_4K, marker='o', label='Error 4K ', color='green')\n",
    "plt.plot(p, errors_after_5K, marker='o', label='Error 5K ', color='blue')\n",
    "plt.plot(p, errors_after_6K,  marker='o', label='Error 6K ', color='orange')\n",
    "\n",
    "\n",
    "plt.axhline(y=errors_before, color='purple', linestyle='--', label='Initial Errors')\n",
    "\n",
    "# Etichette e titolo\n",
    "plt.xlabel('probability for the synthetic point to belong to the class 0')\n",
    "plt.ylabel('Count Errors')\n",
    "plt.title(f'Errors, err, #subgroups = {K}, support = {min_sup} on {filtered_instances}, redundancy = 0.01')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.yticks(range(1175, 1350, 25))\n",
    "plt.xticks(np.arange(0, 1.1, 0.1)) \n",
    "# Mostra il grafico\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(940, 926)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_1, count_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
