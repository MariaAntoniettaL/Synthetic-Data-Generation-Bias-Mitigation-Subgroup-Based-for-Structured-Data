{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_for_adult import preprocessing_funct_not_enc, encoding_funct, K_subgroups_dataset_and_or, metrics_to_compare,encoding_funct_SMOTE\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "\n",
    "from divexplorer import DivergenceExplorer\n",
    "from divexplorer import DivergencePatternProcessor\n",
    "\n",
    "from divexplorer.outcomes import get_false_negative_rate_outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "df = pd.read_csv(\"adult.data\", header = None, names = col_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "pd.options.display.float_format = '{:.3f}'.format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup = 0.4\n",
    "percentage =  15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET ROWS:  13014\n",
      "VALIDATION SET ROWS:  6507\n",
      "HOLDOUT SET ROWS:  6508\n",
      "TEST SET ROWS:  6508\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val, df_test, df_holdout = preprocessing_funct_not_enc(df)\n",
    "#controllo divisione dataset\n",
    "print(f\"TRAIN SET ROWS: \", df_train.shape[0]) #su 32536, il 40%  dovrebbe essere circa 13014\n",
    "print(f\"VALIDATION SET ROWS: \", df_val.shape[0]) #su 32536, il 20%  dovrebbe essere circa 6500\n",
    "print(f\"HOLDOUT SET ROWS: \", df_holdout.shape[0])\n",
    "print(f\"TEST SET ROWS: \", df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET ROWS:  13014\n",
      "VALIDATION SET ROWS:  6507\n",
      "HOLDOUT SET ROWS:  6508\n",
      "TEST SET ROWS:  6508\n"
     ]
    }
   ],
   "source": [
    "df_train_enc, df_test_enc, df_holdout_enc, df_val_enc = encoding_funct(df_train=df_train, df_test=df_test, df_holdout=df_holdout, df_val=df_val)\n",
    "#controllo coerenza con numerosità precedente\n",
    "print(f\"TRAIN SET ROWS: \", df_train_enc.shape[0]) #su 32536, il 40%  dovrebbe essere circa 13014\n",
    "print(f\"VALIDATION SET ROWS: \", df_val_enc.shape[0]) #su 32536, il 20%  dovrebbe essere circa 6500\n",
    "print(f\"HOLDOUT SET ROWS: \", df_holdout_enc.shape[0])\n",
    "print(f\"TEST SET ROWS: \", df_test_enc.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#GDecisioN tree predictions\n",
    "X_train = df_train_enc.drop(columns = 'income', axis = 1)\n",
    "y_train = df_train_enc['income']\n",
    "\n",
    "X_test = df_test_enc.drop(columns = 'income', axis = 1)\n",
    "y_test = df_test_enc['income']\n",
    "\n",
    "X_val = df_val_enc.drop(columns = 'income', axis = 1)\n",
    "y_val = df_val_enc['income']\n",
    "\n",
    "X_holdout = df_holdout_enc.drop(columns = 'income', axis = 1)\n",
    "y_holdout = df_holdout_enc['income']\n",
    "\n",
    "classifier_train = LogisticRegression(random_state=seed)\n",
    "classifier_train.fit(X_train, y_train)\n",
    "y_pred = classifier_train.predict(X_test)\n",
    "cm_classifier = confusion_matrix(y_test, y_pred)\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=cm_classifier, display_labels=[False, True])\n",
    "#disp.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics            Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation     0.809     0.474                0.047   \n",
       "\n",
       "Metrics            False Negative Rate  False Positives  False Negatives  \\\n",
       "Before Mitigation                0.643              234             1008   \n",
       "\n",
       "Metrics            Test Size  \n",
       "Before Mitigation       6508  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before = metrics_to_compare(y_true = y_test, y_pred = y_pred )\n",
    "metrics_before_df = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Test Size'],\n",
    "    'Before Mitigation': [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_test)],\n",
    "})\n",
    "metrics_before_df = metrics_before_df.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_before_df[metric] = metrics_before_df[metric].astype(int)\n",
    "\n",
    "metrics_before_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBGROUPS SEARCH\n",
    "\n",
    "-Identifico i gruppi applicando DivExplorer sul validation not encoded (a cui ho aggiunto la feature sui falsi positivi da passare a boolean outcomes e la feature accuracy che vale 1 se la predizione è giusta e 0 se sbagliata )\n",
    "\n",
    "-Integro nel training set dati che matchano sottogruppi problematici prendendoli dall'holdout, (primi K = 5, tutte le righe holdout che matchano)\n",
    "\n",
    "-Ripeto training e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>native-country</th>\n",
       "      <th>age_group</th>\n",
       "      <th>edu_num_group</th>\n",
       "      <th>hours_per_week_group</th>\n",
       "      <th>y_val_true</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18761</th>\n",
       "      <td>2</td>\n",
       "      <td>0.077</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27582</th>\n",
       "      <td>3</td>\n",
       "      <td>0.048</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30911</th>\n",
       "      <td>2</td>\n",
       "      <td>0.174</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11128</th>\n",
       "      <td>0</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.507</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>0</td>\n",
       "      <td>0.284</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       workclass  fnlwgt  education  marital-status  occupation  relationship  \\\n",
       "18761          2   0.077          3               0           4             1   \n",
       "27582          3   0.048          3               1           4             0   \n",
       "30911          2   0.174          3               3           4             4   \n",
       "11128          0   0.012          2               1           2             0   \n",
       "683            0   0.284          3               3           2             4   \n",
       "\n",
       "       race  sex  capital-gain  capital-loss  native-country  age_group  \\\n",
       "18761     4    0         0.000         0.000               5          2   \n",
       "27582     4    1         0.000         0.000               5          5   \n",
       "30911     2    0         0.039         0.000               5          1   \n",
       "11128     2    1         0.000         0.507               5          3   \n",
       "683       2    1         0.000         0.000               5          0   \n",
       "\n",
       "       edu_num_group  hours_per_week_group  y_val_true  y_pred  \n",
       "18761              1                     1           0       0  \n",
       "27582              1                     2           1       0  \n",
       "30911              1                     1           0       0  \n",
       "11128              4                     1           1       1  \n",
       "683                1                     1           0       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predizioni per il validation set\n",
    "y_pred_val_dt = classifier_train.predict(X_val)\n",
    "\n",
    "df_val_class = X_val.copy()\n",
    "df_val_class['y_val_true'] = y_val\n",
    "df_val_class['y_pred'] = y_pred_val_dt\n",
    "\n",
    "df_val_class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "      <th>age_group</th>\n",
       "      <th>edu_num_group</th>\n",
       "      <th>hours_per_week_group</th>\n",
       "      <th>fn</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18761</th>\n",
       "      <td>Private</td>\n",
       "      <td>0.077</td>\n",
       "      <td>Non Graduated</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Self-emp-occ</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>35-44</td>\n",
       "      <td>10 College</td>\n",
       "      <td>Overtime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27582</th>\n",
       "      <td>Self-emp</td>\n",
       "      <td>0.048</td>\n",
       "      <td>Non Graduated</td>\n",
       "      <td>Married</td>\n",
       "      <td>Self-emp-occ</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>65-100</td>\n",
       "      <td>10 College</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30911</th>\n",
       "      <td>Private</td>\n",
       "      <td>0.174</td>\n",
       "      <td>Non Graduated</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Self-emp-occ</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>25-34</td>\n",
       "      <td>10 College</td>\n",
       "      <td>Overtime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11128</th>\n",
       "      <td>Government</td>\n",
       "      <td>0.012</td>\n",
       "      <td>Master's Degree</td>\n",
       "      <td>Married</td>\n",
       "      <td>Private-occ</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.507</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>45-54</td>\n",
       "      <td>14 Master's Degree</td>\n",
       "      <td>Overtime</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>Government</td>\n",
       "      <td>0.284</td>\n",
       "      <td>Non Graduated</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Private-occ</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>17-24</td>\n",
       "      <td>10 College</td>\n",
       "      <td>Overtime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        workclass  fnlwgt        education marital-status    occupation  \\\n",
       "18761     Private   0.077    Non Graduated       Divorced  Self-emp-occ   \n",
       "27582    Self-emp   0.048    Non Graduated        Married  Self-emp-occ   \n",
       "30911     Private   0.174    Non Graduated      Separated  Self-emp-occ   \n",
       "11128  Government   0.012  Master's Degree        Married   Private-occ   \n",
       "683    Government   0.284    Non Graduated      Separated   Private-occ   \n",
       "\n",
       "         relationship    race      sex  capital-gain  capital-loss  \\\n",
       "18761   Not-in-family   White   Female         0.000         0.000   \n",
       "27582         Husband   White     Male         0.000         0.000   \n",
       "30911       Unmarried   Black   Female         0.039         0.000   \n",
       "11128         Husband   Black     Male         0.000         0.507   \n",
       "683         Unmarried   Black     Male         0.000         0.000   \n",
       "\n",
       "      native-country  income age_group       edu_num_group  \\\n",
       "18761  United-States       0     35-44          10 College   \n",
       "27582  United-States       1    65-100          10 College   \n",
       "30911  United-States       0     25-34          10 College   \n",
       "11128  United-States       1     45-54  14 Master's Degree   \n",
       "683    United-States       0     17-24          10 College   \n",
       "\n",
       "      hours_per_week_group    fn  y_pred  accuracy  \n",
       "18761             Overtime   NaN       0         1  \n",
       "27582            Part-time 1.000       0         0  \n",
       "30911             Overtime   NaN       0         1  \n",
       "11128             Overtime 0.000       1         1  \n",
       "683               Overtime   NaN       0         1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trues = df_val_class[\"y_val_true\"]\n",
    "y_preds = df_val_class[\"y_pred\"]\n",
    "\n",
    "df_val_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_val['fn'] = df_val_class['fn']\n",
    "\n",
    "#aggiungo la feature accuracy a df_val non encoded che assume valore 1 se la predizione è giusta 0 se la predizione è sbagliata\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_val['y_pred'] = df_val_class['y_pred'] \n",
    "df_val['accuracy'] = (df_val_class['y_val_true']==df_val_class['y_pred']).astype(int)\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemset</th>\n",
       "      <th>fn</th>\n",
       "      <th>fn_div</th>\n",
       "      <th>fn_t</th>\n",
       "      <th>length</th>\n",
       "      <th>support_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.453</td>\n",
       "      <td>(workclass=Private, education=Non Graduated, capital-loss=0.0, capital-gain=0.0)</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.323</td>\n",
       "      <td>24.196</td>\n",
       "      <td>4</td>\n",
       "      <td>2945.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.442</td>\n",
       "      <td>(hours_per_week_group=Overtime, education=Non Graduated, capital-loss=0.0, capital-gain=0.0)</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.309</td>\n",
       "      <td>22.001</td>\n",
       "      <td>4</td>\n",
       "      <td>2878.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.508</td>\n",
       "      <td>(education=Non Graduated, race= White, capital-loss=0.0, capital-gain=0.0)</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.300</td>\n",
       "      <td>20.269</td>\n",
       "      <td>4</td>\n",
       "      <td>3306.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.608</td>\n",
       "      <td>(education=Non Graduated, capital-loss=0.0, capital-gain=0.0)</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.299</td>\n",
       "      <td>20.556</td>\n",
       "      <td>3</td>\n",
       "      <td>3958.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.464</td>\n",
       "      <td>(education=Non Graduated, native-country=United-States, capital-gain=0.0, race= White, capital-loss=0.0)</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.299</td>\n",
       "      <td>19.946</td>\n",
       "      <td>5</td>\n",
       "      <td>3021.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   support  \\\n",
       "0    0.453   \n",
       "1    0.442   \n",
       "2    0.508   \n",
       "3    0.608   \n",
       "4    0.464   \n",
       "\n",
       "                                                                                                    itemset  \\\n",
       "0                          (workclass=Private, education=Non Graduated, capital-loss=0.0, capital-gain=0.0)   \n",
       "1              (hours_per_week_group=Overtime, education=Non Graduated, capital-loss=0.0, capital-gain=0.0)   \n",
       "2                                (education=Non Graduated, race= White, capital-loss=0.0, capital-gain=0.0)   \n",
       "3                                             (education=Non Graduated, capital-loss=0.0, capital-gain=0.0)   \n",
       "4  (education=Non Graduated, native-country=United-States, capital-gain=0.0, race= White, capital-loss=0.0)   \n",
       "\n",
       "     fn  fn_div   fn_t  length  support_count  \n",
       "0 0.991   0.323 24.196       4       2945.000  \n",
       "1 0.977   0.309 22.001       4       2878.000  \n",
       "2 0.968   0.300 20.269       4       3306.000  \n",
       "3 0.967   0.299 20.556       3       3958.000  \n",
       "4 0.967   0.299 19.946       5       3021.000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_diver = DivergenceExplorer(df_val)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by=[\"fn_div\", \"fn_t\"], ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "FP_fm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemset</th>\n",
       "      <th>length</th>\n",
       "      <th>support_count</th>\n",
       "      <th>fn</th>\n",
       "      <th>fn_div</th>\n",
       "      <th>fn_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.453</td>\n",
       "      <td>(workclass=Private, education=Non Graduated, capital-loss=0.0, capital-gain=0.0)</td>\n",
       "      <td>4</td>\n",
       "      <td>2945.000</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.323</td>\n",
       "      <td>24.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.442</td>\n",
       "      <td>(hours_per_week_group=Overtime, education=Non Graduated, capital-loss=0.0, capital-gain=0.0)</td>\n",
       "      <td>4</td>\n",
       "      <td>2878.000</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.309</td>\n",
       "      <td>22.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.608</td>\n",
       "      <td>(education=Non Graduated, capital-loss=0.0, capital-gain=0.0)</td>\n",
       "      <td>3</td>\n",
       "      <td>3958.000</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.299</td>\n",
       "      <td>20.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.471</td>\n",
       "      <td>(workclass=Private, education=Non Graduated, capital-gain=0.0)</td>\n",
       "      <td>3</td>\n",
       "      <td>3062.000</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.278</td>\n",
       "      <td>16.434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.632</td>\n",
       "      <td>(education=Non Graduated, capital-gain=0.0)</td>\n",
       "      <td>2</td>\n",
       "      <td>4113.000</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.260</td>\n",
       "      <td>15.863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    support  \\\n",
       "0     0.453   \n",
       "1     0.442   \n",
       "3     0.608   \n",
       "8     0.471   \n",
       "12    0.632   \n",
       "\n",
       "                                                                                         itemset  \\\n",
       "0               (workclass=Private, education=Non Graduated, capital-loss=0.0, capital-gain=0.0)   \n",
       "1   (hours_per_week_group=Overtime, education=Non Graduated, capital-loss=0.0, capital-gain=0.0)   \n",
       "3                                  (education=Non Graduated, capital-loss=0.0, capital-gain=0.0)   \n",
       "8                                 (workclass=Private, education=Non Graduated, capital-gain=0.0)   \n",
       "12                                                   (education=Non Graduated, capital-gain=0.0)   \n",
       "\n",
       "    length  support_count    fn  fn_div   fn_t  \n",
       "0        4       2945.000 0.991   0.323 24.196  \n",
       "1        4       2878.000 0.977   0.309 22.001  \n",
       "3        3       3958.000 0.967   0.299 20.556  \n",
       "8        3       3062.000 0.946   0.278 16.434  \n",
       "12       2       4113.000 0.929   0.260 15.863  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values(\"fn_div\", ascending=False)\n",
    "df_pruned_fp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total subgroups 34\n",
      "total problematic 27\n"
     ]
    }
   ],
   "source": [
    "# Numero totale di istanze\n",
    "total_instances = len(df_pruned_fp)\n",
    "\n",
    "# Numero di istanze con fp_div > 0 e fp_t > 2\n",
    "filtered_instances = len(df_pruned_fp[(df_pruned_fp['fn_div'] > 0) & (df_pruned_fp['fn_t'] > 2)])\n",
    "\n",
    "print('total subgroups', total_instances)\n",
    "print('total problematic', filtered_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim original:  (147, 7)\n",
      "Dim pruned th_redundancy  (34, 7)\n"
     ]
    }
   ],
   "source": [
    "prun_size = df_pruned_fp.shape\n",
    "original_size = FP_fm.shape\n",
    "print(\"Dim original: \", original_size)\n",
    "print(\"Dim pruned th_redundancy \", prun_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = int((percentage / 100) * filtered_instances)\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIAS MITIGATION: ADDING DATA: prendo dati dall'hold-out e li aggiungo al train set, questi dati matchano gli itemset trovati prima (i primi 5)\n",
    "\n",
    "1. prendo dati dall'holdout con la funzione K_subgroups_dataset3 li aggiungo train \n",
    "2. riapplico encoding tutto\n",
    "3. Decision tree nuovamente e vedo come sono cambiate le performance (ad es Accuracy, false positive rate, false negative rate) overall e per sottogruppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prima 4059\n",
      "dopo 511\n"
     ]
    }
   ],
   "source": [
    "df_holdout_filtered = K_subgroups_dataset_and_or(df_pruned_fp, df_holdout, K) #da aggiungere a train set e ripetere train e test\n",
    "print('prima', len(df_holdout_filtered))\n",
    "df_holdout_filtered_solo1 = df_holdout_filtered[df_holdout_filtered['income']==1]\n",
    "\n",
    "\n",
    "\n",
    "df_combinated = pd.concat([df_holdout_filtered_solo1, df_train], ignore_index=True)\n",
    "df_train_mitigated= df_combinated.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "df_holdout_filtered = df_holdout_filtered_solo1 \n",
    "\n",
    "print(\"dopo\", len(df_holdout_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET ROWS:  13014\n",
      "TRAIN SET MITIGATED ROWS:  13525\n",
      "VALIDATION SET ROWS:  6508\n",
      "FILTERED DF holdout ROWS:  511\n",
      "TEST SET FILTERED ROWS:  6507\n"
     ]
    }
   ],
   "source": [
    "#riapplico funzione di encoding, ma al posto di holdout, uso il df filtrato che devo usare per inserire dati \n",
    "df_train_enc_mit, inutile1, inutile3, inutile2 = encoding_funct(df_train=df_train_mitigated, df_test=df_test, df_holdout=df_holdout_filtered, df_val=df_val)\n",
    "#controllo divisione dataset\n",
    "df_train_enc_mit_fp = df_train_enc_mit  \n",
    "print(f\"TRAIN SET ROWS: \", df_train_enc.shape[0])\n",
    "print(f\"TRAIN SET MITIGATED ROWS: \", df_train_enc_mit.shape[0]) #su 32536, il 40%  dovrebbe essere circa 13014\n",
    "print(f\"VALIDATION SET ROWS: \", inutile1.shape[0]) #su 32536, il 20%  dovrebbe essere circa 6500\n",
    "print(f\"FILTERED DF holdout ROWS: \", inutile3.shape[0])\n",
    "print(f\"TEST SET FILTERED ROWS: \", inutile2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_mitigated = df_train_enc_mit.drop(columns = 'income', axis = 1)\n",
    "y_train_mitigated = df_train_enc_mit['income']\n",
    "\n",
    "\n",
    "classifier_train_mitigated = LogisticRegression(random_state=seed)\n",
    "\n",
    "classifier_train_mitigated.fit(X_train_mitigated, y_train_mitigated)\n",
    "y_mitigated_pred = classifier_train_mitigated.predict(X_test)\n",
    "cm_dt = confusion_matrix(y_test, y_mitigated_pred)\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=[False, True])\n",
    "#disp.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511\n",
      "verifica : 511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#per veriicare cosa accade se aggiungo in modo randomico lo stesso numero di righe al train, ripeto l'analisi facebdo mitigation con righe randomiche (uguali in numero)\n",
    "print(len(df_holdout_filtered))\n",
    "n = len(df_holdout_filtered)\n",
    "df_holdout_sampled = df_holdout_enc.sample(n=len(df_holdout_filtered), replace=True, random_state=seed)\n",
    "print(\"verifica :\", len(df_holdout_sampled)) #verifica\n",
    "\n",
    "\n",
    "\n",
    "df_combinated_random = pd.concat([df_holdout_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random= df_combinated_random.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "X_train_mitigated_random = df_train_mitigated_random.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random = df_train_mitigated_random['income']\n",
    "\n",
    "classifier_train_mitigated_random = LogisticRegression(random_state=seed)\n",
    "\n",
    "classifier_train_mitigated_random.fit(X_train_mitigated_random, y_train_mitigated_random)\n",
    "y_mitigated_pred_random = classifier_train_mitigated_random.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Gradient Boosting performance when boolean outcomes = fn \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, fp)</th>\n",
       "      <td>0.807</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.591</td>\n",
       "      <td>327</td>\n",
       "      <td>927</td>\n",
       "      <td>13525</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.640</td>\n",
       "      <td>240</td>\n",
       "      <td>1003</td>\n",
       "      <td>13525</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                    Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation             0.809     0.474                0.047   \n",
       "After Mitigation(K=5, fp)     0.807     0.506                0.066   \n",
       "After RANDOM mitigation       0.809     0.476                0.049   \n",
       "\n",
       "Metrics                    False Negative Rate  False Positives  \\\n",
       "Before Mitigation                        0.643              234   \n",
       "After Mitigation(K=5, fp)                0.591              327   \n",
       "After RANDOM mitigation                  0.640              240   \n",
       "\n",
       "Metrics                    False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                     1008       13014       6508  \n",
       "After Mitigation(K=5, fp)              927       13525       6508  \n",
       "After RANDOM mitigation               1003       13525       6508  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "accuracy_fp_after, f1_score_fp_after, fpr_fp_after, fnr_fp_after, fp_fp_after, fn_fp_after = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred )\n",
    "accuracy_fp_after_random, f1_score_fp_after_random, fpr_fp_after_random, fnr_fp_after_random, fp_fp_after_random, fn_fp_after_random= metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random)\n",
    "\n",
    "metrics_after_fp = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After Mitigation(K=5, fp)': [accuracy_fp_after, f1_score_fp_after, fpr_fp_after, fnr_fp_after, fp_fp_after, fn_fp_after, len(y_train_mitigated), len(y_test)],\n",
    "    'After RANDOM mitigation' : [accuracy_fp_after_random, f1_score_fp_after_random, fpr_fp_after_random, fnr_fp_after_random, fp_fp_after_random, fn_fp_after_random, len(y_train_mitigated_random), len(y_test)]\n",
    "})\n",
    "metrics_after_fp = metrics_after_fp.set_index('Metrics').T\n",
    "\n",
    "\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp[metric] = metrics_after_fp[metric].astype(int)\n",
    "print(\"Overall Gradient Boosting performance when boolean outcomes = fn \")\n",
    "metrics_after_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation, on subgroups</th>\n",
       "      <td>0.874</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.955</td>\n",
       "      <td>32</td>\n",
       "      <td>489</td>\n",
       "      <td>13014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp)</th>\n",
       "      <td>0.866</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.881</td>\n",
       "      <td>103</td>\n",
       "      <td>451</td>\n",
       "      <td>13525</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5, on subgroups, fp)</th>\n",
       "      <td>0.874</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.955</td>\n",
       "      <td>32</td>\n",
       "      <td>489</td>\n",
       "      <td>13525</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                         Accuracy  F1 Score  \\\n",
       "Before Mitigation, on subgroups                    0.874     0.081   \n",
       "After Mitigation(K=5, on subgroups, fp)            0.866     0.180   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)     0.874     0.081   \n",
       "\n",
       "Metrics                                         False Positive Rate  \\\n",
       "Before Mitigation, on subgroups                               0.009   \n",
       "After Mitigation(K=5, on subgroups, fp)                       0.028   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                0.009   \n",
       "\n",
       "Metrics                                         False Negative Rate  \\\n",
       "Before Mitigation, on subgroups                               0.955   \n",
       "After Mitigation(K=5, on subgroups, fp)                       0.881   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                0.955   \n",
       "\n",
       "Metrics                                         False Positives  \\\n",
       "Before Mitigation, on subgroups                              32   \n",
       "After Mitigation(K=5, on subgroups, fp)                     103   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)               32   \n",
       "\n",
       "Metrics                                         False Negatives  Train Size  \\\n",
       "Before Mitigation, on subgroups                             489       13014   \n",
       "After Mitigation(K=5, on subgroups, fp)                     451       13525   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)              489       13525   \n",
       "\n",
       "Metrics                                         Test Size  \n",
       "Before Mitigation, on subgroups                      4135  \n",
       "After Mitigation(K=5, on subgroups, fp)              4135  \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)       4135  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_filtered_fp = K_subgroups_dataset_and_or(df_pruned_fp, df_test, K)\n",
    "inutile, df_test_filtered_enc_fp, inutile2, inutile3 = encoding_funct(df_train, df_test_filtered_fp, df_holdout, df_val)\n",
    "\n",
    "X_test_filtered_fp = df_test_filtered_enc_fp.drop(columns='income', axis = 1)\n",
    "y_true_test_filtered_fp = df_test_filtered_enc_fp['income']\n",
    "\n",
    "y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after = classifier_train_mitigated.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_random = classifier_train_mitigated_random.predict(X_test_filtered_fp)\n",
    "\n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after, f1_score_fp_sottogruppi_after, fpr_fp_sottogruppi_after, fnr_fp_sottogruppi_after, fp_fp_sottogruppi_after, fn_fp_sottogruppi_after = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after )\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_random)\n",
    "\n",
    "\n",
    "metrics_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after, f1_score_fp_sottogruppi_after, fpr_fp_sottogruppi_after, fnr_fp_sottogruppi_after, fp_fp_sottogruppi_after, fn_fp_sottogruppi_after, len(y_train_mitigated), len(y_pred_test_filtered_fp_after)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(y_train_mitigated_random), len(y_pred_test_filtered_fp_after_random)]\n",
    "})\n",
    "metrics_after_fp_sottogruppi = metrics_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi[metric] = metrics_after_fp_sottogruppi[metric].astype(int)\n",
    "\n",
    "metrics_after_fp\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fn\")\n",
    "metrics_after_fp_sottogruppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALISI DIVERGENZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per vedere cosa accade ai sottogruppi: vedo cosa succede alle divergenze del test dopo la mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values([\"fn_div\", \"fn_t\"], ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values(\"fn_div\", ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fn_div_list_no_mitigation  = df_pruned_fp[\"fn_div\"].tolist()\n",
    "#fn_div_list_no_mitigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred \n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by=\"fn_div\", ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values(\"fn_div\", ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fn_div_list_baseline1  = df_pruned_fp[\"fn_div\"].tolist()\n",
    "#fn_div_list_baseline1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PER VEDERE COSA SUCCEDE ALLE DIVERGENZE DEI SOTTOGRUPPI CON MITIGATION RADOMICA\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values([\"fn_div\", \"fn_t\"], ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values(\"fn_div\", ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fn_div_list_random_per_confrontare_con_baseline1  = df_pruned_fp[\"fn_div\"].tolist()\n",
    "#fn_div_list_random_per_confrontare_con_baseline1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp)</th>\n",
       "      <td>0.807</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.153</td>\n",
       "      <td>511.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.107</td>\n",
       "      <td>511.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                          Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                   0.809     0.474             0.106   \n",
       "After Mitigation(K=5 fp)            0.807     0.506             0.112   \n",
       "After RANDOM Mitigation(K=5 fp)     0.809     0.476             0.107   \n",
       "\n",
       "Metrics                          max div  media div primi 10  \\\n",
       "Before Mitigation                  0.345               0.246   \n",
       "After Mitigation(K=5 fp)           0.351               0.272   \n",
       "After RANDOM Mitigation(K=5 fp)    0.348               0.247   \n",
       "\n",
       "Metrics                          media div primi 20  media div primi 40  \\\n",
       "Before Mitigation                             0.173               0.106   \n",
       "After Mitigation(K=5 fp)                      0.228               0.153   \n",
       "After RANDOM Mitigation(K=5 fp)               0.174               0.107   \n",
       "\n",
       "Metrics                          # new samples  \n",
       "Before Mitigation                        0.000  \n",
       "After Mitigation(K=5 fp)               511.000  \n",
       "After RANDOM Mitigation(K=5 fp)        511.000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fn_div_list_no_mitigation = np.nanmean(fn_div_list_no_mitigation)\n",
    "media_fn_div_list_nomitigation_primi10 = np.nanmean(fn_div_list_no_mitigation[:10])\n",
    "media_fn_div_list_nomitigation_primi20 = np.nanmean(fn_div_list_no_mitigation[:20])\n",
    "media_fn_div_list_nomitigation_primi40 = np.nanmean(fn_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fn_div_no_mitigation = max(abs(x) for x in fn_div_list_no_mitigation)\n",
    "\n",
    "media_fn_div_list_baseline1 = np.nanmean(fn_div_list_baseline1)\n",
    "media_fn_div_list_baseline1_primi10 = np.nanmean(fn_div_list_baseline1[:10])\n",
    "media_fn_div_list_baseline1_primi20 = np.nanmean(fn_div_list_baseline1[:20])\n",
    "media_fn_div_list_baseline1_primi40 = np.nanmean(fn_div_list_baseline1[:40])\n",
    "fn_div_massimo_valore_assoluto_fn_div_baseline1 = max(abs(x) for x in fn_div_list_baseline1)\n",
    "\n",
    "media_fn_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fn_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fn_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fn_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fn_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fn_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fn_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fn_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fn_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fn_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fn_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fn_div_list_no_mitigation, massimo_valore_assoluto_fn_div_no_mitigation,\n",
    "        media_fn_div_list_nomitigation_primi10, media_fn_div_list_nomitigation_primi20, media_fn_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after, f1_score_fp_after, media_fn_div_list_baseline1, fn_div_massimo_valore_assoluto_fn_div_baseline1,\n",
    "        media_fn_div_list_baseline1_primi10, media_fn_div_list_baseline1_primi20, media_fn_div_list_baseline1_primi40, len(df_holdout_filtered)\n",
    "    ],\n",
    "    'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fn_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fn_div_random_per_confrontare_con_baseline1, media_fn_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fn_div_list_random_per_confrontare_con_baseline1_primi20, media_fn_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        len(df_holdout_filtered)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fn_sottogruppi = divergence_after_fn_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fn_sottogruppi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUI INIZIA SMOTE come si deve DA METTERE NEL REPORT\n",
    "BIAS MITIGATION SMOTENC\n",
    "\n",
    "- FISSO N VARIA p, p è la probabilità che il campione simulato sia di classe 0 qui (perchè voglio diminuire il numero di falsi negativi)\n",
    "- FISSO p VARIA N , aumento il numero di 0 per diminuire il numero di falsi negativi  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Righe del dataset filtrato qunado K = 5 4108\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "df_val_filtered = K_subgroups_dataset_and_or(df_pruned_fp, df_val, K)\n",
    "print(\"Righe del dataset filtrato qunado K = 5\",len(df_val_filtered))\n",
    "#df_val_filtered.head() #var categoriche e numeriche \n",
    "#print(len(df_val_filtered)) #--2610 con = 5\n",
    "df_val_filtered, inutile12, inutile222 = encoding_funct_SMOTE(df_val_filtered, df_test, df_holdout)\n",
    "X_to_SMOTE =  df_val_filtered.drop(columns = ['fn', 'y_pred', 'accuracy', 'income'], axis = 1)\n",
    "y_to_SMOTE = df_val_filtered['income']\n",
    "X_to_SMOTE.head()\n",
    "\n",
    "categorical_features = [0, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534, 3574)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_1 = df_val_filtered['income'].sum()\n",
    "count_0 = len(df_val_filtered) - count_1\n",
    "count_1, count_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUI MODIFICO FACENDO SMOTE SU DF_VAL_FILTERED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smote_nc = SMOTENC(categorical_features=categorical_features,random_state=seed)\\nX_to_SMOTE, y_to_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\\ncount_1 = y_to_SMOTE.sum()\\ncount_0 = len(y_to_SMOTE)-count_1\\ncount_0, count_1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''smote_nc = SMOTENC(categorical_features=categorical_features,random_state=seed)\n",
    "X_to_SMOTE, y_to_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    "count_1 = y_to_SMOTE.sum()\n",
    "count_0 = len(y_to_SMOTE)-count_1\n",
    "count_0, count_1'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "per confronto con targeted\n",
    "N come len_df_holdout_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation N = 5000</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.640</td>\n",
       "      <td>240</td>\n",
       "      <td>1003</td>\n",
       "      <td>13525</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.5</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.619</td>\n",
       "      <td>274</td>\n",
       "      <td>971</td>\n",
       "      <td>13525</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.6</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.631</td>\n",
       "      <td>254</td>\n",
       "      <td>990</td>\n",
       "      <td>13525</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.7</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.633</td>\n",
       "      <td>246</td>\n",
       "      <td>993</td>\n",
       "      <td>13525</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.8</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.640</td>\n",
       "      <td>239</td>\n",
       "      <td>1004</td>\n",
       "      <td>13525</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.9</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.643</td>\n",
       "      <td>239</td>\n",
       "      <td>1009</td>\n",
       "      <td>13525</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 4000 p_class 0 = 1</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.651</td>\n",
       "      <td>226</td>\n",
       "      <td>1021</td>\n",
       "      <td>13525</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                               Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation                        0.809     0.474                0.047   \n",
       "After RANDOM mitigation N = 5000         0.809     0.476                0.049   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5     0.809     0.490                0.055   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6     0.809     0.482                0.051   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7     0.810     0.481                0.050   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8     0.809     0.476                0.048   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9     0.808     0.473                0.048   \n",
       "After SMOTE N = 4000 p_class 0 = 1       0.808     0.467                0.046   \n",
       "\n",
       "Metrics                               False Negative Rate  False Positives  \\\n",
       "Before Mitigation                                   0.643              234   \n",
       "After RANDOM mitigation N = 5000                    0.640              240   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5                0.619              274   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6                0.631              254   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7                0.633              246   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8                0.640              239   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9                0.643              239   \n",
       "After SMOTE N = 4000 p_class 0 = 1                  0.651              226   \n",
       "\n",
       "Metrics                               False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                                1008       13014       6508  \n",
       "After RANDOM mitigation N = 5000                 1003       13525       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.5              971       13525       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.6              990       13525       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.7              993       13525       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.8             1004       13525       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.9             1009       13525       6508  \n",
       "After SMOTE N = 4000 p_class 0 = 1               1021       13525       6508  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p1 = 0.5\n",
    "N = len(df_holdout_filtered)\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {1: count_1 + int(N*0.5), 0: count_0 + int(N*0.5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "#p2 = 0.6 \n",
    "sampling_strategy = {0: count_0 + int(N*0.6), 1: count_1 + int(N*0.4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#p3 = 0.7\n",
    "sampling_strategy = {0: count_0 + int(N*0.7), 1: count_1 + int(N*0.3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "#p4 = 0.8\n",
    "sampling_strategy = {0: count_0 +int(N*0.8), 1: count_1 + int(N*0.2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "#p5 = 0.9\n",
    "sampling_strategy = {0: count_0 + int(N*0.9), 1: count_1 + int(N*0.1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#p6 = 1\n",
    "sampling_strategy = {0: count_0 + int(N*1), 1: count_1 + int(N*0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[3]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[5]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[3]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[5]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.107</td>\n",
       "      <td>511.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 2K, p=0.5)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.092</td>\n",
       "      <td>511.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 2K, p=0.8)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.103</td>\n",
       "      <td>511.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 2K, p=1)</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.115</td>\n",
       "      <td>511.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.106   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.476             0.107   \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.5)     0.809     0.482             0.092   \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.8)     0.809     0.476             0.103   \n",
       "After Mitigation(K=5 fp, N = 2K, p=1)       0.808     0.467             0.115   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.345               0.246   \n",
       "After RANDOM Mitigation(K=5 fp)            0.348               0.247   \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.5)    0.341               0.248   \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.8)    0.347               0.246   \n",
       "After Mitigation(K=5 fp, N = 2K, p=1)      0.336               0.244   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.173   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.174   \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.5)               0.184   \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.8)               0.174   \n",
       "After Mitigation(K=5 fp, N = 2K, p=1)                 0.171   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.106          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.107        511.000  \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.5)               0.092        511.000  \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.8)               0.103        511.000  \n",
       "After Mitigation(K=5 fp, N = 2K, p=1)                 0.115        511.000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p1\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p1_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p4\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p4_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p6\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p6_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values(['fn_div', 'fn_t'], ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p1_5K = np.nanmean(fp_div_list_baseline2_p1_5K)\n",
    "media_fp_div_list_baseline2_p1_5K_primi10 = np.nanmean(fp_div_list_baseline2_p1_5K[:10])\n",
    "media_fp_div_list_baseline2_p1_5K_primi20 = np.nanmean(fp_div_list_baseline2_p1_5K[:20])\n",
    "media_fp_div_list_baseline2_p1_5K_primi40 = np.nanmean(fp_div_list_baseline2_p1_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K = max(abs(x) for x in fp_div_list_baseline2_p1_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p4_5K = np.nanmean(fp_div_list_baseline2_p4_5K)\n",
    "media_fp_div_list_baseline2_p4_5K_primi10 = np.nanmean(fp_div_list_baseline2_p4_5K[:10])\n",
    "media_fp_div_list_baseline2_p4_5K_primi20 = np.nanmean(fp_div_list_baseline2_p4_5K[:20])\n",
    "media_fp_div_list_baseline2_p4_5K_primi40 = np.nanmean(fp_div_list_baseline2_p4_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K = max(abs(x) for x in fp_div_list_baseline2_p4_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p6_5K = np.nanmean(fp_div_list_baseline2_p6_5K)\n",
    "media_fp_div_list_baseline2_p6_5K_primi10 = np.nanmean(fp_div_list_baseline2_p6_5K[:10])\n",
    "media_fp_div_list_baseline2_p6_5K_primi20 = np.nanmean(fp_div_list_baseline2_p6_5K[:20])\n",
    "media_fp_div_list_baseline2_p6_5K_primi40 = np.nanmean(fp_div_list_baseline2_p6_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K = max(abs(x) for x in fp_div_list_baseline2_p6_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 2K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p1_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K,\n",
    "        media_fp_div_list_baseline2_p1_5K_primi10, media_fp_div_list_baseline2_p1_5K_primi20, media_fp_div_list_baseline2_p1_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 2K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p4_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K,\n",
    "        media_fp_div_list_baseline2_p4_5K_primi10, media_fp_div_list_baseline2_p4_5K_primi20, media_fp_div_list_baseline2_p4_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 2K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p6_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K,\n",
    "        media_fp_div_list_baseline2_p6_5K_primi10, media_fp_div_list_baseline2_p6_5K_primi20, media_fp_div_list_baseline2_p6_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation N = 5000</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.641</td>\n",
       "      <td>246</td>\n",
       "      <td>1005</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 2000 p_class 0 = 0.5</th>\n",
       "      <td>0.801</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.583</td>\n",
       "      <td>381</td>\n",
       "      <td>914</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 2000 p_class 0 = 0.6</th>\n",
       "      <td>0.796</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.564</td>\n",
       "      <td>441</td>\n",
       "      <td>885</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 2000 p_class 0 = 0.7</th>\n",
       "      <td>0.794</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.538</td>\n",
       "      <td>499</td>\n",
       "      <td>843</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 2000 p_class 0 = 0.8</th>\n",
       "      <td>0.787</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.513</td>\n",
       "      <td>580</td>\n",
       "      <td>805</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 2000 p_class 0 = 0.9</th>\n",
       "      <td>0.781</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.497</td>\n",
       "      <td>647</td>\n",
       "      <td>779</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 2000 p_class 0 = 1</th>\n",
       "      <td>0.775</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.488</td>\n",
       "      <td>700</td>\n",
       "      <td>765</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                               Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation                        0.809     0.474                0.047   \n",
       "After RANDOM mitigation N = 5000         0.808     0.474                0.050   \n",
       "After SMOTE N = 2000 p_class 0 = 0.5     0.801     0.502                0.077   \n",
       "After SMOTE N = 2000 p_class 0 = 0.6     0.796     0.507                0.089   \n",
       "After SMOTE N = 2000 p_class 0 = 0.7     0.794     0.519                0.101   \n",
       "After SMOTE N = 2000 p_class 0 = 0.8     0.787     0.524                0.117   \n",
       "After SMOTE N = 2000 p_class 0 = 0.9     0.781     0.525                0.131   \n",
       "After SMOTE N = 2000 p_class 0 = 1       0.775     0.523                0.142   \n",
       "\n",
       "Metrics                               False Negative Rate  False Positives  \\\n",
       "Before Mitigation                                   0.643              234   \n",
       "After RANDOM mitigation N = 5000                    0.641              246   \n",
       "After SMOTE N = 2000 p_class 0 = 0.5                0.583              381   \n",
       "After SMOTE N = 2000 p_class 0 = 0.6                0.564              441   \n",
       "After SMOTE N = 2000 p_class 0 = 0.7                0.538              499   \n",
       "After SMOTE N = 2000 p_class 0 = 0.8                0.513              580   \n",
       "After SMOTE N = 2000 p_class 0 = 0.9                0.497              647   \n",
       "After SMOTE N = 2000 p_class 0 = 1                  0.488              700   \n",
       "\n",
       "Metrics                               False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                                1008       13014       6508  \n",
       "After RANDOM mitigation N = 5000                 1005       15014       6508  \n",
       "After SMOTE N = 2000 p_class 0 = 0.5              914       15014       6508  \n",
       "After SMOTE N = 2000 p_class 0 = 0.6              885       15014       6508  \n",
       "After SMOTE N = 2000 p_class 0 = 0.7              843       15014       6508  \n",
       "After SMOTE N = 2000 p_class 0 = 0.8              805       15014       6508  \n",
       "After SMOTE N = 2000 p_class 0 = 0.9              779       15014       6508  \n",
       "After SMOTE N = 2000 p_class 0 = 1                765       15014       6508  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p1 = 0.5\n",
    "N = 2000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {1: count_1 + 1000, 0: count_0 + 1000}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "#p2 = 0.6 \n",
    "sampling_strategy = {1: count_1 + 1200, 0: count_0 + 800}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#p3 = 0.7\n",
    "sampling_strategy = {1: count_1 + 1400, 0: count_0 + 600}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "#p4 = 0.8\n",
    "sampling_strategy = {1: count_1 + 1600, 0: count_0 + 400}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "#p5 = 0.9\n",
    "sampling_strategy = {1: count_1 + 1800, 0: count_0 + 200}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#p6 = 1\n",
    "sampling_strategy = {1: count_1 + 2000, 0: count_0}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 2000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 2000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 2000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 2000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 2000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 2000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[3]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[5]\n",
    "accuracy1= metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[3]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[5]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.104</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 2K, p=0.5)</th>\n",
       "      <td>0.796</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.164</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 2K, p=0.8)</th>\n",
       "      <td>0.787</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.137</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 2K, p=1)</th>\n",
       "      <td>0.775</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.142</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.106   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.476             0.104   \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.5)     0.796     0.507             0.112   \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.8)     0.787     0.524             0.072   \n",
       "After Mitigation(K=5 fp, N = 2K, p=1)       0.775     0.523             0.070   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.345               0.246   \n",
       "After RANDOM Mitigation(K=5 fp)            0.347               0.245   \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.5)    0.353               0.277   \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.8)    0.286               0.217   \n",
       "After Mitigation(K=5 fp, N = 2K, p=1)      0.299               0.222   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.173   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.175   \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.5)               0.235   \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.8)               0.183   \n",
       "After Mitigation(K=5 fp, N = 2K, p=1)                 0.189   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.106          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.104       2000.000  \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.5)               0.164       2000.000  \n",
       "After Mitigation(K=5 fp, N = 2K, p=0.8)               0.137       2000.000  \n",
       "After Mitigation(K=5 fp, N = 2K, p=1)                 0.142       2000.000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p1\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p1_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p4\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p4_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p6\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p6_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values(['fn_div', 'fn_t'], ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p1_5K = np.nanmean(fp_div_list_baseline2_p1_5K)\n",
    "media_fp_div_list_baseline2_p1_5K_primi10 = np.nanmean(fp_div_list_baseline2_p1_5K[:10])\n",
    "media_fp_div_list_baseline2_p1_5K_primi20 = np.nanmean(fp_div_list_baseline2_p1_5K[:20])\n",
    "media_fp_div_list_baseline2_p1_5K_primi40 = np.nanmean(fp_div_list_baseline2_p1_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K = max(abs(x) for x in fp_div_list_baseline2_p1_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p4_5K = np.nanmean(fp_div_list_baseline2_p4_5K)\n",
    "media_fp_div_list_baseline2_p4_5K_primi10 = np.nanmean(fp_div_list_baseline2_p4_5K[:10])\n",
    "media_fp_div_list_baseline2_p4_5K_primi20 = np.nanmean(fp_div_list_baseline2_p4_5K[:20])\n",
    "media_fp_div_list_baseline2_p4_5K_primi40 = np.nanmean(fp_div_list_baseline2_p4_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K = max(abs(x) for x in fp_div_list_baseline2_p4_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p6_5K = np.nanmean(fp_div_list_baseline2_p6_5K)\n",
    "media_fp_div_list_baseline2_p6_5K_primi10 = np.nanmean(fp_div_list_baseline2_p6_5K[:10])\n",
    "media_fp_div_list_baseline2_p6_5K_primi20 = np.nanmean(fp_div_list_baseline2_p6_5K[:20])\n",
    "media_fp_div_list_baseline2_p6_5K_primi40 = np.nanmean(fp_div_list_baseline2_p6_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K = max(abs(x) for x in fp_div_list_baseline2_p6_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 2K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p1_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K,\n",
    "        media_fp_div_list_baseline2_p1_5K_primi10, media_fp_div_list_baseline2_p1_5K_primi20, media_fp_div_list_baseline2_p1_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 2K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p4_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K,\n",
    "        media_fp_div_list_baseline2_p4_5K_primi10, media_fp_div_list_baseline2_p4_5K_primi20, media_fp_div_list_baseline2_p4_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 2K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p6_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K,\n",
    "        media_fp_div_list_baseline2_p6_5K_primi10, media_fp_div_list_baseline2_p6_5K_primi20, media_fp_div_list_baseline2_p6_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation N = 5000</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.634</td>\n",
       "      <td>246</td>\n",
       "      <td>994</td>\n",
       "      <td>16014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.5</th>\n",
       "      <td>0.796</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.557</td>\n",
       "      <td>457</td>\n",
       "      <td>873</td>\n",
       "      <td>16014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.6</th>\n",
       "      <td>0.787</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.519</td>\n",
       "      <td>569</td>\n",
       "      <td>814</td>\n",
       "      <td>16014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.7</th>\n",
       "      <td>0.780</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.498</td>\n",
       "      <td>653</td>\n",
       "      <td>781</td>\n",
       "      <td>16014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.8</th>\n",
       "      <td>0.773</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.453</td>\n",
       "      <td>766</td>\n",
       "      <td>711</td>\n",
       "      <td>16014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.9</th>\n",
       "      <td>0.762</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.430</td>\n",
       "      <td>873</td>\n",
       "      <td>675</td>\n",
       "      <td>16014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 4000 p_class 0 = 1</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.398</td>\n",
       "      <td>1005</td>\n",
       "      <td>624</td>\n",
       "      <td>16014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                               Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation                        0.809     0.474                0.047   \n",
       "After RANDOM mitigation N = 5000         0.809     0.481                0.050   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5     0.796     0.511                0.093   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6     0.787     0.522                0.115   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7     0.780     0.523                0.132   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8     0.773     0.537                0.155   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9     0.762     0.536                0.177   \n",
       "After SMOTE N = 4000 p_class 0 = 1       0.750     0.537                0.203   \n",
       "\n",
       "Metrics                               False Negative Rate  False Positives  \\\n",
       "Before Mitigation                                   0.643              234   \n",
       "After RANDOM mitigation N = 5000                    0.634              246   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5                0.557              457   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6                0.519              569   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7                0.498              653   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8                0.453              766   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9                0.430              873   \n",
       "After SMOTE N = 4000 p_class 0 = 1                  0.398             1005   \n",
       "\n",
       "Metrics                               False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                                1008       13014       6508  \n",
       "After RANDOM mitigation N = 5000                  994       16014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.5              873       16014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.6              814       16014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.7              781       16014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.8              711       16014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.9              675       16014       6508  \n",
       "After SMOTE N = 4000 p_class 0 = 1                624       16014       6508  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p1 = 0.5\n",
    "N = 3000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {1: count_1 + 1500, 0: count_0 + 1500}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "#p2 = 0.6 \n",
    "sampling_strategy = {1: count_1 + 1800, 0: count_0 + 1200}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#p3 = 0.7\n",
    "sampling_strategy = {1: count_1 + 2100, 0: count_0 + 900}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "#p4 = 0.8\n",
    "sampling_strategy = {1: count_1 + 2400, 0: count_0 + 600}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "#p5 = 0.9\n",
    "sampling_strategy = {1: count_1 + 2700, 0: count_0 + 300}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#p6 = 1\n",
    "sampling_strategy = {1: count_1 + 3000, 0: count_0}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "falsi_positivi_3K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_3K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_3K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_3K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[3]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[5]\n",
    "accuracy1= metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[3]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[5]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.104</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 3K, p=0.5)</th>\n",
       "      <td>0.787</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.155</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 3K, p=0.8)</th>\n",
       "      <td>0.773</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.117</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 3K, p=1)</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.085</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.106   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.476             0.104   \n",
       "After Mitigation(K=5 fp, N = 3K, p=0.5)     0.787     0.522             0.088   \n",
       "After Mitigation(K=5 fp, N = 3K, p=0.8)     0.773     0.537             0.039   \n",
       "After Mitigation(K=5 fp, N = 3K, p=1)       0.750     0.537             0.010   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.345               0.246   \n",
       "After RANDOM Mitigation(K=5 fp)            0.349               0.252   \n",
       "After Mitigation(K=5 fp, N = 3K, p=0.5)    0.306               0.245   \n",
       "After Mitigation(K=5 fp, N = 3K, p=0.8)    0.215               0.178   \n",
       "After Mitigation(K=5 fp, N = 3K, p=1)      0.180               0.139   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.173   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.187   \n",
       "After Mitigation(K=5 fp, N = 3K, p=0.5)               0.207   \n",
       "After Mitigation(K=5 fp, N = 3K, p=0.8)               0.151   \n",
       "After Mitigation(K=5 fp, N = 3K, p=1)                 0.116   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.106          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.104       3000.000  \n",
       "After Mitigation(K=5 fp, N = 3K, p=0.5)               0.155       3000.000  \n",
       "After Mitigation(K=5 fp, N = 3K, p=0.8)               0.117       3000.000  \n",
       "After Mitigation(K=5 fp, N = 3K, p=1)                 0.085       3000.000  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p1\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p1_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p4\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p4_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p6\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p6_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values(['fn_div', 'fn_t'], ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p1_5K = np.nanmean(fp_div_list_baseline2_p1_5K)\n",
    "media_fp_div_list_baseline2_p1_5K_primi10 = np.nanmean(fp_div_list_baseline2_p1_5K[:10])\n",
    "media_fp_div_list_baseline2_p1_5K_primi20 = np.nanmean(fp_div_list_baseline2_p1_5K[:20])\n",
    "media_fp_div_list_baseline2_p1_5K_primi40 = np.nanmean(fp_div_list_baseline2_p1_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K = max(abs(x) for x in fp_div_list_baseline2_p1_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p4_5K = np.nanmean(fp_div_list_baseline2_p4_5K)\n",
    "media_fp_div_list_baseline2_p4_5K_primi10 = np.nanmean(fp_div_list_baseline2_p4_5K[:10])\n",
    "media_fp_div_list_baseline2_p4_5K_primi20 = np.nanmean(fp_div_list_baseline2_p4_5K[:20])\n",
    "media_fp_div_list_baseline2_p4_5K_primi40 = np.nanmean(fp_div_list_baseline2_p4_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K = max(abs(x) for x in fp_div_list_baseline2_p4_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p6_5K = np.nanmean(fp_div_list_baseline2_p6_5K)\n",
    "media_fp_div_list_baseline2_p6_5K_primi10 = np.nanmean(fp_div_list_baseline2_p6_5K[:10])\n",
    "media_fp_div_list_baseline2_p6_5K_primi20 = np.nanmean(fp_div_list_baseline2_p6_5K[:20])\n",
    "media_fp_div_list_baseline2_p6_5K_primi40 = np.nanmean(fp_div_list_baseline2_p6_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K = max(abs(x) for x in fp_div_list_baseline2_p6_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 3K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p1_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K,\n",
    "        media_fp_div_list_baseline2_p1_5K_primi10, media_fp_div_list_baseline2_p1_5K_primi20, media_fp_div_list_baseline2_p1_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 3K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p4_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K,\n",
    "        media_fp_div_list_baseline2_p4_5K_primi10, media_fp_div_list_baseline2_p4_5K_primi20, media_fp_div_list_baseline2_p4_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 3K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p6_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K,\n",
    "        media_fp_div_list_baseline2_p6_5K_primi10, media_fp_div_list_baseline2_p6_5K_primi20, media_fp_div_list_baseline2_p6_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation N = 5000</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.629</td>\n",
       "      <td>252</td>\n",
       "      <td>987</td>\n",
       "      <td>17014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.5</th>\n",
       "      <td>0.789</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.524</td>\n",
       "      <td>549</td>\n",
       "      <td>821</td>\n",
       "      <td>17014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.6</th>\n",
       "      <td>0.779</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.497</td>\n",
       "      <td>657</td>\n",
       "      <td>780</td>\n",
       "      <td>17014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.7</th>\n",
       "      <td>0.769</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.464</td>\n",
       "      <td>776</td>\n",
       "      <td>727</td>\n",
       "      <td>17014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.8</th>\n",
       "      <td>0.754</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.419</td>\n",
       "      <td>947</td>\n",
       "      <td>657</td>\n",
       "      <td>17014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.9</th>\n",
       "      <td>0.745</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.390</td>\n",
       "      <td>1048</td>\n",
       "      <td>612</td>\n",
       "      <td>17014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 4000 p_class 0 = 1</th>\n",
       "      <td>0.734</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.344</td>\n",
       "      <td>1189</td>\n",
       "      <td>540</td>\n",
       "      <td>17014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                               Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation                        0.809     0.474                0.047   \n",
       "After RANDOM mitigation N = 5000         0.810     0.484                0.051   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5     0.789     0.522                0.111   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6     0.779     0.523                0.133   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7     0.769     0.528                0.157   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8     0.754     0.532                0.192   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9     0.745     0.535                0.212   \n",
       "After SMOTE N = 4000 p_class 0 = 1       0.734     0.543                0.241   \n",
       "\n",
       "Metrics                               False Negative Rate  False Positives  \\\n",
       "Before Mitigation                                   0.643              234   \n",
       "After RANDOM mitigation N = 5000                    0.629              252   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5                0.524              549   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6                0.497              657   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7                0.464              776   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8                0.419              947   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9                0.390             1048   \n",
       "After SMOTE N = 4000 p_class 0 = 1                  0.344             1189   \n",
       "\n",
       "Metrics                               False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                                1008       13014       6508  \n",
       "After RANDOM mitigation N = 5000                  987       17014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.5              821       17014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.6              780       17014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.7              727       17014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.8              657       17014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.9              612       17014       6508  \n",
       "After SMOTE N = 4000 p_class 0 = 1                540       17014       6508  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p1 = 0.5\n",
    "N = 4000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {1: count_1 + 2000, 0: count_0 + 2000}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "#p2 = 0.6 \n",
    "sampling_strategy = {1: count_1 + 2400, 0: count_0 + 1600}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#p3 = 0.7\n",
    "sampling_strategy = {1: count_1 + 2800, 0: count_0 + 1200}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "#p4 = 0.8\n",
    "sampling_strategy = {1: count_1 + 3200, 0: count_0 + 800}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "#p5 = 0.9\n",
    "sampling_strategy = {1: count_1 + 3600, 0: count_0 + 400}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#p6 = 1\n",
    "sampling_strategy = {1: count_1 + 4000, 0: count_0}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "falsi_positivi_4K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_4K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_4K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_4K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[3]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[5]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[3]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[5]\n",
    "accuracy1= metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.354</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.104</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 4K, p=0.5)</th>\n",
       "      <td>0.779</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.139</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 4K, p=0.8)</th>\n",
       "      <td>0.754</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.077</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 4K, p=1)</th>\n",
       "      <td>0.734</td>\n",
       "      <td>0.543</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.055</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.106   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.476             0.104   \n",
       "After Mitigation(K=5 fp, N = 4K, p=0.5)     0.779     0.523             0.070   \n",
       "After Mitigation(K=5 fp, N = 4K, p=0.8)     0.754     0.532             0.015   \n",
       "After Mitigation(K=5 fp, N = 4K, p=1)       0.734     0.543            -0.009   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.345               0.246   \n",
       "After RANDOM Mitigation(K=5 fp)            0.354               0.257   \n",
       "After Mitigation(K=5 fp, N = 4K, p=0.5)    0.282               0.221   \n",
       "After Mitigation(K=5 fp, N = 4K, p=0.8)    0.178               0.127   \n",
       "After Mitigation(K=5 fp, N = 4K, p=1)      0.146               0.097   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.173   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.189   \n",
       "After Mitigation(K=5 fp, N = 4K, p=0.5)               0.186   \n",
       "After Mitigation(K=5 fp, N = 4K, p=0.8)               0.105   \n",
       "After Mitigation(K=5 fp, N = 4K, p=1)                 0.079   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.106          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.104       4000.000  \n",
       "After Mitigation(K=5 fp, N = 4K, p=0.5)               0.139       4000.000  \n",
       "After Mitigation(K=5 fp, N = 4K, p=0.8)               0.077       4000.000  \n",
       "After Mitigation(K=5 fp, N = 4K, p=1)                 0.055       4000.000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p1\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p1_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p4\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p4_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p6\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p6_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values(['fn_div', 'fn_t'], ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p1_5K = np.nanmean(fp_div_list_baseline2_p1_5K)\n",
    "media_fp_div_list_baseline2_p1_5K_primi10 = np.nanmean(fp_div_list_baseline2_p1_5K[:10])\n",
    "media_fp_div_list_baseline2_p1_5K_primi20 = np.nanmean(fp_div_list_baseline2_p1_5K[:20])\n",
    "media_fp_div_list_baseline2_p1_5K_primi40 = np.nanmean(fp_div_list_baseline2_p1_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K = max(abs(x) for x in fp_div_list_baseline2_p1_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p4_5K = np.nanmean(fp_div_list_baseline2_p4_5K)\n",
    "media_fp_div_list_baseline2_p4_5K_primi10 = np.nanmean(fp_div_list_baseline2_p4_5K[:10])\n",
    "media_fp_div_list_baseline2_p4_5K_primi20 = np.nanmean(fp_div_list_baseline2_p4_5K[:20])\n",
    "media_fp_div_list_baseline2_p4_5K_primi40 = np.nanmean(fp_div_list_baseline2_p4_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K = max(abs(x) for x in fp_div_list_baseline2_p4_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p6_5K = np.nanmean(fp_div_list_baseline2_p6_5K)\n",
    "media_fp_div_list_baseline2_p6_5K_primi10 = np.nanmean(fp_div_list_baseline2_p6_5K[:10])\n",
    "media_fp_div_list_baseline2_p6_5K_primi20 = np.nanmean(fp_div_list_baseline2_p6_5K[:20])\n",
    "media_fp_div_list_baseline2_p6_5K_primi40 = np.nanmean(fp_div_list_baseline2_p6_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K = max(abs(x) for x in fp_div_list_baseline2_p6_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 4K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p1_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K,\n",
    "        media_fp_div_list_baseline2_p1_5K_primi10, media_fp_div_list_baseline2_p1_5K_primi20, media_fp_div_list_baseline2_p1_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 4K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p4_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K,\n",
    "        media_fp_div_list_baseline2_p4_5K_primi10, media_fp_div_list_baseline2_p4_5K_primi20, media_fp_div_list_baseline2_p4_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 4K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p6_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K,\n",
    "        media_fp_div_list_baseline2_p6_5K_primi10, media_fp_div_list_baseline2_p6_5K_primi20, media_fp_div_list_baseline2_p6_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation N = 5000</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.633</td>\n",
       "      <td>258</td>\n",
       "      <td>992</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.5</th>\n",
       "      <td>0.785</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.511</td>\n",
       "      <td>595</td>\n",
       "      <td>801</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.6</th>\n",
       "      <td>0.772</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.469</td>\n",
       "      <td>747</td>\n",
       "      <td>735</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.7</th>\n",
       "      <td>0.760</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.423</td>\n",
       "      <td>899</td>\n",
       "      <td>663</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.8</th>\n",
       "      <td>0.745</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.390</td>\n",
       "      <td>1050</td>\n",
       "      <td>611</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.9</th>\n",
       "      <td>0.726</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.320</td>\n",
       "      <td>1284</td>\n",
       "      <td>501</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 4000 p_class 0 = 1</th>\n",
       "      <td>0.709</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.335</td>\n",
       "      <td>1365</td>\n",
       "      <td>526</td>\n",
       "      <td>18014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                               Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation                        0.809     0.474                0.047   \n",
       "After RANDOM mitigation N = 5000         0.808     0.480                0.052   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5     0.785     0.524                0.120   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6     0.772     0.529                0.151   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7     0.760     0.537                0.182   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8     0.745     0.535                0.213   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9     0.726     0.545                0.260   \n",
       "After SMOTE N = 4000 p_class 0 = 1       0.709     0.524                0.276   \n",
       "\n",
       "Metrics                               False Negative Rate  False Positives  \\\n",
       "Before Mitigation                                   0.643              234   \n",
       "After RANDOM mitigation N = 5000                    0.633              258   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5                0.511              595   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6                0.469              747   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7                0.423              899   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8                0.390             1050   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9                0.320             1284   \n",
       "After SMOTE N = 4000 p_class 0 = 1                  0.335             1365   \n",
       "\n",
       "Metrics                               False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                                1008       13014       6508  \n",
       "After RANDOM mitigation N = 5000                  992       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.5              801       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.6              735       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.7              663       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.8              611       18014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.9              501       18014       6508  \n",
       "After SMOTE N = 4000 p_class 0 = 1                526       18014       6508  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p1 = 0.5\n",
    "N = 5000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {1: count_1 + 2500, 0: count_0 + 2500}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "#p2 = 0.6 \n",
    "sampling_strategy = {1: count_1 + 3000, 0: count_0 + 2000}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#p3 = 0.7\n",
    "sampling_strategy = {1: count_1 + 3500, 0: count_0 + 1500}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "#p4 = 0.8\n",
    "sampling_strategy = {1: count_1 + 4000, 0: count_0 + 1000}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "#p5 = 0.9\n",
    "sampling_strategy = {1: count_1 + 4500, 0: count_0 + 500}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#p6 = 1\n",
    "sampling_strategy = {1: count_1 + 5000, 0: count_0}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "falsi_positivi_5K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[3]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[5]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOTTOGRUPPI \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation, on subgroups</th>\n",
       "      <td>0.874</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.955</td>\n",
       "      <td>32</td>\n",
       "      <td>489</td>\n",
       "      <td>13014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5, on subgroups, fp)</th>\n",
       "      <td>0.874</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.949</td>\n",
       "      <td>35</td>\n",
       "      <td>486</td>\n",
       "      <td>18014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)</th>\n",
       "      <td>0.831</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.707</td>\n",
       "      <td>335</td>\n",
       "      <td>362</td>\n",
       "      <td>18014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)</th>\n",
       "      <td>0.813</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.602</td>\n",
       "      <td>465</td>\n",
       "      <td>308</td>\n",
       "      <td>18014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)</th>\n",
       "      <td>0.794</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.494</td>\n",
       "      <td>600</td>\n",
       "      <td>253</td>\n",
       "      <td>18014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)</th>\n",
       "      <td>0.769</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.439</td>\n",
       "      <td>729</td>\n",
       "      <td>225</td>\n",
       "      <td>18014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)</th>\n",
       "      <td>0.738</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.348</td>\n",
       "      <td>906</td>\n",
       "      <td>178</td>\n",
       "      <td>18014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)</th>\n",
       "      <td>0.717</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.299</td>\n",
       "      <td>1018</td>\n",
       "      <td>153</td>\n",
       "      <td>18014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                                 Accuracy  F1 Score  \\\n",
       "Before Mitigation, on subgroups                            0.874     0.081   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)             0.874     0.091   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)     0.831     0.301   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)     0.813     0.345   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)     0.794     0.378   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)     0.769     0.376   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)     0.738     0.381   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)       0.717     0.380   \n",
       "\n",
       "Metrics                                                 False Positive Rate  \\\n",
       "Before Mitigation, on subgroups                                       0.009   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                        0.010   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)                0.092   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)                0.128   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)                0.166   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)                0.201   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)                0.250   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)                  0.281   \n",
       "\n",
       "Metrics                                                 False Negative Rate  \\\n",
       "Before Mitigation, on subgroups                                       0.955   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                        0.949   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)                0.707   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)                0.602   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)                0.494   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)                0.439   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)                0.348   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)                  0.299   \n",
       "\n",
       "Metrics                                                 False Positives  \\\n",
       "Before Mitigation, on subgroups                                      32   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                       35   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)              335   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)              465   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)              600   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)              729   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)              906   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)               1018   \n",
       "\n",
       "Metrics                                                 False Negatives  \\\n",
       "Before Mitigation, on subgroups                                     489   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                      486   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)              362   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)              308   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)              253   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)              225   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)              178   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)                153   \n",
       "\n",
       "Metrics                                                 Train Size  Test Size  \n",
       "Before Mitigation, on subgroups                              13014       4135  \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)               18014       4135  \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)       18014       4135  \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)       18014       4135  \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)       18014       4135  \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)       18014       4135  \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)       18014       4135  \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)         18014       4135  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[3]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[5]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.103</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.772</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.136</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.745</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.082</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.709</td>\n",
       "      <td>0.524</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.045</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.106   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.476             0.103   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.772     0.529             0.068   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.745     0.535             0.008   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.709     0.524            -0.036   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.345               0.246   \n",
       "After RANDOM Mitigation(K=5 fp)            0.349               0.250   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.286               0.214   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.169               0.135   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.205               0.091   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.173   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.186   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.182   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.113   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.072   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.106          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.103       5000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.136       5000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.082       5000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.045       5000.000  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p1\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p1_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p4\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p4_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p6\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p6_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values(['fn_div', 'fn_t'], ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p1_5K = np.nanmean(fp_div_list_baseline2_p1_5K)\n",
    "media_fp_div_list_baseline2_p1_5K_primi10 = np.nanmean(fp_div_list_baseline2_p1_5K[:10])\n",
    "media_fp_div_list_baseline2_p1_5K_primi20 = np.nanmean(fp_div_list_baseline2_p1_5K[:20])\n",
    "media_fp_div_list_baseline2_p1_5K_primi40 = np.nanmean(fp_div_list_baseline2_p1_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K = max(abs(x) for x in fp_div_list_baseline2_p1_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p4_5K = np.nanmean(fp_div_list_baseline2_p4_5K)\n",
    "media_fp_div_list_baseline2_p4_5K_primi10 = np.nanmean(fp_div_list_baseline2_p4_5K[:10])\n",
    "media_fp_div_list_baseline2_p4_5K_primi20 = np.nanmean(fp_div_list_baseline2_p4_5K[:20])\n",
    "media_fp_div_list_baseline2_p4_5K_primi40 = np.nanmean(fp_div_list_baseline2_p4_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K = max(abs(x) for x in fp_div_list_baseline2_p4_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p6_5K = np.nanmean(fp_div_list_baseline2_p6_5K)\n",
    "media_fp_div_list_baseline2_p6_5K_primi10 = np.nanmean(fp_div_list_baseline2_p6_5K[:10])\n",
    "media_fp_div_list_baseline2_p6_5K_primi20 = np.nanmean(fp_div_list_baseline2_p6_5K[:20])\n",
    "media_fp_div_list_baseline2_p6_5K_primi40 = np.nanmean(fp_div_list_baseline2_p6_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K = max(abs(x) for x in fp_div_list_baseline2_p6_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p1_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K,\n",
    "        media_fp_div_list_baseline2_p1_5K_primi10, media_fp_div_list_baseline2_p1_5K_primi20, media_fp_div_list_baseline2_p1_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p4_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K,\n",
    "        media_fp_div_list_baseline2_p4_5K_primi10, media_fp_div_list_baseline2_p4_5K_primi20, media_fp_div_list_baseline2_p4_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p6_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K,\n",
    "        media_fp_div_list_baseline2_p6_5K_primi10, media_fp_div_list_baseline2_p6_5K_primi20, media_fp_div_list_baseline2_p6_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 6000, p changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation N = 6000</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.631</td>\n",
       "      <td>259</td>\n",
       "      <td>990</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 6000 p_class 0 = 0.5</th>\n",
       "      <td>0.780</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.473</td>\n",
       "      <td>694</td>\n",
       "      <td>741</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 6000 p_class 0 = 0.6</th>\n",
       "      <td>0.766</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.433</td>\n",
       "      <td>842</td>\n",
       "      <td>679</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 6000 p_class 0 = 0.7</th>\n",
       "      <td>0.757</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.368</td>\n",
       "      <td>1005</td>\n",
       "      <td>577</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 6000 p_class 0 = 0.8</th>\n",
       "      <td>0.738</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.349</td>\n",
       "      <td>1157</td>\n",
       "      <td>547</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 6000 p_class 0 = 0.9</th>\n",
       "      <td>0.710</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.335</td>\n",
       "      <td>1361</td>\n",
       "      <td>526</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 6000 p_class 0 = 1</th>\n",
       "      <td>0.690</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.318</td>\n",
       "      <td>1516</td>\n",
       "      <td>499</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                               Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation                        0.809     0.474                0.047   \n",
       "After RANDOM mitigation N = 6000         0.808     0.481                0.052   \n",
       "After SMOTE N = 6000 p_class 0 = 0.5     0.780     0.535                0.140   \n",
       "After SMOTE N = 6000 p_class 0 = 0.6     0.766     0.539                0.170   \n",
       "After SMOTE N = 6000 p_class 0 = 0.7     0.757     0.556                0.203   \n",
       "After SMOTE N = 6000 p_class 0 = 0.8     0.738     0.545                0.234   \n",
       "After SMOTE N = 6000 p_class 0 = 0.9     0.710     0.525                0.276   \n",
       "After SMOTE N = 6000 p_class 0 = 1       0.690     0.515                0.307   \n",
       "\n",
       "Metrics                               False Negative Rate  False Positives  \\\n",
       "Before Mitigation                                   0.643              234   \n",
       "After RANDOM mitigation N = 6000                    0.631              259   \n",
       "After SMOTE N = 6000 p_class 0 = 0.5                0.473              694   \n",
       "After SMOTE N = 6000 p_class 0 = 0.6                0.433              842   \n",
       "After SMOTE N = 6000 p_class 0 = 0.7                0.368             1005   \n",
       "After SMOTE N = 6000 p_class 0 = 0.8                0.349             1157   \n",
       "After SMOTE N = 6000 p_class 0 = 0.9                0.335             1361   \n",
       "After SMOTE N = 6000 p_class 0 = 1                  0.318             1516   \n",
       "\n",
       "Metrics                               False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                                1008       13014       6508  \n",
       "After RANDOM mitigation N = 6000                  990       19014       6508  \n",
       "After SMOTE N = 6000 p_class 0 = 0.5              741       19014       6508  \n",
       "After SMOTE N = 6000 p_class 0 = 0.6              679       19014       6508  \n",
       "After SMOTE N = 6000 p_class 0 = 0.7              577       19014       6508  \n",
       "After SMOTE N = 6000 p_class 0 = 0.8              547       19014       6508  \n",
       "After SMOTE N = 6000 p_class 0 = 0.9              526       19014       6508  \n",
       "After SMOTE N = 6000 p_class 0 = 1                499       19014       6508  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 6000\n",
    "#p1 = 0.5\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {1: count_1 + 3000, 0: count_0 + 3000}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "#p2 = 0.6 \n",
    "sampling_strategy = {1: count_1 + 3600, 0: count_0 + 2400}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#p3 = 0.7\n",
    "sampling_strategy = {1: count_1 + 4100, 0: count_0 + 1800}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "#p4 = 0.8\n",
    "sampling_strategy = {1: count_1 + 4800, 0: count_0 + 1200}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "#p5 = 0.9\n",
    "sampling_strategy = {1: count_1 + 5400, 0: count_0 + 600}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#p6 = 1\n",
    "sampling_strategy = {1: count_1+ 6000, 0: count_0}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 6000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 6000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 6000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 6000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 6000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 6000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 6000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_6K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_6K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "falsi_positivi_6K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_6K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[3]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[5]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[3]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[5]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOTTOGRUPPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation, on subgroups</th>\n",
       "      <td>0.874</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.955</td>\n",
       "      <td>32</td>\n",
       "      <td>489</td>\n",
       "      <td>13014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5, on subgroups, fp)</th>\n",
       "      <td>0.873</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.951</td>\n",
       "      <td>38</td>\n",
       "      <td>487</td>\n",
       "      <td>19014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)</th>\n",
       "      <td>0.822</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.682</td>\n",
       "      <td>386</td>\n",
       "      <td>349</td>\n",
       "      <td>19014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)</th>\n",
       "      <td>0.804</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.533</td>\n",
       "      <td>537</td>\n",
       "      <td>273</td>\n",
       "      <td>19014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)</th>\n",
       "      <td>0.787</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.469</td>\n",
       "      <td>642</td>\n",
       "      <td>240</td>\n",
       "      <td>19014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)</th>\n",
       "      <td>0.756</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.414</td>\n",
       "      <td>799</td>\n",
       "      <td>212</td>\n",
       "      <td>19014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)</th>\n",
       "      <td>0.718</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.301</td>\n",
       "      <td>1013</td>\n",
       "      <td>154</td>\n",
       "      <td>19014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)</th>\n",
       "      <td>0.691</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.258</td>\n",
       "      <td>1144</td>\n",
       "      <td>132</td>\n",
       "      <td>19014</td>\n",
       "      <td>4135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                                 Accuracy  F1 Score  \\\n",
       "Before Mitigation, on subgroups                            0.874     0.081   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)             0.873     0.087   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)     0.822     0.307   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)     0.804     0.371   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)     0.787     0.381   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)     0.756     0.372   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)     0.718     0.380   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)       0.691     0.373   \n",
       "\n",
       "Metrics                                                 False Positive Rate  \\\n",
       "Before Mitigation, on subgroups                                       0.009   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                        0.010   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)                0.107   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)                0.148   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)                0.177   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)                0.221   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)                0.280   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)                  0.316   \n",
       "\n",
       "Metrics                                                 False Negative Rate  \\\n",
       "Before Mitigation, on subgroups                                       0.955   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                        0.951   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)                0.682   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)                0.533   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)                0.469   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)                0.414   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)                0.301   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)                  0.258   \n",
       "\n",
       "Metrics                                                 False Positives  \\\n",
       "Before Mitigation, on subgroups                                      32   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                       38   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)              386   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)              537   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)              642   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)              799   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)             1013   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)               1144   \n",
       "\n",
       "Metrics                                                 False Negatives  \\\n",
       "Before Mitigation, on subgroups                                     489   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                      487   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)              349   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)              273   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)              240   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)              212   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)              154   \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)                132   \n",
       "\n",
       "Metrics                                                 Train Size  Test Size  \n",
       "Before Mitigation, on subgroups                              13014       4135  \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)               19014       4135  \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)       19014       4135  \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)       19014       4135  \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)       19014       4135  \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)       19014       4135  \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)       19014       4135  \n",
       "After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)         19014       4135  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_6K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_6K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "falsi_positivi_6K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_6K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.104</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.766</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.142</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.738</td>\n",
       "      <td>0.545</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.056</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.690</td>\n",
       "      <td>0.515</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.051</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.106   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.476             0.104   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.766     0.539             0.047   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.738     0.545            -0.009   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.690     0.515            -0.036   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.345               0.246   \n",
       "After RANDOM Mitigation(K=5 fp)            0.352               0.254   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.306               0.219   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.147               0.099   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.214               0.103   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.173   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.188   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.188   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.081   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.081   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.106          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.104       6000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.142       6000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.056       6000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.051       6000.000  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p1\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p1_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p4\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p4_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p6\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p6_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p1_5K = np.nanmean(fp_div_list_baseline2_p1_5K)\n",
    "media_fp_div_list_baseline2_p1_5K_primi10 = np.nanmean(fp_div_list_baseline2_p1_5K[:10])\n",
    "media_fp_div_list_baseline2_p1_5K_primi20 = np.nanmean(fp_div_list_baseline2_p1_5K[:20])\n",
    "media_fp_div_list_baseline2_p1_5K_primi40 = np.nanmean(fp_div_list_baseline2_p1_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K = max(abs(x) for x in fp_div_list_baseline2_p1_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p4_5K = np.nanmean(fp_div_list_baseline2_p4_5K)\n",
    "media_fp_div_list_baseline2_p4_5K_primi10 = np.nanmean(fp_div_list_baseline2_p4_5K[:10])\n",
    "media_fp_div_list_baseline2_p4_5K_primi20 = np.nanmean(fp_div_list_baseline2_p4_5K[:20])\n",
    "media_fp_div_list_baseline2_p4_5K_primi40 = np.nanmean(fp_div_list_baseline2_p4_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K = max(abs(x) for x in fp_div_list_baseline2_p4_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p6_5K = np.nanmean(fp_div_list_baseline2_p6_5K)\n",
    "media_fp_div_list_baseline2_p6_5K_primi10 = np.nanmean(fp_div_list_baseline2_p6_5K[:10])\n",
    "media_fp_div_list_baseline2_p6_5K_primi20 = np.nanmean(fp_div_list_baseline2_p6_5K[:20])\n",
    "media_fp_div_list_baseline2_p6_5K_primi40 = np.nanmean(fp_div_list_baseline2_p6_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K = max(abs(x) for x in fp_div_list_baseline2_p6_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p1_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K,\n",
    "        media_fp_div_list_baseline2_p1_5K_primi10, media_fp_div_list_baseline2_p1_5K_primi20, media_fp_div_list_baseline2_p1_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p4_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K,\n",
    "        media_fp_div_list_baseline2_p4_5K_primi10, media_fp_div_list_baseline2_p4_5K_primi20, media_fp_div_list_baseline2_p4_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p6_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K,\n",
    "        media_fp_div_list_baseline2_p6_5K_primi10, media_fp_div_list_baseline2_p6_5K_primi20, media_fp_div_list_baseline2_p6_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 7000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation N = 5000</th>\n",
       "      <td>0.808</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.635</td>\n",
       "      <td>256</td>\n",
       "      <td>995</td>\n",
       "      <td>21014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 8000 p_class 0 = 0.5</th>\n",
       "      <td>0.772</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.446</td>\n",
       "      <td>784</td>\n",
       "      <td>700</td>\n",
       "      <td>21014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 8000 p_class 0 = 0.6</th>\n",
       "      <td>0.752</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.408</td>\n",
       "      <td>977</td>\n",
       "      <td>639</td>\n",
       "      <td>21014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 8000 p_class 0 = 0.7</th>\n",
       "      <td>0.737</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.367</td>\n",
       "      <td>1137</td>\n",
       "      <td>576</td>\n",
       "      <td>21014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 8000 p_class 0 = 0.8</th>\n",
       "      <td>0.709</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.327</td>\n",
       "      <td>1382</td>\n",
       "      <td>513</td>\n",
       "      <td>21014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 8000 p_class 0 = 0.9</th>\n",
       "      <td>0.687</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.315</td>\n",
       "      <td>1542</td>\n",
       "      <td>494</td>\n",
       "      <td>21014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 8000 p_class 0 = 1</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.286</td>\n",
       "      <td>1766</td>\n",
       "      <td>448</td>\n",
       "      <td>21014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                               Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation                        0.809     0.474                0.047   \n",
       "After RANDOM mitigation N = 5000         0.808     0.478                0.052   \n",
       "After SMOTE N = 8000 p_class 0 = 0.5     0.772     0.539                0.159   \n",
       "After SMOTE N = 8000 p_class 0 = 0.6     0.752     0.535                0.198   \n",
       "After SMOTE N = 8000 p_class 0 = 0.7     0.737     0.537                0.230   \n",
       "After SMOTE N = 8000 p_class 0 = 0.8     0.709     0.527                0.280   \n",
       "After SMOTE N = 8000 p_class 0 = 0.9     0.687     0.513                0.312   \n",
       "After SMOTE N = 8000 p_class 0 = 1       0.660     0.503                0.357   \n",
       "\n",
       "Metrics                               False Negative Rate  False Positives  \\\n",
       "Before Mitigation                                   0.643              234   \n",
       "After RANDOM mitigation N = 5000                    0.635              256   \n",
       "After SMOTE N = 8000 p_class 0 = 0.5                0.446              784   \n",
       "After SMOTE N = 8000 p_class 0 = 0.6                0.408              977   \n",
       "After SMOTE N = 8000 p_class 0 = 0.7                0.367             1137   \n",
       "After SMOTE N = 8000 p_class 0 = 0.8                0.327             1382   \n",
       "After SMOTE N = 8000 p_class 0 = 0.9                0.315             1542   \n",
       "After SMOTE N = 8000 p_class 0 = 1                  0.286             1766   \n",
       "\n",
       "Metrics                               False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                                1008       13014       6508  \n",
       "After RANDOM mitigation N = 5000                  995       21014       6508  \n",
       "After SMOTE N = 8000 p_class 0 = 0.5              700       21014       6508  \n",
       "After SMOTE N = 8000 p_class 0 = 0.6              639       21014       6508  \n",
       "After SMOTE N = 8000 p_class 0 = 0.7              576       21014       6508  \n",
       "After SMOTE N = 8000 p_class 0 = 0.8              513       21014       6508  \n",
       "After SMOTE N = 8000 p_class 0 = 0.9              494       21014       6508  \n",
       "After SMOTE N = 8000 p_class 0 = 1                448       21014       6508  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 8000\n",
    "#p1 = 0.5\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {1: count_1 + 4000, 0: count_0 + 4000}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "#p2 = 0.6 \n",
    "sampling_strategy = {1: count_1 + 4800, 0: count_0 + 3200}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#p3 = 0.7\n",
    "sampling_strategy = {1: count_1 + 5600, 0: count_0 + 2400}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "#p4 = 0.8\n",
    "sampling_strategy = {1: count_1 + 6400, 0: count_0 + 1600}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "#p5 = 0.9\n",
    "sampling_strategy = {1: count_1 + 7200, 0: count_0 + 800}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#p6 = 1\n",
    "sampling_strategy = {1: count_1 + 8000, 0: count_0 }\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 8000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 8000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 8000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 8000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 8000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 8000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_8K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_8K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[3]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[5]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[3]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[5]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.103</td>\n",
       "      <td>8000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.752</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.107</td>\n",
       "      <td>8000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.709</td>\n",
       "      <td>0.527</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.029</td>\n",
       "      <td>8000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.503</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.027</td>\n",
       "      <td>8000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.106   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.476             0.103   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.752     0.535             0.028   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.709     0.527            -0.039   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.660     0.503            -0.046   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.345               0.246   \n",
       "After RANDOM Mitigation(K=5 fp)            0.349               0.250   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.198               0.154   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.189               0.066   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.232               0.063   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.173   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.188   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.133   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.051   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.048   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.106          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.103       8000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.107       8000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.029       8000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.027       8000.000  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p1\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p1_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p4\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p4_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p6\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p6_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p1_5K = np.nanmean(fp_div_list_baseline2_p1_5K)\n",
    "media_fp_div_list_baseline2_p1_5K_primi10 = np.nanmean(fp_div_list_baseline2_p1_5K[:10])\n",
    "media_fp_div_list_baseline2_p1_5K_primi20 = np.nanmean(fp_div_list_baseline2_p1_5K[:20])\n",
    "media_fp_div_list_baseline2_p1_5K_primi40 = np.nanmean(fp_div_list_baseline2_p1_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K = max(abs(x) for x in fp_div_list_baseline2_p1_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p4_5K = np.nanmean(fp_div_list_baseline2_p4_5K)\n",
    "media_fp_div_list_baseline2_p4_5K_primi10 = np.nanmean(fp_div_list_baseline2_p4_5K[:10])\n",
    "media_fp_div_list_baseline2_p4_5K_primi20 = np.nanmean(fp_div_list_baseline2_p4_5K[:20])\n",
    "media_fp_div_list_baseline2_p4_5K_primi40 = np.nanmean(fp_div_list_baseline2_p4_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K = max(abs(x) for x in fp_div_list_baseline2_p4_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p6_5K = np.nanmean(fp_div_list_baseline2_p6_5K)\n",
    "media_fp_div_list_baseline2_p6_5K_primi10 = np.nanmean(fp_div_list_baseline2_p6_5K[:10])\n",
    "media_fp_div_list_baseline2_p6_5K_primi20 = np.nanmean(fp_div_list_baseline2_p6_5K[:20])\n",
    "media_fp_div_list_baseline2_p6_5K_primi40 = np.nanmean(fp_div_list_baseline2_p6_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K = max(abs(x) for x in fp_div_list_baseline2_p6_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p1_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K,\n",
    "        media_fp_div_list_baseline2_p1_5K_primi10, media_fp_div_list_baseline2_p1_5K_primi20, media_fp_div_list_baseline2_p1_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p4_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K,\n",
    "        media_fp_div_list_baseline2_p4_5K_primi10, media_fp_div_list_baseline2_p4_5K_primi20, media_fp_div_list_baseline2_p4_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p6_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K,\n",
    "        media_fp_div_list_baseline2_p6_5K_primi10, media_fp_div_list_baseline2_p6_5K_primi20, media_fp_div_list_baseline2_p6_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/mariaantoniettalongo/Desktop/TESI/.tesi/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.643</td>\n",
       "      <td>234</td>\n",
       "      <td>1008</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation N = 5000</th>\n",
       "      <td>0.807</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.629</td>\n",
       "      <td>272</td>\n",
       "      <td>987</td>\n",
       "      <td>22014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 9000 p_class 0 = 0.5</th>\n",
       "      <td>0.768</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.426</td>\n",
       "      <td>841</td>\n",
       "      <td>668</td>\n",
       "      <td>22014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 9000 p_class 0 = 0.6</th>\n",
       "      <td>0.749</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.364</td>\n",
       "      <td>1064</td>\n",
       "      <td>571</td>\n",
       "      <td>22014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 9000 p_class 0 = 0.7</th>\n",
       "      <td>0.724</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.358</td>\n",
       "      <td>1237</td>\n",
       "      <td>562</td>\n",
       "      <td>22014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 9000 p_class 0 = 0.8</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.322</td>\n",
       "      <td>1452</td>\n",
       "      <td>505</td>\n",
       "      <td>22014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 9000 p_class 0 = 0.9</th>\n",
       "      <td>0.679</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.298</td>\n",
       "      <td>1623</td>\n",
       "      <td>467</td>\n",
       "      <td>22014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 9000 p_class 0 = 1</th>\n",
       "      <td>0.651</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.291</td>\n",
       "      <td>1816</td>\n",
       "      <td>456</td>\n",
       "      <td>22014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                               Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation                        0.809     0.474                0.047   \n",
       "After RANDOM mitigation N = 5000         0.807     0.480                0.055   \n",
       "After SMOTE N = 9000 p_class 0 = 0.5     0.768     0.544                0.170   \n",
       "After SMOTE N = 9000 p_class 0 = 0.6     0.749     0.549                0.215   \n",
       "After SMOTE N = 9000 p_class 0 = 0.7     0.724     0.528                0.250   \n",
       "After SMOTE N = 9000 p_class 0 = 0.8     0.699     0.521                0.294   \n",
       "After SMOTE N = 9000 p_class 0 = 0.9     0.679     0.513                0.329   \n",
       "After SMOTE N = 9000 p_class 0 = 1       0.651     0.495                0.368   \n",
       "\n",
       "Metrics                               False Negative Rate  False Positives  \\\n",
       "Before Mitigation                                   0.643              234   \n",
       "After RANDOM mitigation N = 5000                    0.629              272   \n",
       "After SMOTE N = 9000 p_class 0 = 0.5                0.426              841   \n",
       "After SMOTE N = 9000 p_class 0 = 0.6                0.364             1064   \n",
       "After SMOTE N = 9000 p_class 0 = 0.7                0.358             1237   \n",
       "After SMOTE N = 9000 p_class 0 = 0.8                0.322             1452   \n",
       "After SMOTE N = 9000 p_class 0 = 0.9                0.298             1623   \n",
       "After SMOTE N = 9000 p_class 0 = 1                  0.291             1816   \n",
       "\n",
       "Metrics                               False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                                1008       13014       6508  \n",
       "After RANDOM mitigation N = 5000                  987       22014       6508  \n",
       "After SMOTE N = 9000 p_class 0 = 0.5              668       22014       6508  \n",
       "After SMOTE N = 9000 p_class 0 = 0.6              571       22014       6508  \n",
       "After SMOTE N = 9000 p_class 0 = 0.7              562       22014       6508  \n",
       "After SMOTE N = 9000 p_class 0 = 0.8              505       22014       6508  \n",
       "After SMOTE N = 9000 p_class 0 = 0.9              467       22014       6508  \n",
       "After SMOTE N = 9000 p_class 0 = 1                456       22014       6508  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 9000\n",
    "#p1 = 0.5\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {1: count_1 + 4500, 0: count_0 + 4500}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "#p2 = 0.6 \n",
    "sampling_strategy = {1: count_1 + 5400, 0: count_0 + 2600}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#p3 = 0.7\n",
    "sampling_strategy = {1: count_1 + 6300, 0: count_0 + 2700}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "#p4 = 0.8\n",
    "sampling_strategy = {1: count_1 + 7200, 0: count_0 + 1800}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "#p5 = 0.9\n",
    "sampling_strategy = {1: count_1 + 8100, 0: count_0 + 900}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#p6 = 1\n",
    "sampling_strategy = {1: count_1 + 9000, 0: count_0}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = LogisticRegression(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = LogisticRegression(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 9000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 9000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 9000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 9000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 9000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 9000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_9K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_9K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[3]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[5]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[3]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[5]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.107</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.749</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.088</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.521</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.033</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.651</td>\n",
       "      <td>0.495</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.037</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.809     0.474             0.106   \n",
       "After RANDOM Mitigation(K=5 fp)             0.809     0.476             0.107   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.749     0.549             0.015   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.699     0.521            -0.040   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.651     0.495            -0.040   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.345               0.246   \n",
       "After RANDOM Mitigation(K=5 fp)            0.352               0.253   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.161               0.127   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.203               0.073   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.244               0.078   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.173   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.188   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.110   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.056   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.060   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.106          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.107       9000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.088       9000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.033       9000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.037       9000.000  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p1\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p1_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p4\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p4_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p6\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p6_5K = df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['fn'] =  get_false_negative_rate_outcome(y_trues, y_preds)\n",
    "\n",
    "#aggiungo la feature 'fn' a df_val non encoded\n",
    "df_test['fn'] = df_test_class['fn']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['fn'])\n",
    "FP_fm = FP_fm.sort_values(by='fn_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'fn')\n",
    "df_pruned_fp = fp_details.redundancy_pruning(th_redundancy=epsilon)\n",
    "df_pruned_fp = df_pruned_fp.sort_values('fn_div', ascending=False)\n",
    "df_pruned_fp.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_fp['fn_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p1_5K = np.nanmean(fp_div_list_baseline2_p1_5K)\n",
    "media_fp_div_list_baseline2_p1_5K_primi10 = np.nanmean(fp_div_list_baseline2_p1_5K[:10])\n",
    "media_fp_div_list_baseline2_p1_5K_primi20 = np.nanmean(fp_div_list_baseline2_p1_5K[:20])\n",
    "media_fp_div_list_baseline2_p1_5K_primi40 = np.nanmean(fp_div_list_baseline2_p1_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K = max(abs(x) for x in fp_div_list_baseline2_p1_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p4_5K = np.nanmean(fp_div_list_baseline2_p4_5K)\n",
    "media_fp_div_list_baseline2_p4_5K_primi10 = np.nanmean(fp_div_list_baseline2_p4_5K[:10])\n",
    "media_fp_div_list_baseline2_p4_5K_primi20 = np.nanmean(fp_div_list_baseline2_p4_5K[:20])\n",
    "media_fp_div_list_baseline2_p4_5K_primi40 = np.nanmean(fp_div_list_baseline2_p4_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K = max(abs(x) for x in fp_div_list_baseline2_p4_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p6_5K = np.nanmean(fp_div_list_baseline2_p6_5K)\n",
    "media_fp_div_list_baseline2_p6_5K_primi10 = np.nanmean(fp_div_list_baseline2_p6_5K[:10])\n",
    "media_fp_div_list_baseline2_p6_5K_primi20 = np.nanmean(fp_div_list_baseline2_p6_5K[:20])\n",
    "media_fp_div_list_baseline2_p6_5K_primi40 = np.nanmean(fp_div_list_baseline2_p6_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K = max(abs(x) for x in fp_div_list_baseline2_p6_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p1_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p1_5K,\n",
    "        media_fp_div_list_baseline2_p1_5K_primi10, media_fp_div_list_baseline2_p1_5K_primi20, media_fp_div_list_baseline2_p1_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p4_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p4_5K,\n",
    "        media_fp_div_list_baseline2_p4_5K_primi10, media_fp_div_list_baseline2_p4_5K_primi20, media_fp_div_list_baseline2_p4_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p6_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p6_5K,\n",
    "        media_fp_div_list_baseline2_p6_5K_primi10, media_fp_div_list_baseline2_p6_5K_primi20, media_fp_div_list_baseline2_p6_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOT: andamento di falsi positivi e di falsi negativi al variare di N e p di appartenere alla classe 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creazione del grafico\n",
    "plt.figure(figsize=(10, 6))\n",
    "p =[0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# Linea per i falsi positivi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(p, falsi_negativi_2K_fp_5sub, marker='o', label='False Negatives 2K  ', color='black')\n",
    "plt.plot(p, falsi_negativi_3K_fp_5sub, marker='o', label='False Negatives 3K  ', color='red')\n",
    "plt.plot(p, falsi_negativi_4K_fp_5sub, marker='o', label='False Negatives 4K  ', color='green')\n",
    "plt.plot(p, falsi_negativi_5K_fp_5sub, marker='o', label='False Negatives 5K  ', color='blue')\n",
    "plt.plot(p, falsi_negativi_6K_fp_5sub, marker='o', label='False Negatives 6K  ', color='orange')\n",
    "\n",
    "\n",
    "plt.axhline(y=falsi_negativi_5K_fp_5sub_before, color='purple', linestyle='--', label='Initial False Negatives')\n",
    "\n",
    "# Etichette e titolo\n",
    "plt.xlabel('probability for the synthetic point to belong to the class 1')\n",
    "plt.ylabel('Count False Negaitives')\n",
    "plt.title(f'False Negatives, fn, #subgroups = {K} on {filtered_instances}, support = {min_sup}, pruning = {epsilon}')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "plt.yticks(range(550, 660, 25))\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3574, 534)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_0, count_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
