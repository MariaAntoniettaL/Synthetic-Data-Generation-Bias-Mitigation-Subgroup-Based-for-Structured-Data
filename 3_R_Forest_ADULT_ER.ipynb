{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ANALISI CONDOTTA CON LA FEATURE error rate (PASSATA A BOOLEAN OUTCOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_for_adult import preprocessing_funct_not_enc, encoding_funct, K_subgroups_dataset_and_or, metrics_to_compare,encoding_funct_SMOTE\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "\n",
    "      \n",
    "from divexplorer import DivergenceExplorer\n",
    "from divexplorer import DivergencePatternProcessor       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "df = pd.read_csv(\"adult.data\", header = None, names = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "pd.options.display.float_format = '{:.3f}'.format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup = 0.15\n",
    "percentage = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET ROWS:  13014\n",
      "VALIDATION SET ROWS:  6507\n",
      "HOLDOUT SET ROWS:  6508\n",
      "TEST SET ROWS:  6508\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val, df_test, df_holdout = preprocessing_funct_not_enc(df)\n",
    "#controllo divisione dataset\n",
    "print(f\"TRAIN SET ROWS: \", df_train.shape[0]) #su 32536, il 40%  dovrebbe essere circa 13014\n",
    "print(f\"VALIDATION SET ROWS: \", df_val.shape[0]) #su 32536, il 20%  dovrebbe essere circa 6500\n",
    "print(f\"HOLDOUT SET ROWS: \", df_holdout.shape[0])\n",
    "print(f\"TEST SET ROWS: \", df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET ROWS:  13014\n",
      "VALIDATION SET ROWS:  6507\n",
      "HOLDOUT SET ROWS:  6508\n",
      "TEST SET ROWS:  6508\n"
     ]
    }
   ],
   "source": [
    "df_train_enc, df_test_enc, df_holdout_enc, df_val_enc = encoding_funct(df_train=df_train, df_test=df_test, df_holdout=df_holdout, df_val=df_val)\n",
    "#controllo coerenza con numerosità precedente\n",
    "print(f\"TRAIN SET ROWS: \", df_train_enc.shape[0]) #su 32536, il 40%  dovrebbe essere circa 13014\n",
    "print(f\"VALIDATION SET ROWS: \", df_val_enc.shape[0]) #su 32536, il 20%  dovrebbe essere circa 6500\n",
    "print(f\"HOLDOUT SET ROWS: \", df_holdout_enc.shape[0])\n",
    "print(f\"TEST SET ROWS: \", df_test_enc.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GDecisioN tree predictions\n",
    "X_train = df_train_enc.drop(columns = 'income', axis = 1)\n",
    "y_train = df_train_enc['income']\n",
    "\n",
    "X_test = df_test_enc.drop(columns = 'income', axis = 1)\n",
    "y_test = df_test_enc['income']\n",
    "\n",
    "X_val = df_val_enc.drop(columns = 'income', axis = 1)\n",
    "y_val = df_val_enc['income']\n",
    "\n",
    "X_holdout = df_holdout_enc.drop(columns = 'income', axis = 1)\n",
    "y_holdout = df_holdout_enc['income']\n",
    "\n",
    "classifier_train = RandomForestClassifier(random_state=seed)\n",
    "classifier_train.fit(X_train, y_train)\n",
    "y_pred = classifier_train.predict(X_test)\n",
    "cm_classifier = confusion_matrix(y_test, y_pred)\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=cm_classifier, display_labels=[False, True])\n",
    "#disp.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.381</td>\n",
       "      <td>426</td>\n",
       "      <td>598</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics            Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation     0.843     0.655                0.086   \n",
       "\n",
       "Metrics            False Negative Rate  False Positives  False Negatives  \\\n",
       "Before Mitigation                0.381              426              598   \n",
       "\n",
       "Metrics            Test Size  \n",
       "Before Mitigation       6508  "
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before = metrics_to_compare(y_true = y_test, y_pred = y_pred )\n",
    "metrics_before_df = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Test Size'],\n",
    "    'Before Mitigation': [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_test)],\n",
    "})\n",
    "metrics_before_df = metrics_before_df.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_before_df[metric] = metrics_before_df[metric].astype(int)\n",
    "\n",
    "metrics_before_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBGROUPS SEARCH\n",
    "\n",
    "-Identifico i gruppi applicando DivExplorer sul validation not encoded (a cui ho aggiunto la feature sui falsi positivi da passare a boolean outcomes e la feature accuracy che vale 1 se la predizione è giusta e 0 se sbagliata )\n",
    "\n",
    "-Integro nel training set dati che matchano sottogruppi problematici prendendoli dall'holdout, (primi K = 5, tutte le righe holdout che matchano)\n",
    "\n",
    "-Ripeto training e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>native-country</th>\n",
       "      <th>age_group</th>\n",
       "      <th>edu_num_group</th>\n",
       "      <th>hours_per_week_group</th>\n",
       "      <th>y_val_true</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18761</th>\n",
       "      <td>2</td>\n",
       "      <td>0.077</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27582</th>\n",
       "      <td>3</td>\n",
       "      <td>0.048</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30911</th>\n",
       "      <td>2</td>\n",
       "      <td>0.174</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11128</th>\n",
       "      <td>0</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.507</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>0</td>\n",
       "      <td>0.284</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       workclass  fnlwgt  education  marital-status  occupation  relationship  \\\n",
       "18761          2   0.077          3               0           4             1   \n",
       "27582          3   0.048          3               1           4             0   \n",
       "30911          2   0.174          3               3           4             4   \n",
       "11128          0   0.012          2               1           2             0   \n",
       "683            0   0.284          3               3           2             4   \n",
       "\n",
       "       race  sex  capital-gain  capital-loss  native-country  age_group  \\\n",
       "18761     4    0         0.000         0.000               5          2   \n",
       "27582     4    1         0.000         0.000               5          5   \n",
       "30911     2    0         0.039         0.000               5          1   \n",
       "11128     2    1         0.000         0.507               5          3   \n",
       "683       2    1         0.000         0.000               5          0   \n",
       "\n",
       "       edu_num_group  hours_per_week_group  y_val_true  y_pred  \n",
       "18761              1                     1           0       0  \n",
       "27582              1                     2           1       0  \n",
       "30911              1                     1           0       0  \n",
       "11128              4                     1           1       1  \n",
       "683                1                     1           0       0  "
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predizioni per il validation set\n",
    "y_pred_val_dt = classifier_train.predict(X_val)\n",
    "\n",
    "df_val_class = X_val.copy()\n",
    "df_val_class['y_val_true'] = y_val\n",
    "df_val_class['y_pred'] = y_pred_val_dt\n",
    "\n",
    "df_val_class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trues = df_val_class[\"y_val_true\"]\n",
    "y_preds = df_val_class[\"y_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "      <th>age_group</th>\n",
       "      <th>edu_num_group</th>\n",
       "      <th>hours_per_week_group</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18761</th>\n",
       "      <td>Private</td>\n",
       "      <td>0.077</td>\n",
       "      <td>Non Graduated</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Self-emp-occ</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>35-44</td>\n",
       "      <td>10 College</td>\n",
       "      <td>Overtime</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27582</th>\n",
       "      <td>Self-emp</td>\n",
       "      <td>0.048</td>\n",
       "      <td>Non Graduated</td>\n",
       "      <td>Married</td>\n",
       "      <td>Self-emp-occ</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>65-100</td>\n",
       "      <td>10 College</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30911</th>\n",
       "      <td>Private</td>\n",
       "      <td>0.174</td>\n",
       "      <td>Non Graduated</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Self-emp-occ</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>25-34</td>\n",
       "      <td>10 College</td>\n",
       "      <td>Overtime</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11128</th>\n",
       "      <td>Government</td>\n",
       "      <td>0.012</td>\n",
       "      <td>Master's Degree</td>\n",
       "      <td>Married</td>\n",
       "      <td>Private-occ</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.507</td>\n",
       "      <td>United-States</td>\n",
       "      <td>1</td>\n",
       "      <td>45-54</td>\n",
       "      <td>14 Master's Degree</td>\n",
       "      <td>Overtime</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>Government</td>\n",
       "      <td>0.284</td>\n",
       "      <td>Non Graduated</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Private-occ</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>17-24</td>\n",
       "      <td>10 College</td>\n",
       "      <td>Overtime</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        workclass  fnlwgt        education marital-status    occupation  \\\n",
       "18761     Private   0.077    Non Graduated       Divorced  Self-emp-occ   \n",
       "27582    Self-emp   0.048    Non Graduated        Married  Self-emp-occ   \n",
       "30911     Private   0.174    Non Graduated      Separated  Self-emp-occ   \n",
       "11128  Government   0.012  Master's Degree        Married   Private-occ   \n",
       "683    Government   0.284    Non Graduated      Separated   Private-occ   \n",
       "\n",
       "         relationship    race      sex  capital-gain  capital-loss  \\\n",
       "18761   Not-in-family   White   Female         0.000         0.000   \n",
       "27582         Husband   White     Male         0.000         0.000   \n",
       "30911       Unmarried   Black   Female         0.039         0.000   \n",
       "11128         Husband   Black     Male         0.000         0.507   \n",
       "683         Unmarried   Black     Male         0.000         0.000   \n",
       "\n",
       "      native-country  income age_group       edu_num_group  \\\n",
       "18761  United-States       0     35-44          10 College   \n",
       "27582  United-States       1    65-100          10 College   \n",
       "30911  United-States       0     25-34          10 College   \n",
       "11128  United-States       1     45-54  14 Master's Degree   \n",
       "683    United-States       0     17-24          10 College   \n",
       "\n",
       "      hours_per_week_group  y_pred  error  \n",
       "18761             Overtime       0      0  \n",
       "27582            Part-time       0      1  \n",
       "30911             Overtime       0      0  \n",
       "11128             Overtime       1      0  \n",
       "683               Overtime       0      0  "
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#aggiungo la feature accuracy a df_val non encoded che assume valore 1 se la predizione è giusta 0 se la predizione è sbagliata\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_val['y_pred'] = df_val_class['y_pred'] \n",
    "df_val['error'] = (df_val_class['y_val_true'] != df_val_class['y_pred']).astype(int)\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemset</th>\n",
       "      <th>error</th>\n",
       "      <th>error_div</th>\n",
       "      <th>error_t</th>\n",
       "      <th>length</th>\n",
       "      <th>support_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.159</td>\n",
       "      <td>(race= White, sex= Male, marital-status=Married, relationship= Husband, native-country=United-States, workclass=Private, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.243</td>\n",
       "      <td>15.306</td>\n",
       "      <td>9</td>\n",
       "      <td>1034.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.159</td>\n",
       "      <td>(race= White, sex= Male, relationship= Husband, native-country=United-States, workclass=Private, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.243</td>\n",
       "      <td>15.306</td>\n",
       "      <td>8</td>\n",
       "      <td>1034.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.159</td>\n",
       "      <td>(race= White, marital-status=Married, relationship= Husband, native-country=United-States, workclass=Private, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.243</td>\n",
       "      <td>15.306</td>\n",
       "      <td>8</td>\n",
       "      <td>1034.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.159</td>\n",
       "      <td>(race= White, relationship= Husband, native-country=United-States, workclass=Private, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.243</td>\n",
       "      <td>15.306</td>\n",
       "      <td>7</td>\n",
       "      <td>1034.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.161</td>\n",
       "      <td>(race= White, sex= Male, marital-status=Married, native-country=United-States, workclass=Private, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.241</td>\n",
       "      <td>15.282</td>\n",
       "      <td>8</td>\n",
       "      <td>1050.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   support  \\\n",
       "0    0.159   \n",
       "1    0.159   \n",
       "2    0.159   \n",
       "3    0.159   \n",
       "4    0.161   \n",
       "\n",
       "                                                                                                                                                                                       itemset  \\\n",
       "0  (race= White, sex= Male, marital-status=Married, relationship= Husband, native-country=United-States, workclass=Private, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)   \n",
       "1                          (race= White, sex= Male, relationship= Husband, native-country=United-States, workclass=Private, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)   \n",
       "2             (race= White, marital-status=Married, relationship= Husband, native-country=United-States, workclass=Private, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)   \n",
       "3                                     (race= White, relationship= Husband, native-country=United-States, workclass=Private, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)   \n",
       "4                         (race= White, sex= Male, marital-status=Married, native-country=United-States, workclass=Private, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)   \n",
       "\n",
       "   error  error_div  error_t  length  support_count  \n",
       "0  0.400      0.243   15.306       9       1034.000  \n",
       "1  0.400      0.243   15.306       8       1034.000  \n",
       "2  0.400      0.243   15.306       8       1034.000  \n",
       "3  0.400      0.243   15.306       7       1034.000  \n",
       "4  0.398      0.241   15.282       8       1050.000  "
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_diver = DivergenceExplorer(df_val)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = error_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by=[\"error_div\", \"error_t\"], ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "FP_fm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemset</th>\n",
       "      <th>length</th>\n",
       "      <th>support_count</th>\n",
       "      <th>error</th>\n",
       "      <th>error_div</th>\n",
       "      <th>error_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.239</td>\n",
       "      <td>(race= White, relationship= Husband, native-country=United-States, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)</td>\n",
       "      <td>6</td>\n",
       "      <td>1554.000</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.235</td>\n",
       "      <td>17.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.172</td>\n",
       "      <td>(race= White, relationship= Husband, native-country=United-States, workclass=Private, capital-gain=0.0, capital-loss=0.0)</td>\n",
       "      <td>6</td>\n",
       "      <td>1122.000</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.232</td>\n",
       "      <td>15.242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.262</td>\n",
       "      <td>(race= White, marital-status=Married, native-country=United-States, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)</td>\n",
       "      <td>6</td>\n",
       "      <td>1707.000</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.229</td>\n",
       "      <td>18.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.258</td>\n",
       "      <td>(relationship= Husband, native-country=United-States, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)</td>\n",
       "      <td>5</td>\n",
       "      <td>1679.000</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.223</td>\n",
       "      <td>17.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.197</td>\n",
       "      <td>(race= White, marital-status=Married, native-country=United-States, workclass=Private, capital-gain=0.0, capital-loss=0.0)</td>\n",
       "      <td>6</td>\n",
       "      <td>1285.000</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.222</td>\n",
       "      <td>15.592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    support  \\\n",
       "6     0.239   \n",
       "18    0.172   \n",
       "21    0.262   \n",
       "35    0.258   \n",
       "36    0.197   \n",
       "\n",
       "                                                                                                                                   itemset  \\\n",
       "6    (race= White, relationship= Husband, native-country=United-States, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)   \n",
       "18               (race= White, relationship= Husband, native-country=United-States, workclass=Private, capital-gain=0.0, capital-loss=0.0)   \n",
       "21  (race= White, marital-status=Married, native-country=United-States, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)   \n",
       "35                (relationship= Husband, native-country=United-States, hours_per_week_group=Overtime, capital-gain=0.0, capital-loss=0.0)   \n",
       "36              (race= White, marital-status=Married, native-country=United-States, workclass=Private, capital-gain=0.0, capital-loss=0.0)   \n",
       "\n",
       "    length  support_count  error  error_div  error_t  \n",
       "6        6       1554.000  0.393      0.235   17.841  \n",
       "18       6       1122.000  0.389      0.232   15.242  \n",
       "21       6       1707.000  0.386      0.229   18.125  \n",
       "35       5       1679.000  0.381      0.223   17.606  \n",
       "36       6       1285.000  0.380      0.222   15.592  "
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pruning \n",
    "error_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = error_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values(\"error_div\", ascending=False)\n",
    "df_pruned_error.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total subgroups 196\n",
      "total problematic 166\n"
     ]
    }
   ],
   "source": [
    "# Numero totale di istanze\n",
    "total_instances = len(df_pruned_error)\n",
    "\n",
    "# Numero di istanze con fp_div > 0 e fp_t > 2\n",
    "filtered_instances = len(df_pruned_error[(df_pruned_error['error_div'] > 0) & (df_pruned_error['error_t'] > 2)])\n",
    "\n",
    "print('total subgroups', total_instances)\n",
    "print('total problematic', filtered_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim original:  (1655, 7)\n",
      "Dim pruned th_redundancy  (196, 7)\n"
     ]
    }
   ],
   "source": [
    "prun_size = df_pruned_error.shape\n",
    "original_size = FP_fm.shape\n",
    "print(\"Dim original: \", original_size)\n",
    "print(\"Dim pruned th_redundancy \", prun_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = int((percentage / 100) * filtered_instances)\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIAS MITIGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIAS MITIGATION: ADDING DATA: prendo dati dall'hold-out e li aggiungo al train set, questi dati matchano gli itemset trovati prima (i primi 5)\n",
    "\n",
    "1. prendo dati dall'holdout con la funzione K_subgroups_dataset_and_or li aggiungo train \n",
    "2. riapplico encoding tutto\n",
    "3. Decision tree nuovamente e vedo come sono cambiate le performance (ad es Accuracy, false positive rate, false negative rate) overall e per sottogruppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holdout_filtered = K_subgroups_dataset_and_or(df_pruned_error, df_holdout, K) #da aggiungere a train set e ripetere train e test\n",
    "\n",
    "df_combinated = pd.concat([df_holdout_filtered, df_train], ignore_index=True)\n",
    "df_train_mitigated= df_combinated.sample(frac=1, random_state=seed).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2126\n"
     ]
    }
   ],
   "source": [
    "print(len(df_holdout_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET ROWS:  13014\n",
      "TRAIN SET MITIGATED ROWS:  15140\n",
      "VALIDATION SET ROWS:  6508\n",
      "FILTERED DF holdout ROWS:  2126\n",
      "TEST SET FILTERED ROWS:  6507\n"
     ]
    }
   ],
   "source": [
    "#riapplico funzione di encoding, ma al posto di holdout, uso il df filtrato che devo usare per inserire dati \n",
    "df_train_enc_mit, inutile1, inutile3, inutile2 = encoding_funct(df_train=df_train_mitigated, df_test=df_test, df_holdout=df_holdout_filtered, df_val=df_val)\n",
    "#controllo divisione dataset\n",
    "df_train_enc_mit_fp = df_train_enc_mit  \n",
    "print(f\"TRAIN SET ROWS: \", df_train_enc.shape[0])\n",
    "print(f\"TRAIN SET MITIGATED ROWS: \", df_train_enc_mit.shape[0]) #su 32536, il 40%  dovrebbe essere circa 13014\n",
    "print(f\"VALIDATION SET ROWS: \", inutile1.shape[0]) #su 32536, il 20%  dovrebbe essere circa 6500\n",
    "print(f\"FILTERED DF holdout ROWS: \", inutile3.shape[0])\n",
    "print(f\"TEST SET FILTERED ROWS: \", inutile2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mitigated = df_train_enc_mit.drop(columns = 'income', axis = 1)\n",
    "y_train_mitigated = df_train_enc_mit['income']\n",
    "\n",
    "\n",
    "classifier_train_mitigated = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "classifier_train_mitigated.fit(X_train_mitigated, y_train_mitigated)\n",
    "y_mitigated_pred = classifier_train_mitigated.predict(X_test)\n",
    "#cm_dt = confusion_matrix(y_test, y_mitigated_pred)\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=[False, True])\n",
    "#disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2126\n",
      "verifica : 2126\n"
     ]
    }
   ],
   "source": [
    "#per veriicare cosa accade se aggiungo in modo randomico lo stesso numero di righe al train, ripeto l'analisi facebdo mitigation con righe randomiche (uguali in numero)\n",
    "print(len(df_holdout_filtered))\n",
    "n = len(df_holdout_filtered)\n",
    "df_holdout_sampled = df_holdout_enc.sample(n=len(df_holdout_filtered), replace=True, random_state=seed)\n",
    "print(\"verifica :\", len(df_holdout_sampled)) #verifica\n",
    "\n",
    "\n",
    "\n",
    "df_combinated_random = pd.concat([df_holdout_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random= df_combinated_random.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "X_train_mitigated_random = df_train_mitigated_random.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random = df_train_mitigated_random['income']\n",
    "\n",
    "classifier_train_mitigated_random = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "classifier_train_mitigated_random.fit(X_train_mitigated_random, y_train_mitigated_random)\n",
    "y_mitigated_pred_random = classifier_train_mitigated_random.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.381</td>\n",
       "      <td>426</td>\n",
       "      <td>598</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, fp)</th>\n",
       "      <td>0.842</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.369</td>\n",
       "      <td>448</td>\n",
       "      <td>579</td>\n",
       "      <td>15140</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.385</td>\n",
       "      <td>436</td>\n",
       "      <td>604</td>\n",
       "      <td>15140</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                    Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation             0.843     0.655                0.086   \n",
       "After Mitigation(K=5, fp)     0.842     0.658                0.091   \n",
       "After RANDOM mitigation       0.840     0.650                0.088   \n",
       "\n",
       "Metrics                    False Negative Rate  False Positives  \\\n",
       "Before Mitigation                        0.381              426   \n",
       "After Mitigation(K=5, fp)                0.369              448   \n",
       "After RANDOM mitigation                  0.385              436   \n",
       "\n",
       "Metrics                    False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                      598       13014       6508  \n",
       "After Mitigation(K=5, fp)              579       15140       6508  \n",
       "After RANDOM mitigation                604       15140       6508  "
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_fp_after, f1_score_fp_after, fpr_fp_after, fnr_fp_after, fp_fp_after, fn_fp_after = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred )\n",
    "accuracy_fp_after_random, f1_score_fp_after_random, fpr_fp_after_random, fnr_fp_after_random, fp_fp_after_random, fn_fp_after_random= metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random)\n",
    "\n",
    "metrics_after_fp = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After Mitigation(K=5, fp)': [accuracy_fp_after, f1_score_fp_after, fpr_fp_after, fnr_fp_after, fp_fp_after, fn_fp_after, len(y_train_mitigated), len(y_test)],\n",
    "    'After RANDOM mitigation' : [accuracy_fp_after_random, f1_score_fp_after_random, fpr_fp_after_random, fnr_fp_after_random, fp_fp_after_random, fn_fp_after_random, len(y_train_mitigated_random), len(y_test)]\n",
    "})\n",
    "metrics_after_fp = metrics_after_fp.set_index('Metrics').T\n",
    "\n",
    "\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp[metric] = metrics_after_fp[metric].astype(int)\n",
    "\n",
    "metrics_after_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance su sottogruppi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation, on subgroups</th>\n",
       "      <td>0.665</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.467</td>\n",
       "      <td>330</td>\n",
       "      <td>392</td>\n",
       "      <td>13014</td>\n",
       "      <td>2154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5, on subgroups, fp)</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.437</td>\n",
       "      <td>350</td>\n",
       "      <td>367</td>\n",
       "      <td>15140</td>\n",
       "      <td>2154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5, on subgroups, fp)</th>\n",
       "      <td>0.666</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.464</td>\n",
       "      <td>331</td>\n",
       "      <td>389</td>\n",
       "      <td>15140</td>\n",
       "      <td>2154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                         Accuracy  F1 Score  \\\n",
       "Before Mitigation, on subgroups                    0.665     0.553   \n",
       "After Mitigation(K=5, on subgroups, fp)            0.667     0.568   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)     0.666     0.556   \n",
       "\n",
       "Metrics                                         False Positive Rate  \\\n",
       "Before Mitigation, on subgroups                               0.251   \n",
       "After Mitigation(K=5, on subgroups, fp)                       0.266   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                0.252   \n",
       "\n",
       "Metrics                                         False Negative Rate  \\\n",
       "Before Mitigation, on subgroups                               0.467   \n",
       "After Mitigation(K=5, on subgroups, fp)                       0.437   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)                0.464   \n",
       "\n",
       "Metrics                                         False Positives  \\\n",
       "Before Mitigation, on subgroups                             330   \n",
       "After Mitigation(K=5, on subgroups, fp)                     350   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)              331   \n",
       "\n",
       "Metrics                                         False Negatives  Train Size  \\\n",
       "Before Mitigation, on subgroups                             392       13014   \n",
       "After Mitigation(K=5, on subgroups, fp)                     367       15140   \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)              389       15140   \n",
       "\n",
       "Metrics                                         Test Size  \n",
       "Before Mitigation, on subgroups                      2154  \n",
       "After Mitigation(K=5, on subgroups, fp)              2154  \n",
       "After RANDOM Mitigation(K=5, on subgroups, fp)       2154  "
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_filtered_fp = K_subgroups_dataset_and_or(df_pruned_error, df_test, K)\n",
    "inutile, df_test_filtered_enc_fp, inutile2, inutile3 = encoding_funct(df_train, df_test_filtered_fp, df_holdout, df_val)\n",
    "\n",
    "X_test_filtered_fp = df_test_filtered_enc_fp.drop(columns='income', axis = 1)\n",
    "y_true_test_filtered_fp = df_test_filtered_enc_fp['income']\n",
    "\n",
    "y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after = classifier_train_mitigated.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_random = classifier_train_mitigated_random.predict(X_test_filtered_fp)\n",
    "\n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after, f1_score_fp_sottogruppi_after, fpr_fp_sottogruppi_after, fnr_fp_sottogruppi_after, fp_fp_sottogruppi_after, fn_fp_sottogruppi_after = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after )\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_random)\n",
    "\n",
    "\n",
    "metrics_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after, f1_score_fp_sottogruppi_after, fpr_fp_sottogruppi_after, fnr_fp_sottogruppi_after, fp_fp_sottogruppi_after, fn_fp_sottogruppi_after, len(y_train_mitigated), len(y_pred_test_filtered_fp_after)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(y_train_mitigated_random), len(y_pred_test_filtered_fp_after_random)]\n",
    "})\n",
    "metrics_after_fp_sottogruppi = metrics_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi[metric] = metrics_after_fp_sottogruppi[metric].astype(int)\n",
    "\n",
    "metrics_after_fp\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp\")\n",
    "metrics_after_fp_sottogruppi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALISI DIVERGENZE\n",
    "\n",
    "\n",
    "Per vedere cosa accade ai sottogruppi: vedo cosa succede alle divergenze del test dopo la mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp)</th>\n",
       "      <td>0.842</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.158</td>\n",
       "      <td>2126.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.153</td>\n",
       "      <td>2126.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                          Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                   0.843     0.655             0.069   \n",
       "After Mitigation(K=5 fp)            0.842     0.658             0.066   \n",
       "After RANDOM Mitigation(K=5 fp)     0.840     0.650             0.060   \n",
       "\n",
       "Metrics                          max div  media div primi 10  \\\n",
       "Before Mitigation                  0.195               0.182   \n",
       "After Mitigation(K=5 fp)           0.194               0.178   \n",
       "After RANDOM Mitigation(K=5 fp)    0.192               0.177   \n",
       "\n",
       "Metrics                          media div primi 20  media div primi 40  \\\n",
       "Before Mitigation                             0.174               0.159   \n",
       "After Mitigation(K=5 fp)                      0.169               0.158   \n",
       "After RANDOM Mitigation(K=5 fp)               0.169               0.153   \n",
       "\n",
       "Metrics                          # new samples  \n",
       "Before Mitigation                        0.000  \n",
       "After Mitigation(K=5 fp)              2126.000  \n",
       "After RANDOM Mitigation(K=5 fp)       2126.000  "
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by=\"error_div\", ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values(\"error_div\", ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "error_div_list_no_mitigation  = df_pruned_error[\"error_div\"].tolist()\n",
    "#error_div_list_no_mitigation\n",
    "\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred \n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by=\"error_div\", ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values(\"error_div\", ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "error_div_list_baseline1  = df_pruned_error[\"error_div\"].tolist()\n",
    "#error_div_list_baseline1\n",
    "\n",
    "#PER VEDERE COSA SUCCEDE ALLE DIVERGENZE DEI SOTTOGRUPPI CON MITIGATION RADOMICA\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by=\"error_div\", ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values(\"error_div\", ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "error_div_list_random_per_confrontare_con_baseline1  = df_pruned_error[\"error_div\"].tolist()\n",
    "#error_div_list_random_per_confrontare_con_baseline1\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_error_div_list_no_mitigation = np.nanmean(error_div_list_no_mitigation)\n",
    "media_error_div_list_nomitigation_primi10 = np.nanmean(error_div_list_no_mitigation[:10])\n",
    "media_error_div_list_nomitigation_primi20 = np.nanmean(error_div_list_no_mitigation[:20])\n",
    "media_error_div_list_nomitigation_primi40 = np.nanmean(error_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_error_div_no_mitigation = max(abs(x) for x in error_div_list_no_mitigation)\n",
    "\n",
    "media_error_div_list_baseline1 = np.nanmean(error_div_list_baseline1)\n",
    "media_error_div_list_baseline1_primi10 = np.nanmean(error_div_list_baseline1[:10])\n",
    "media_error_div_list_baseline1_primi20 = np.nanmean(error_div_list_baseline1[:20])\n",
    "media_error_div_list_baseline1_primi40 = np.nanmean(error_div_list_baseline1[:40])\n",
    "error_div_massimo_valore_assoluto_error_div_baseline1 = max(abs(x) for x in error_div_list_baseline1)\n",
    "\n",
    "media_error_div_list_random_per_confrontare_con_baseline1 = np.nanmean(error_div_list_random_per_confrontare_con_baseline1)\n",
    "media_error_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(error_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_error_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(error_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_error_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(error_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_error_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in error_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_correctness_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_error_div_list_no_mitigation, massimo_valore_assoluto_error_div_no_mitigation,\n",
    "        media_error_div_list_nomitigation_primi10, media_error_div_list_nomitigation_primi20, media_error_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after, f1_score_fp_after, media_error_div_list_baseline1, error_div_massimo_valore_assoluto_error_div_baseline1,\n",
    "        media_error_div_list_baseline1_primi10, media_error_div_list_baseline1_primi20, media_error_div_list_baseline1_primi40, len(df_holdout_filtered)\n",
    "    ],\n",
    "    'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_error_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_error_div_random_per_confrontare_con_baseline1, media_error_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_error_div_list_random_per_confrontare_con_baseline1_primi20, media_error_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        len(df_holdout_filtered)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_correctness_sottogruppi = divergence_after_correctness_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_correctness_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIAS MITIGATION SIMULANDO DATI ATTRAVERSO SMOTE\n",
    "\n",
    "SEGUE CODICE USANDO SMOTE \n",
    "DIVIDO IN TRAIN, TEST E VALIDATION -- ora uso quelli gia esistenti\n",
    "DIV EXPLORER SUL VALIDATION  -- già fatto \n",
    "GENERO NUOVI DATI CON SMOTE a partire dai dati di divexplorer sul validation\n",
    "INSERISCO QUESTI NUOVI DATI NEL TRAIN SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Righe del dataset filtrato qunado K = 5 2641\n",
      "numero di dati simulati con smotenc 3164\n",
      "income\n",
      "0    1582\n",
      "1    1582\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "df_val_filtered = K_subgroups_dataset_and_or(df_pruned_error, df_val, K)\n",
    "print(\"Righe del dataset filtrato qunado K = 5\",len(df_val_filtered))\n",
    "#df_val_filtered.head() #var categoriche e numeriche \n",
    "#print(len(df_val_filtered)) #--2610 con = 5\n",
    "df_val_filtered, inutile12, inutile222 = encoding_funct_SMOTE(df_val_filtered, df_test, df_holdout)\n",
    "X_to_SMOTE =  df_val_filtered.drop(columns = ['error', 'y_pred', 'income'], axis = 1)\n",
    "y_to_SMOTE = df_val_filtered['income']\n",
    "X_to_SMOTE.head()\n",
    "\n",
    "categorical_features = [0, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13]\n",
    "\n",
    "smote_nc = SMOTENC( categorical_features=categorical_features, random_state=seed)\n",
    "X_resampled, y_resampled = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    "\n",
    "print(\"numero di dati simulati con smotenc\",len(y_resampled))\n",
    "\n",
    "class_counts = y_resampled.value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16178\n"
     ]
    }
   ],
   "source": [
    "X_train_mitigated_SMOTE = pd.concat([X_train, X_resampled], ignore_index=True)\n",
    "y_train_mitigated_SMOTE = pd.concat([y_train, y_resampled], ignore_index=True)\n",
    "print(len(X_train_mitigated_SMOTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_train_mitigated_SMOTE = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mitigated_SMOTE.fit(X_train_mitigated_SMOTE, y_train_mitigated_SMOTE)\n",
    "y_mitigated_SMOTE_pred = classifier_train_mitigated_SMOTE.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#vediamo che succede se prendo lo stesso numero di righe ma random da holdout\\nprint(len(X_resampled))\\nn_random_smote = len(X_resampled)\\n\\ndf_holdout_smote_sampled = df_holdout_enc.sample(n=n_random_smote, random_state=seed)\\nprint(\"verifica :\", len(df_holdout_smote_sampled)) #verifica\\n\\ndf_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\\ndf_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\\n\\nX_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\\ny_train_mitigated_random_smote = df_train_mitigated_random_smote[\\'income\\']\\n\\nclassifier_train_mitigated_random_smote = RandomForestClassifier(random_state=seed)\\n\\nclassifier_train_mitigated_random_smote.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\\ny_mitigated_pred_random_smote = classifier_train_mitigated_random_smote.predict(X_test)\\n\\n'"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#vediamo che succede se prendo lo stesso numero di righe ma random da holdout\n",
    "print(len(X_resampled))\n",
    "n_random_smote = len(X_resampled)\n",
    "\n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=n_random_smote, random_state=seed)\n",
    "print(\"verifica :\", len(df_holdout_smote_sampled)) #verifica\n",
    "\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "\n",
    "classifier_train_mitigated_random_smote = RandomForestClassifier(random_state=seed)\n",
    "\n",
    "classifier_train_mitigated_random_smote.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote = classifier_train_mitigated_random_smote.predict(X_test)\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"accuracy_fp_after_SMOTE, f1_score_fp_after_SMOTE, fpr_fp_after_SMOTE, fnr_fp_after_SMOTE, fp_fp_after_SMOTE, fn_fp_after_SMOTE = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_SMOTE_pred )\\naccuracy_fp_after_SMOTE_random, f1_score_fp_after_SMOTE_random, fpr_fp_after_SMOTE_random, fnr_fp_after_SMOTE_random, fp_fp_after_SMOTE_random, fn_fp_after_SMOTE_random = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote )\\n\\n\\nmetrics_after_fp_SMOTE = pd.DataFrame({\\n    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\\n    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\\n    'After SMOTE fp mitigation' : [accuracy_fp_after_SMOTE, f1_score_fp_after_SMOTE, fpr_fp_after_SMOTE, fnr_fp_after_SMOTE, fp_fp_after_SMOTE, fn_fp_after_SMOTE, len(y_train_mitigated_SMOTE), len(y_mitigated_SMOTE_pred)],\\n    'After RANDOM mitigation' : [accuracy_fp_after_SMOTE_random, f1_score_fp_after_SMOTE_random, fpr_fp_after_SMOTE_random, fnr_fp_after_SMOTE_random, fp_fp_after_SMOTE_random, fn_fp_after_SMOTE_random, len(y_train_mitigated_random_smote), len(y_mitigated_pred_random_smote)]\\n    \\n})\\nmetrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\\nmetrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\\nfor metric in metrics_to_cast:\\n    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\\n    \\nmetrics_after_fp_SMOTE\""
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''accuracy_fp_after_SMOTE, f1_score_fp_after_SMOTE, fpr_fp_after_SMOTE, fnr_fp_after_SMOTE, fp_fp_after_SMOTE, fn_fp_after_SMOTE = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_SMOTE_pred )\n",
    "accuracy_fp_after_SMOTE_random, f1_score_fp_after_SMOTE_random, fpr_fp_after_SMOTE_random, fnr_fp_after_SMOTE_random, fp_fp_after_SMOTE_random, fn_fp_after_SMOTE_random = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote )\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After SMOTE fp mitigation' : [accuracy_fp_after_SMOTE, f1_score_fp_after_SMOTE, fpr_fp_after_SMOTE, fnr_fp_after_SMOTE, fp_fp_after_SMOTE, fn_fp_after_SMOTE, len(y_train_mitigated_SMOTE), len(y_mitigated_SMOTE_pred)],\n",
    "    'After RANDOM mitigation' : [accuracy_fp_after_SMOTE_random, f1_score_fp_after_SMOTE_random, fpr_fp_after_SMOTE_random, fnr_fp_after_SMOTE_random, fp_fp_after_SMOTE_random, fn_fp_after_SMOTE_random, len(y_train_mitigated_random_smote), len(y_mitigated_pred_random_smote)]\n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "    \n",
    "metrics_after_fp_SMOTE'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A QUESTO PUNTO POSSIAMO VEDERE LE PERFORMANCE SUI SOTTOGRUPPI PRIMA E DOPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \\ny_pred_test_filtered_fp_after_SMOTE = classifier_train_mitigated_SMOTE.predict(X_test_filtered_fp)\\ny_pred_RANDOM_subgroups = classifier_train_mitigated_random_smote.predict(X_test_filtered_fp)\\n\\n\\n#accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\\naccuracy_fp_sottogruppi_after_SMOTE, f1_score_fp_sottogruppi_after_SMOTE, fpr_fp_sottogruppi_after_SMOTE, fnr_fp_sottogruppi_after_SMOTE, fp_fp_sottogruppi_after_SMOTE, fn_fp_sottogruppi_after_SMOTE = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE )\\naccuracy_fp_sottogruppi_random_SMOTE, f1_score_fp_sottogruppi_random_SMOTE, fpr_fp_sottogruppi_random_SMOTE, fnr_fp_sottogruppi_random_SMOTE, fp_fp_sottogruppi_random_SMOTE, fn_fp_sottogruppi_random_SMOTE = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_RANDOM_subgroups )\\n\\nmetrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\\n    \\'Metrics\\' : [\\'Accuracy\\', \\'F1 Score\\', \\'False Positive Rate\\', \\'False Negative Rate\\', \\'False Positives\\', \\'False Negatives\\', \\'Train Size\\', \\'Test Size\\'],\\n    \\'Before Mitigation, on subgroups\\' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\\n    \\'After RANDOM mitigation, on subgroups\\' : [accuracy_fp_sottogruppi_random_SMOTE, f1_score_fp_sottogruppi_random_SMOTE, fpr_fp_sottogruppi_random_SMOTE, fnr_fp_sottogruppi_random_SMOTE, fp_fp_sottogruppi_random_SMOTE, fn_fp_sottogruppi_random_SMOTE, len(y_train), len(y_pred_RANDOM_subgroups)],\\n    \\'After Mitigation(K=5, on subgroups, fp and SMOTE)\\': [accuracy_fp_sottogruppi_after_SMOTE, f1_score_fp_sottogruppi_after_SMOTE, fpr_fp_sottogruppi_after_SMOTE, fnr_fp_sottogruppi_after_SMOTE, fp_fp_sottogruppi_after_SMOTE, fn_fp_sottogruppi_after_SMOTE, len(y_train_mitigated_SMOTE), len(y_pred_test_filtered_fp_after_SMOTE)],\\n})\\nmetrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index(\\'Metrics\\').T\\n\\nmetrics_to_cast = [\\'False Positives\\', \\'False Negatives\\', \\'Train Size\\', \\'Test Size\\']\\n\\nfor metric in metrics_to_cast:\\n    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\\n\\nmetrics_after_fp_SMOTE\\n\\n\\nprint(\"Subgroups Decision Tree performance when boolean outcomes = correctness e SMOTE \")\\nmetrics_after_fp_sottogruppi_SMOTE'"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "y_pred_test_filtered_fp_after_SMOTE = classifier_train_mitigated_SMOTE.predict(X_test_filtered_fp)\n",
    "y_pred_RANDOM_subgroups = classifier_train_mitigated_random_smote.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "#accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE, f1_score_fp_sottogruppi_after_SMOTE, fpr_fp_sottogruppi_after_SMOTE, fnr_fp_sottogruppi_after_SMOTE, fp_fp_sottogruppi_after_SMOTE, fn_fp_sottogruppi_after_SMOTE = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE )\n",
    "accuracy_fp_sottogruppi_random_SMOTE, f1_score_fp_sottogruppi_random_SMOTE, fpr_fp_sottogruppi_random_SMOTE, fnr_fp_sottogruppi_random_SMOTE, fp_fp_sottogruppi_random_SMOTE, fn_fp_sottogruppi_random_SMOTE = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_RANDOM_subgroups )\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM mitigation, on subgroups' : [accuracy_fp_sottogruppi_random_SMOTE, f1_score_fp_sottogruppi_random_SMOTE, fpr_fp_sottogruppi_random_SMOTE, fnr_fp_sottogruppi_random_SMOTE, fp_fp_sottogruppi_random_SMOTE, fn_fp_sottogruppi_random_SMOTE, len(y_train), len(y_pred_RANDOM_subgroups)],\n",
    "    'After Mitigation(K=5, on subgroups, fp and SMOTE)': [accuracy_fp_sottogruppi_after_SMOTE, f1_score_fp_sottogruppi_after_SMOTE, fpr_fp_sottogruppi_after_SMOTE, fnr_fp_sottogruppi_after_SMOTE, fp_fp_sottogruppi_after_SMOTE, fn_fp_sottogruppi_after_SMOTE, len(y_train_mitigated_SMOTE), len(y_pred_test_filtered_fp_after_SMOTE)],\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = correctness e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIAS MITIGATION SMOTENC\n",
    "\n",
    "- FISSO N VARIA p, p è la probabilità che il campione simulato sia di classe 0 qui (perchè voglio diminuire il numero di falsi positivi)\n",
    "- FISSO p VARIA N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Righe del dataset filtrato qunado K = 5 2641\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "df_val_filtered = K_subgroups_dataset_and_or(df_pruned_error, df_val, K)\n",
    "print(\"Righe del dataset filtrato qunado K = 5\",len(df_val_filtered))\n",
    "#df_val_filtered.head() #var categoriche e numeriche \n",
    "#print(len(df_val_filtered)) #--2610 con = 5\n",
    "df_val_filtered, inutile12, inutile222 = encoding_funct_SMOTE(df_val_filtered, df_test, df_holdout)\n",
    "X_to_SMOTE =  df_val_filtered.drop(columns = [ 'y_pred', 'error', 'income'], axis = 1)\n",
    "y_to_SMOTE = df_val_filtered['income']\n",
    "X_to_SMOTE.head()\n",
    "\n",
    "categorical_features = [0, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1059, 1582)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_1 = df_val_filtered['income'].sum()\n",
    "count_0 = len(df_val_filtered) - count_1\n",
    "count_1, count_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "per confronto con targeted\n",
    "N come len_df_holdout_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0=0\n",
    "p1 = 0.1\n",
    "p2 = 0.2\n",
    "p3 = 0.3 \n",
    "p4 = 0.4\n",
    "p5 = 0.5\n",
    "p6 = 0.6\n",
    "p7 = 0.7\n",
    "p8 = 0.8\n",
    "p9 = 0.9\n",
    "p10 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df_holdout_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.153</td>\n",
       "      <td>2126.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.170</td>\n",
       "      <td>2126.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.170</td>\n",
       "      <td>2126.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.169</td>\n",
       "      <td>2126.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.164</td>\n",
       "      <td>2126.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.843     0.655             0.069   \n",
       "After RANDOM Mitigation(K=5 fp)             0.840     0.650             0.060   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.839     0.662             0.073   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.836     0.640             0.073   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.837     0.630             0.075   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.839     0.625             0.069   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.195               0.182   \n",
       "After RANDOM Mitigation(K=5 fp)            0.192               0.177   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.215               0.197   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.210               0.193   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.206               0.193   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.200               0.186   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.174   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.169   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.186   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.183   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.183   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.177   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.159          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.153       2126.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.170       2126.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.170       2126.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.169       2126.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.164       2126.000  "
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = len(df_holdout_filtered)\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_2K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.158</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.160</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.174</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.156</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.160</td>\n",
       "      <td>2000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.843     0.655             0.069   \n",
       "After RANDOM Mitigation(K=5 fp)             0.840     0.650             0.070   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.837     0.657             0.084   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.839     0.648             0.084   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.836     0.629             0.065   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.839     0.625             0.069   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.195               0.182   \n",
       "After RANDOM Mitigation(K=5 fp)            0.191               0.178   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.206               0.191   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.203               0.194   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.203               0.184   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.200               0.183   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.174   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.170   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.181   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.186   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.173   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.174   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.159          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.158       2000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.160       2000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.174       2000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.156       2000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.160       2000.000  "
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 2000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_2K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.381</td>\n",
       "      <td>426</td>\n",
       "      <td>598</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation N = 5000</th>\n",
       "      <td>0.844</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.379</td>\n",
       "      <td>420</td>\n",
       "      <td>594</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0</th>\n",
       "      <td>0.834</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.330</td>\n",
       "      <td>562</td>\n",
       "      <td>517</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.1</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.338</td>\n",
       "      <td>537</td>\n",
       "      <td>530</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.2</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.351</td>\n",
       "      <td>511</td>\n",
       "      <td>550</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.3</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.355</td>\n",
       "      <td>504</td>\n",
       "      <td>556</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.4</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.370</td>\n",
       "      <td>479</td>\n",
       "      <td>580</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.5</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.384</td>\n",
       "      <td>447</td>\n",
       "      <td>602</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.6</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.403</td>\n",
       "      <td>427</td>\n",
       "      <td>632</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.7</th>\n",
       "      <td>0.838</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.409</td>\n",
       "      <td>413</td>\n",
       "      <td>641</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.8</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.423</td>\n",
       "      <td>406</td>\n",
       "      <td>663</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.9</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.430</td>\n",
       "      <td>376</td>\n",
       "      <td>675</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 4000 p_class 0 = 1</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.443</td>\n",
       "      <td>353</td>\n",
       "      <td>694</td>\n",
       "      <td>15014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                               Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation                        0.843     0.655                0.086   \n",
       "After RANDOM mitigation N = 5000         0.844     0.658                0.085   \n",
       "After SMOTE N = 5000 p_class 0 = 0       0.834     0.661                0.114   \n",
       "After SMOTE N = 5000 p_class 0 = 0.1     0.836     0.661                0.109   \n",
       "After SMOTE N = 5000 p_class 0 = 0.2     0.837     0.657                0.103   \n",
       "After SMOTE N = 5000 p_class 0 = 0.3     0.837     0.656                0.102   \n",
       "After SMOTE N = 5000 p_class 0 = 0.4     0.837     0.651                0.097   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5     0.839     0.648                0.090   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6     0.837     0.639                0.086   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7     0.838     0.638                0.084   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8     0.836     0.629                0.082   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9     0.839     0.630                0.076   \n",
       "After SMOTE N = 4000 p_class 0 = 1       0.839     0.625                0.071   \n",
       "\n",
       "Metrics                               False Negative Rate  False Positives  \\\n",
       "Before Mitigation                                   0.381              426   \n",
       "After RANDOM mitigation N = 5000                    0.379              420   \n",
       "After SMOTE N = 5000 p_class 0 = 0                  0.330              562   \n",
       "After SMOTE N = 5000 p_class 0 = 0.1                0.338              537   \n",
       "After SMOTE N = 5000 p_class 0 = 0.2                0.351              511   \n",
       "After SMOTE N = 5000 p_class 0 = 0.3                0.355              504   \n",
       "After SMOTE N = 5000 p_class 0 = 0.4                0.370              479   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5                0.384              447   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6                0.403              427   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7                0.409              413   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8                0.423              406   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9                0.430              376   \n",
       "After SMOTE N = 4000 p_class 0 = 1                  0.443              353   \n",
       "\n",
       "Metrics                               False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                                 598       13014       6508  \n",
       "After RANDOM mitigation N = 5000                  594       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0                517       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.1              530       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.2              550       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.3              556       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.4              580       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.5              602       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.6              632       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.7              641       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.8              663       15014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.9              675       15014       6508  \n",
       "After SMOTE N = 4000 p_class 0 = 1                694       15014       6508  "
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_after_fp_SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.169</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.151</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.167</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.158</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.159</td>\n",
       "      <td>3000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.843     0.655             0.069   \n",
       "After RANDOM Mitigation(K=5 fp)             0.840     0.650             0.077   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.839     0.666             0.074   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.836     0.644             0.074   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.837     0.630             0.067   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.837     0.618             0.069   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.195               0.182   \n",
       "After RANDOM Mitigation(K=5 fp)            0.203               0.189   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.199               0.185   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.207               0.191   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.201               0.183   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.203               0.186   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.174   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.181   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.175   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.181   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.173   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.176   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.159          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.169       3000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.151       3000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.167       3000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.158       3000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.159       3000.000  "
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 3000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_3K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.152</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.834</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.180</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.833</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.173</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.838</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.159</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.158</td>\n",
       "      <td>4000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.843     0.655             0.069   \n",
       "After RANDOM Mitigation(K=5 fp)             0.840     0.650             0.060   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.834     0.661             0.078   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.833     0.638             0.078   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.838     0.631             0.069   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.836     0.607             0.072   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.195               0.182   \n",
       "After RANDOM Mitigation(K=5 fp)            0.195               0.183   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.228               0.204   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.208               0.194   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.199               0.183   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.206               0.184   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.174   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.173   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.194   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.186   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.173   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.173   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.159          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.152       4000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.180       4000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.173       4000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.159       4000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.158       4000.000  "
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 4000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_4K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 5000 p changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.158</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.832</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.172</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.835</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.158</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.163</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.158</td>\n",
       "      <td>5000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.843     0.655             0.069   \n",
       "After RANDOM Mitigation(K=5 fp)             0.840     0.650             0.061   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.832     0.656             0.066   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.835     0.645             0.066   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.837     0.627             0.080   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.837     0.608             0.071   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.195               0.182   \n",
       "After RANDOM Mitigation(K=5 fp)            0.198               0.183   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.214               0.196   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.200               0.183   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.203               0.185   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.200               0.184   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.174   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.174   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.187   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.175   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.176   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.174   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.159          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.158       5000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.172       5000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.158       5000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.163       5000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.158       5000.000  "
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 5000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_5K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 6000 p changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.164</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.827</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.199</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.163</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.174</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.157</td>\n",
       "      <td>6000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.843     0.655             0.069   \n",
       "After RANDOM Mitigation(K=5 fp)             0.840     0.650             0.067   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.827     0.645             0.068   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.836     0.647             0.068   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.836     0.622             0.082   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.837     0.604             0.070   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.195               0.182   \n",
       "After RANDOM Mitigation(K=5 fp)            0.197               0.186   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.225               0.216   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.202               0.189   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.208               0.195   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.202               0.185   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.174   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.177   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.209   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.179   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.187   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.174   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.159          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.164       6000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.199       6000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.163       6000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.174       6000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.157       6000.000  "
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 6000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_6K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>False Positives</th>\n",
       "      <th>False Negatives</th>\n",
       "      <th>Train Size</th>\n",
       "      <th>Test Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.381</td>\n",
       "      <td>426</td>\n",
       "      <td>598</td>\n",
       "      <td>13014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM mitigation N = 5000</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.374</td>\n",
       "      <td>457</td>\n",
       "      <td>586</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0</th>\n",
       "      <td>0.830</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.294</td>\n",
       "      <td>643</td>\n",
       "      <td>461</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.1</th>\n",
       "      <td>0.832</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.316</td>\n",
       "      <td>598</td>\n",
       "      <td>495</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.2</th>\n",
       "      <td>0.827</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.348</td>\n",
       "      <td>582</td>\n",
       "      <td>545</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.3</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.353</td>\n",
       "      <td>514</td>\n",
       "      <td>553</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.4</th>\n",
       "      <td>0.835</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.366</td>\n",
       "      <td>499</td>\n",
       "      <td>574</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.5</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.378</td>\n",
       "      <td>475</td>\n",
       "      <td>592</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.6</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.401</td>\n",
       "      <td>439</td>\n",
       "      <td>628</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.7</th>\n",
       "      <td>0.833</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.420</td>\n",
       "      <td>430</td>\n",
       "      <td>658</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.8</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.438</td>\n",
       "      <td>382</td>\n",
       "      <td>687</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 5000 p_class 0 = 0.9</th>\n",
       "      <td>0.835</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.461</td>\n",
       "      <td>351</td>\n",
       "      <td>723</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After SMOTE N = 4000 p_class 0 = 1</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.485</td>\n",
       "      <td>298</td>\n",
       "      <td>760</td>\n",
       "      <td>19014</td>\n",
       "      <td>6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                               Accuracy  F1 Score  False Positive Rate  \\\n",
       "Before Mitigation                        0.843     0.655                0.086   \n",
       "After RANDOM mitigation N = 5000         0.840     0.653                0.093   \n",
       "After SMOTE N = 5000 p_class 0 = 0       0.830     0.667                0.130   \n",
       "After SMOTE N = 5000 p_class 0 = 0.1     0.832     0.663                0.121   \n",
       "After SMOTE N = 5000 p_class 0 = 0.2     0.827     0.645                0.118   \n",
       "After SMOTE N = 5000 p_class 0 = 0.3     0.836     0.655                0.104   \n",
       "After SMOTE N = 5000 p_class 0 = 0.4     0.835     0.649                0.101   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5     0.836     0.647                0.096   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6     0.836     0.638                0.089   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7     0.833     0.626                0.087   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8     0.836     0.622                0.077   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9     0.835     0.611                0.071   \n",
       "After SMOTE N = 4000 p_class 0 = 1       0.837     0.604                0.060   \n",
       "\n",
       "Metrics                               False Negative Rate  False Positives  \\\n",
       "Before Mitigation                                   0.381              426   \n",
       "After RANDOM mitigation N = 5000                    0.374              457   \n",
       "After SMOTE N = 5000 p_class 0 = 0                  0.294              643   \n",
       "After SMOTE N = 5000 p_class 0 = 0.1                0.316              598   \n",
       "After SMOTE N = 5000 p_class 0 = 0.2                0.348              582   \n",
       "After SMOTE N = 5000 p_class 0 = 0.3                0.353              514   \n",
       "After SMOTE N = 5000 p_class 0 = 0.4                0.366              499   \n",
       "After SMOTE N = 5000 p_class 0 = 0.5                0.378              475   \n",
       "After SMOTE N = 5000 p_class 0 = 0.6                0.401              439   \n",
       "After SMOTE N = 5000 p_class 0 = 0.7                0.420              430   \n",
       "After SMOTE N = 5000 p_class 0 = 0.8                0.438              382   \n",
       "After SMOTE N = 5000 p_class 0 = 0.9                0.461              351   \n",
       "After SMOTE N = 4000 p_class 0 = 1                  0.485              298   \n",
       "\n",
       "Metrics                               False Negatives  Train Size  Test Size  \n",
       "Before Mitigation                                 598       13014       6508  \n",
       "After RANDOM mitigation N = 5000                  586       19014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0                461       19014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.1              495       19014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.2              545       19014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.3              553       19014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.4              574       19014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.5              592       19014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.6              628       19014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.7              658       19014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.8              687       19014       6508  \n",
       "After SMOTE N = 5000 p_class 0 = 0.9              723       19014       6508  \n",
       "After SMOTE N = 4000 p_class 0 = 1                760       19014       6508  "
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_after_fp_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.170</td>\n",
       "      <td>7000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.829</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.184</td>\n",
       "      <td>7000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.161</td>\n",
       "      <td>7000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.173</td>\n",
       "      <td>7000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.840</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.151</td>\n",
       "      <td>7000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.843     0.655             0.069   \n",
       "After RANDOM Mitigation(K=5 fp)             0.840     0.650             0.078   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.829     0.649             0.067   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.837     0.648             0.067   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.836     0.622             0.086   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.840     0.609             0.068   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.195               0.182   \n",
       "After RANDOM Mitigation(K=5 fp)            0.213               0.192   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.236               0.210   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.200               0.185   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.208               0.195   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.199               0.181   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.174   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.183   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.200   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.176   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.186   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.169   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.159          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.170       7000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.184       7000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.161       7000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.173       7000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.151       7000.000  "
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 7000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_6K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    }
   ],
   "source": [
    "#0\n",
    "N = 8000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_6K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metrics</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>media divergenze</th>\n",
       "      <th>max div</th>\n",
       "      <th>media div primi 10</th>\n",
       "      <th>media div primi 20</th>\n",
       "      <th>media div primi 40</th>\n",
       "      <th># new samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Before Mitigation</th>\n",
       "      <td>0.843</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After RANDOM Mitigation(K=5 fp)</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.164</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.2)</th>\n",
       "      <td>0.832</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.164</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.5)</th>\n",
       "      <td>0.831</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.174</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=0.8)</th>\n",
       "      <td>0.835</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.152</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After Mitigation(K=5 fp, N = 5K, p=1)</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.149</td>\n",
       "      <td>9000.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metrics                                  Accuracy  F1 Score  media divergenze  \\\n",
       "Before Mitigation                           0.843     0.655             0.069   \n",
       "After RANDOM Mitigation(K=5 fp)             0.839     0.648             0.070   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)     0.832     0.658             0.078   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)     0.831     0.636             0.078   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)     0.835     0.622             0.062   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)       0.836     0.595             0.067   \n",
       "\n",
       "Metrics                                  max div  media div primi 10  \\\n",
       "Before Mitigation                          0.195               0.182   \n",
       "After RANDOM Mitigation(K=5 fp)            0.196               0.183   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)    0.208               0.189   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)    0.211               0.196   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)    0.200               0.182   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)      0.199               0.178   \n",
       "\n",
       "Metrics                                  media div primi 20  \\\n",
       "Before Mitigation                                     0.174   \n",
       "After RANDOM Mitigation(K=5 fp)                       0.175   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.180   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.186   \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.171   \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.166   \n",
       "\n",
       "Metrics                                  media div primi 40  # new samples  \n",
       "Before Mitigation                                     0.159          0.000  \n",
       "After RANDOM Mitigation(K=5 fp)                       0.164       9000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.2)               0.164       9000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.5)               0.174       9000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=0.8)               0.152       9000.000  \n",
       "After Mitigation(K=5 fp, N = 5K, p=1)                 0.149       9000.000  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0\n",
    "N = 9000\n",
    "original_size = len(X_to_SMOTE)\n",
    "sampling_strategy = {0: count_0 + int(N*p0), 1: count_1 + int(N*p10)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p0 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p0 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p0 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p0.fit(X_train_mit_SMOTE_p0, y_train_mit_SMOTE_p0)\n",
    "y_pred_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test)\n",
    "\n",
    "#0.1\n",
    "sampling_strategy = {0: count_0 + int(N*p1), 1: count_1 + int(N*p9)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p1 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p1 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p1 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p1.fit(X_train_mit_SMOTE_p1, y_train_mit_SMOTE_p1)\n",
    "y_pred_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test)\n",
    "\n",
    "\n",
    "#0.2\n",
    "sampling_strategy = {0: count_0 + int(N*p2), 1: count_1 + int(N*p8)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p2 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p2 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p2 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p2.fit(X_train_mit_SMOTE_p2, y_train_mit_SMOTE_p2)\n",
    "y_pred_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test)\n",
    "\n",
    "#0.3\n",
    "sampling_strategy = {0: count_0 + int(N*p3), 1: count_1 + int(N*p7)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p3 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p3 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p3 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p3.fit(X_train_mit_SMOTE_p3, y_train_mit_SMOTE_p3)\n",
    "y_pred_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test)\n",
    "\n",
    "\n",
    "#0.4\n",
    "sampling_strategy = {0: count_0 + int(N*p4), 1: count_1 + int(N*p6)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p4 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p4 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p4 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p4.fit(X_train_mit_SMOTE_p4, y_train_mit_SMOTE_p4)\n",
    "y_pred_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#0.5\n",
    "sampling_strategy = {0: count_0 + int(N*p5), 1: count_1 + int(N*p5)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p5 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p5 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p5 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p5.fit(X_train_mit_SMOTE_p5, y_train_mit_SMOTE_p5)\n",
    "y_pred_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.6\n",
    "sampling_strategy = {0: count_0 + int(N*p6), 1: count_1 + int(N*p4)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p6 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p6 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p6 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p6.fit(X_train_mit_SMOTE_p6, y_train_mit_SMOTE_p6)\n",
    "y_pred_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.7\n",
    "sampling_strategy = {0: count_0 + int(N*p7), 1: count_1 + int(N*p3)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p7 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p7 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p7 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p7.fit(X_train_mit_SMOTE_p7, y_train_mit_SMOTE_p7)\n",
    "y_pred_SMOTE_p7= classifier_train_mit_SMOTE_p7.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.8\n",
    "sampling_strategy = {0: count_0 + int(N*p8), 1: count_1 + int(N*p2)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p8 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p8 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p8 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p8.fit(X_train_mit_SMOTE_p8, y_train_mit_SMOTE_p8)\n",
    "y_pred_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.9\n",
    "sampling_strategy = {0: count_0 + int(N*p9), 1: count_1 + int(N*p1)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p9 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p9 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p9 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p9.fit(X_train_mit_SMOTE_p9, y_train_mit_SMOTE_p9)\n",
    "y_pred_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1\n",
    "sampling_strategy = {0: count_0 + int(N*p10), 1: count_1 + int(N*p0)}\n",
    "\n",
    "smote_nc = SMOTENC(sampling_strategy = sampling_strategy, categorical_features=categorical_features, random_state=seed)\n",
    "X_sampled_SMOTE, y_sampled_SMOTE = smote_nc.fit_resample(X_to_SMOTE, y_to_SMOTE)\n",
    " \n",
    "# solo i campioni generati \n",
    "X_generated = X_sampled_SMOTE[-N:]\n",
    "y_generated = y_sampled_SMOTE[-N:]\n",
    "\n",
    "X_train_mit_SMOTE_p10 = pd.concat([X_train, X_generated], ignore_index=True)\n",
    "y_train_mit_SMOTE_p10 = pd.concat([y_train, y_generated], ignore_index=True)\n",
    "\n",
    "\n",
    "#train and test\n",
    "classifier_train_mit_SMOTE_p10 = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mit_SMOTE_p10.fit(X_train_mit_SMOTE_p10, y_train_mit_SMOTE_p10)\n",
    "y_pred_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qui i valori randomici \n",
    "df_holdout_smote_sampled = df_holdout_enc.sample(n=N, replace = True, random_state=seed)\n",
    "df_combinated_random_smote = pd.concat([df_holdout_smote_sampled, df_train_enc], ignore_index=True)\n",
    "df_train_mitigated_random_smote = df_combinated_random_smote.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "X_train_mitigated_random_smote = df_train_mitigated_random_smote.drop(columns=\"income\", axis = 1)\n",
    "y_train_mitigated_random_smote = df_train_mitigated_random_smote['income']\n",
    "classifier_train_mitigated_random_smote_p = RandomForestClassifier(random_state=seed)\n",
    "classifier_train_mitigated_random_smote_p.fit(X_train_mitigated_random_smote, y_train_mitigated_random_smote)\n",
    "y_mitigated_pred_random_smote_p = classifier_train_mitigated_random_smote_p.predict(X_test)\n",
    "\n",
    "    \n",
    "    \n",
    "accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p0 )    \n",
    "accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p1 )\n",
    "accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p2 )\n",
    "accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p3 )\n",
    "accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p4 )\n",
    "accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p5 )\n",
    "accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p6 )\n",
    "accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p7 )\n",
    "accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p8 )\n",
    "accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p9 )\n",
    "accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10 = metrics_to_compare(y_true = y_test, y_pred = y_pred_SMOTE_p10 )\n",
    "\n",
    "\n",
    "\n",
    "accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p = metrics_to_compare(y_true = y_test, y_pred = y_mitigated_pred_random_smote_p)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation' : [accuracy_before, f1_score_before, fpr_before, fnr_before, fp_before, fn_before, len(y_train), len(y_test)],\n",
    "    'After RANDOM mitigation N = 5000' : [accuracy_fp_after_SMOTE_random_p, f1_score_fp_after_SMOTE_random_p, fpr_fp_after_SMOTE_random_p, fnr_fp_after_SMOTE_random_p, fp_fp_after_SMOTE_random_p, fn_fp_after_SMOTE_random_p, len(X_train_mitigated_random_smote), len(y_mitigated_pred_random_smote_p)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0' : [accuracy_fp_after_SMOTE_p0, f1_score_fp_after_SMOTE_p0, fpr_fp_after_SMOTE_p0, fnr_fp_after_SMOTE_p0, fp_fp_after_SMOTE_p0, fn_fp_after_SMOTE_p0, len(X_train_mit_SMOTE_p0), len(y_pred_SMOTE_p0)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.1' : [accuracy_fp_after_SMOTE_p1, f1_score_fp_after_SMOTE_p1, fpr_fp_after_SMOTE_p1, fnr_fp_after_SMOTE_p1, fp_fp_after_SMOTE_p1, fn_fp_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_SMOTE_p1)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.2' : [accuracy_fp_after_SMOTE_p2, f1_score_fp_after_SMOTE_p2, fpr_fp_after_SMOTE_p2, fnr_fp_after_SMOTE_p2, fp_fp_after_SMOTE_p2, fn_fp_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_SMOTE_p2)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.3' : [accuracy_fp_after_SMOTE_p3, f1_score_fp_after_SMOTE_p3, fpr_fp_after_SMOTE_p3, fnr_fp_after_SMOTE_p3, fp_fp_after_SMOTE_p3, fn_fp_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_SMOTE_p3)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.4' : [accuracy_fp_after_SMOTE_p4, f1_score_fp_after_SMOTE_p4, fpr_fp_after_SMOTE_p4, fnr_fp_after_SMOTE_p4, fp_fp_after_SMOTE_p4, fn_fp_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_SMOTE_p4)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.5' : [accuracy_fp_after_SMOTE_p5, f1_score_fp_after_SMOTE_p5, fpr_fp_after_SMOTE_p5, fnr_fp_after_SMOTE_p5, fp_fp_after_SMOTE_p5, fn_fp_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_SMOTE_p5)] ,\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.6' : [accuracy_fp_after_SMOTE_p6, f1_score_fp_after_SMOTE_p6, fpr_fp_after_SMOTE_p6, fnr_fp_after_SMOTE_p6, fp_fp_after_SMOTE_p6, fn_fp_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_SMOTE_p6)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.7' : [accuracy_fp_after_SMOTE_p7, f1_score_fp_after_SMOTE_p7, fpr_fp_after_SMOTE_p7, fnr_fp_after_SMOTE_p7, fp_fp_after_SMOTE_p7, fn_fp_after_SMOTE_p7, len(X_train_mit_SMOTE_p7), len(y_pred_SMOTE_p7)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.8' : [accuracy_fp_after_SMOTE_p8, f1_score_fp_after_SMOTE_p8, fpr_fp_after_SMOTE_p8, fnr_fp_after_SMOTE_p8, fp_fp_after_SMOTE_p8, fn_fp_after_SMOTE_p8, len(X_train_mit_SMOTE_p8), len(y_pred_SMOTE_p8)],\n",
    "    'After SMOTE N = 5000 p_class 0 = 0.9' : [accuracy_fp_after_SMOTE_p9, f1_score_fp_after_SMOTE_p9, fpr_fp_after_SMOTE_p9, fnr_fp_after_SMOTE_p9, fp_fp_after_SMOTE_p9, fn_fp_after_SMOTE_p9, len(X_train_mit_SMOTE_p9), len(y_pred_SMOTE_p9)] ,\n",
    "    'After SMOTE N = 4000 p_class 0 = 1  ' : [accuracy_fp_after_SMOTE_p10, f1_score_fp_after_SMOTE_p10, fpr_fp_after_SMOTE_p10, fnr_fp_after_SMOTE_p10, fp_fp_after_SMOTE_p10, fn_fp_after_SMOTE_p10, len(X_train_mit_SMOTE_p10), len(y_pred_SMOTE_p10)]\n",
    "    \n",
    "    \n",
    "})\n",
    "metrics_after_fp_SMOTE = metrics_after_fp_SMOTE.set_index('Metrics').T\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_SMOTE[metric] = metrics_after_fp_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "metrics_after_fp_SMOTE\n",
    "\n",
    "falsi_positivi_2K_fp_5sub = metrics_after_fp_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_2K_fp_5sub = metrics_after_fp_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_2K_fp_5sub_before = metrics_after_fp_SMOTE['False Negatives'].iloc[0]\n",
    "\n",
    "\n",
    "errors_after_6K = [fp + fn for fp, fn in zip(falsi_positivi_2K_fp_5sub, falsi_negativi_2K_fp_5sub)]\n",
    "errors_before = falsi_negativi_2K_fp_5sub_before + falsi_positivi_2K_fp_5sub_before \n",
    "\n",
    "accuracy02 = metrics_after_fp_SMOTE['Accuracy'].iloc[4]\n",
    "accuracy05 = metrics_after_fp_SMOTE['Accuracy'].iloc[7]\n",
    "accuracy08 = metrics_after_fp_SMOTE['Accuracy'].iloc[10]\n",
    "accuracy1 = metrics_after_fp_SMOTE['Accuracy'].iloc[12]\n",
    "\n",
    "f1score02 = metrics_after_fp_SMOTE['F1 Score'].iloc[4]\n",
    "f1score05 = metrics_after_fp_SMOTE['F1 Score'].iloc[7]\n",
    "f1score08 = metrics_after_fp_SMOTE['F1 Score'].iloc[10]\n",
    "f1score1 = metrics_after_fp_SMOTE['F1 Score'].iloc[12]\n",
    "#SOTTOGRUPPI\n",
    "# y_pred_test_filtered_fp_before = classifier_train.predict(X_test_filtered_fp) trovato prima \n",
    "#previsione su sottogruppi al variare di p fissato n = 5K\n",
    "y_pred_test_filtered_fp_after_SMOTE_p0 = classifier_train_mit_SMOTE_p0.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p1 = classifier_train_mit_SMOTE_p1.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p2 = classifier_train_mit_SMOTE_p2.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p3 = classifier_train_mit_SMOTE_p3.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p4 = classifier_train_mit_SMOTE_p4.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p5 = classifier_train_mit_SMOTE_p5.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p6 = classifier_train_mit_SMOTE_p6.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p7 = classifier_train_mit_SMOTE_p7.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p8 = classifier_train_mit_SMOTE_p8.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p9 = classifier_train_mit_SMOTE_p9.predict(X_test_filtered_fp)\n",
    "y_pred_test_filtered_fp_after_SMOTE_p10 = classifier_train_mit_SMOTE_p10.predict(X_test_filtered_fp)\n",
    "\n",
    "\n",
    "\n",
    "#non ci importa dei sttogruppi \n",
    "accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_before )\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p1)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p2)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p3)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p4)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p5)\n",
    "accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6 = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_fp_after_SMOTE_p6)\n",
    "\n",
    "#random\n",
    "y_pred_test_filtered_random_mit = classifier_train_mitigated_random_smote_p.predict(X_test_filtered_fp)\n",
    "accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random = metrics_to_compare(y_true = y_true_test_filtered_fp, y_pred = y_pred_test_filtered_random_mit)\n",
    "\n",
    "metrics_after_fp_sottogruppi_SMOTE = pd.DataFrame({\n",
    "    'Metrics' : ['Accuracy', 'F1 Score', 'False Positive Rate', 'False Negative Rate', 'False Positives', 'False Negatives', 'Train Size', 'Test Size'],\n",
    "    'Before Mitigation, on subgroups' : [accuracy_fp_sottogruppi_before, f1_score_fp_sottogruppi_before, fpr_fp_sottogruppi_before, fnr_fp_sottogruppi_before, fp_fp_sottogruppi_before, fn_fp_sottogruppi_before, len(y_train), len(y_pred_test_filtered_fp_before)],\n",
    "    'After RANDOM Mitigation(K=5, on subgroups, fp)': [accuracy_fp_sottogruppi_after_random, f1_score_fp_sottogruppi_after_random, fpr_fp_sottogruppi_after_random, fnr_fp_sottogruppi_after_random, fp_fp_sottogruppi_after_random, fn_fp_sottogruppi_after_random, len(X_train_mitigated_random_smote), len(y_pred_test_filtered_random_mit)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.5)': [accuracy_fp_sottogruppi_after_SMOTE_p1, f1_score_fp_sottogruppi_after_SMOTE_p1, fpr_fp_sottogruppi_after_SMOTE_p1, fnr_fp_sottogruppi_after_SMOTE_p1, fp_fp_sottogruppi_after_SMOTE_p1, fn_fp_sottogruppi_after_SMOTE_p1, len(X_train_mit_SMOTE_p1), len(y_pred_test_filtered_fp_after_SMOTE_p1)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.6)': [accuracy_fp_sottogruppi_after_SMOTE_p2, f1_score_fp_sottogruppi_after_SMOTE_p2, fpr_fp_sottogruppi_after_SMOTE_p2, fnr_fp_sottogruppi_after_SMOTE_p2, fp_fp_sottogruppi_after_SMOTE_p2, fn_fp_sottogruppi_after_SMOTE_p2, len(X_train_mit_SMOTE_p2), len(y_pred_test_filtered_fp_after_SMOTE_p2)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.7)': [accuracy_fp_sottogruppi_after_SMOTE_p3, f1_score_fp_sottogruppi_after_SMOTE_p3, fpr_fp_sottogruppi_after_SMOTE_p3, fnr_fp_sottogruppi_after_SMOTE_p3, fp_fp_sottogruppi_after_SMOTE_p3, fn_fp_sottogruppi_after_SMOTE_p3, len(X_train_mit_SMOTE_p3), len(y_pred_test_filtered_fp_after_SMOTE_p3)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.8)': [accuracy_fp_sottogruppi_after_SMOTE_p4, f1_score_fp_sottogruppi_after_SMOTE_p4, fpr_fp_sottogruppi_after_SMOTE_p4, fnr_fp_sottogruppi_after_SMOTE_p4, fp_fp_sottogruppi_after_SMOTE_p4, fn_fp_sottogruppi_after_SMOTE_p4, len(X_train_mit_SMOTE_p4), len(y_pred_test_filtered_fp_after_SMOTE_p4)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 0.9)': [accuracy_fp_sottogruppi_after_SMOTE_p5, f1_score_fp_sottogruppi_after_SMOTE_p5, fpr_fp_sottogruppi_after_SMOTE_p5, fnr_fp_sottogruppi_after_SMOTE_p5, fp_fp_sottogruppi_after_SMOTE_p5, fn_fp_sottogruppi_after_SMOTE_p5, len(X_train_mit_SMOTE_p5), len(y_pred_test_filtered_fp_after_SMOTE_p5)],\n",
    "    'After Mitigation(K=5, on subgroups, fp, SMOTE, 0: 1)': [accuracy_fp_sottogruppi_after_SMOTE_p6, f1_score_fp_sottogruppi_after_SMOTE_p6, fpr_fp_sottogruppi_after_SMOTE_p6, fnr_fp_sottogruppi_after_SMOTE_p6, fp_fp_sottogruppi_after_SMOTE_p6, fn_fp_sottogruppi_after_SMOTE_p6, len(X_train_mit_SMOTE_p6), len(y_pred_test_filtered_fp_after_SMOTE_p6)]\n",
    "\n",
    "})\n",
    "metrics_after_fp_sottogruppi_SMOTE = metrics_after_fp_sottogruppi_SMOTE.set_index('Metrics').T\n",
    "\n",
    "metrics_to_cast = ['False Positives', 'False Negatives', 'Train Size', 'Test Size']\n",
    "\n",
    "for metric in metrics_to_cast:\n",
    "    metrics_after_fp_sottogruppi_SMOTE[metric] = metrics_after_fp_sottogruppi_SMOTE[metric].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Subgroups Decision Tree performance when boolean outcomes = fp e SMOTE \")\n",
    "metrics_after_fp_sottogruppi_SMOTE\n",
    "#salvo risultati che mi servono per i plot\n",
    "falsi_positivi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[2:].tolist()\n",
    "falsi_negativi_5K_fp_5sub_sub = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[2:].tolist()\n",
    "\n",
    "\n",
    "falsi_positivi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Positives'].iloc[0]\n",
    "falsi_negativi_5K_fp_5sub_sub_before = metrics_after_fp_sottogruppi_SMOTE['False Negatives'].iloc[0]\n",
    "#Analisi divergenza per  p=0.5, p=0.8, p=1  \n",
    "#all'inizio sul test set senza nessuna mitigation\n",
    "#prima per la baseline 1 che è quella che replica il metodo del paper \n",
    "#predizioni per il test set y_mitigated_pred \n",
    "\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_no_mitigation  = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_no_mitigation\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.2\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p2\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p2_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#prima per la baseline 2 che è SMOTENC\n",
    "#p=0.5\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p5\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p5_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#p baseline 2 che è SMOTENC p=0.8\n",
    "#p=0.8\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p8\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p8_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p1_5K\n",
    "\n",
    "\n",
    "#baseline 2 che è SMOTENC p=1\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_pred_SMOTE_p10\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_baseline2_p10_5K = df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_p6_5K\n",
    "\n",
    "\n",
    "#random\n",
    "\n",
    "\n",
    "df_test_class = X_test.copy()\n",
    "df_test_class['y_test_true'] = y_test\n",
    "df_test_class['y_pred'] = y_mitigated_pred_random_smote_p\n",
    "\n",
    "#df_test_class.head()\n",
    "\n",
    "y_trues = df_test_class[\"y_test_true\"]\n",
    "y_preds = df_test_class[\"y_pred\"]\n",
    "\n",
    "df_test_class['error'] = (y_trues != y_preds).astype(int)\n",
    "\n",
    "#aggiungo la feature 'error' a df_val non encoded\n",
    "df_test['error'] = df_test_class['error']\n",
    "\n",
    "#come controllo che sia corretto aggiungo la feature y_pred \n",
    "df_test['y_pred'] = df_test_class['y_pred'] \n",
    "\n",
    "#df_test.head()\n",
    "\n",
    "#sottogruppi\n",
    "\n",
    "fp_diver = DivergenceExplorer(df_test)\n",
    "attributes = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'native-country', 'age_group', 'edu_num_group', 'hours_per_week_group']\n",
    "FP_fm = fp_diver.get_pattern_divergence(min_support=min_sup, attributes=attributes, boolean_outcomes=['error'])\n",
    "FP_fm = FP_fm.sort_values(by='error_div', ascending=False, ignore_index=True)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#FP_fm.head()\n",
    "#pruning \n",
    "fp_details = DivergencePatternProcessor(FP_fm, 'error')\n",
    "df_pruned_error = fp_details.redundancy_pruning(th_redundancy=pruning)\n",
    "df_pruned_error = df_pruned_error.sort_values('error_div', ascending=False)\n",
    "df_pruned_error.head()\n",
    "\n",
    "\n",
    "\n",
    "fp_div_list_random_per_confrontare_con_baseline1= df_pruned_error['error_div'].tolist()\n",
    "#fp_div_list_baseline2_random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calcolo delle medie e del massimo con valore assoluto solo dopo\n",
    "media_fp_div_list_no_mitigation = np.nanmean(fp_div_list_no_mitigation)\n",
    "media_fp_div_list_nomitigation_primi10 = np.nanmean(fp_div_list_no_mitigation[:10])\n",
    "media_fp_div_list_nomitigation_primi20 = np.nanmean(fp_div_list_no_mitigation[:20])\n",
    "media_fp_div_list_nomitigation_primi40 = np.nanmean(fp_div_list_no_mitigation[:40])\n",
    "massimo_valore_assoluto_fp_div_no_mitigation = max(abs(x) for x in fp_div_list_no_mitigation)\n",
    "\n",
    "media_fp_div_list_baseline2_p2_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p2_5K_primi10 = np.nanmean(fp_div_list_baseline2_p2_5K[:10])\n",
    "media_fp_div_list_baseline2_p2_5K_primi20 = np.nanmean(fp_div_list_baseline2_p2_5K[:20])\n",
    "media_fp_div_list_baseline2_p2_5K_primi40 = np.nanmean(fp_div_list_baseline2_p2_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K = max(abs(x) for x in fp_div_list_baseline2_p2_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p5_5K = np.nanmean(fp_div_list_baseline2_p5_5K)\n",
    "media_fp_div_list_baseline2_p5_5K_primi10 = np.nanmean(fp_div_list_baseline2_p5_5K[:10])\n",
    "media_fp_div_list_baseline2_p5_5K_primi20 = np.nanmean(fp_div_list_baseline2_p5_5K[:20])\n",
    "media_fp_div_list_baseline2_p5_5K_primi40 = np.nanmean(fp_div_list_baseline2_p5_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K = max(abs(x) for x in fp_div_list_baseline2_p5_5K)\n",
    "\n",
    "\n",
    "media_fp_div_list_baseline2_p8_5K = np.nanmean(fp_div_list_baseline2_p8_5K)\n",
    "media_fp_div_list_baseline2_p8_5K_primi10 = abs(sum(fp_div_list_baseline2_p8_5K[:10]) / len(fp_div_list_baseline2_p8_5K[:10]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi20 = abs(sum(fp_div_list_baseline2_p8_5K[:20]) / len(fp_div_list_baseline2_p8_5K[:20]))\n",
    "media_fp_div_list_baseline2_p8_5K_primi40 = abs(sum(fp_div_list_baseline2_p8_5K[:40]) / len(fp_div_list_baseline2_p8_5K[:40]))\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K = max(abs(x) for x in fp_div_list_baseline2_p8_5K)\n",
    "\n",
    "media_fp_div_list_baseline2_p10_5K = np.nanmean(fp_div_list_baseline2_p10_5K)\n",
    "media_fp_div_list_baseline2_p10_5K_primi10 = np.nanmean(fp_div_list_baseline2_p10_5K[:10])\n",
    "media_fp_div_list_baseline2_p10_5K_primi20 = np.nanmean(fp_div_list_baseline2_p10_5K[:20])\n",
    "media_fp_div_list_baseline2_p10_5K_primi40 = np.nanmean(fp_div_list_baseline2_p10_5K[:40])\n",
    "fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K = max(abs(x) for x in fp_div_list_baseline2_p10_5K)\n",
    "\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1)\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi10 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:10])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi20 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:20])\n",
    "media_fp_div_list_random_per_confrontare_con_baseline1_primi40 = np.nanmean(fp_div_list_random_per_confrontare_con_baseline1[:40])\n",
    "massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1 = max(abs(x) for x in fp_div_list_random_per_confrontare_con_baseline1)\n",
    "\n",
    "# Creazione del DataFrame finale\n",
    "divergence_after_fp_sottogruppi = pd.DataFrame({\n",
    "    'Metrics': [\n",
    "        'Accuracy', 'F1 Score', 'media divergenze', 'max div', 'media div primi 10', 'media div primi 20', 'media div primi 40', '# new samples'\n",
    "    ],\n",
    "    \n",
    "    'Before Mitigation': [\n",
    "        accuracy_before, f1_score_before, media_fp_div_list_no_mitigation, massimo_valore_assoluto_fp_div_no_mitigation,\n",
    "        media_fp_div_list_nomitigation_primi10, media_fp_div_list_nomitigation_primi20, media_fp_div_list_nomitigation_primi40, 0\n",
    "    ],\n",
    "        'After RANDOM Mitigation(K=5 fp)': [\n",
    "        accuracy_fp_after_random, f1_score_fp_after_random, media_fp_div_list_random_per_confrontare_con_baseline1,\n",
    "        massimo_valore_assoluto_fp_div_random_per_confrontare_con_baseline1, media_fp_div_list_random_per_confrontare_con_baseline1_primi10,\n",
    "        media_fp_div_list_random_per_confrontare_con_baseline1_primi20, media_fp_div_list_random_per_confrontare_con_baseline1_primi40,\n",
    "        N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=0.2)': [\n",
    "        accuracy02, f1score02, media_fp_div_list_baseline2_p2_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p2_5K,\n",
    "        media_fp_div_list_baseline2_p2_5K_primi10, media_fp_div_list_baseline2_p2_5K_primi20, media_fp_div_list_baseline2_p2_5K_primi40, N\n",
    "    ],\n",
    "     'After Mitigation(K=5 fp, N = 5K, p=0.5)': [\n",
    "        accuracy05, f1score05, media_fp_div_list_baseline2_p5_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p5_5K,\n",
    "        media_fp_div_list_baseline2_p5_5K_primi10, media_fp_div_list_baseline2_p5_5K_primi20, media_fp_div_list_baseline2_p5_5K_primi40, N\n",
    "    ],\n",
    "      'After Mitigation(K=5 fp, N = 5K, p=0.8)': [\n",
    "        accuracy08, f1score08, media_fp_div_list_baseline2_p8_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p8_5K,\n",
    "        media_fp_div_list_baseline2_p8_5K_primi10, media_fp_div_list_baseline2_p8_5K_primi20, media_fp_div_list_baseline2_p8_5K_primi40, N\n",
    "    ],\n",
    "    'After Mitigation(K=5 fp, N = 5K, p=1)': [\n",
    "        accuracy1, f1score1, media_fp_div_list_baseline2_p10_5K , fp_div_massimo_valore_assoluto_fp_div_list_baseline2_p10_5K,\n",
    "        media_fp_div_list_baseline2_p10_5K_primi10, media_fp_div_list_baseline2_p10_5K_primi20, media_fp_div_list_baseline2_p10_5K_primi40, N\n",
    "    ]\n",
    "\n",
    "})\n",
    "\n",
    "# Trasposizione per visualizzazione\n",
    "divergence_after_fp_sottogruppi = divergence_after_fp_sottogruppi.set_index('Metrics').T\n",
    "\n",
    "divergence_after_fp_sottogruppi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creazione del grafico\n",
    "plt.figure(figsize=(10, 6))\n",
    "p =[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# Linea per i falsi positivi\n",
    "\n",
    "plt.plot(p, errors_after_2K, marker='o', label='Error 2K ', color='black')\n",
    "plt.plot(p, errors_after_3K, marker='o', label='Error 3K ', color='red')\n",
    "plt.plot(p, errors_after_4K, marker='o', label='Error 4K ', color='green')\n",
    "plt.plot(p, errors_after_5K, marker='o', label='Error 5K ', color='blue')\n",
    "plt.plot(p, errors_after_6K,  marker='o', label='Error 6K ', color='orange')\n",
    "\n",
    "\n",
    "plt.axhline(y=errors_before, color='purple', linestyle='--', label='Initial Errors')\n",
    "\n",
    "# Etichette e titolo\n",
    "plt.xlabel('probability for the synthetic point to belong to the class 0')\n",
    "plt.ylabel('Count Errors')\n",
    "plt.title(f'Errors, err, #subgroups = {K}, support = {min_sup} on {filtered_instances}, redundancy = 0.01')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.yticks(range(1175, 1350, 25))\n",
    "plt.xticks(np.arange(0, 1.1, 0.1)) \n",
    "# Mostra il grafico\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1147, 1645)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_1, count_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
